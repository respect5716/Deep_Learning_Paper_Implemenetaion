{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yongsun-yoon/deep-learning-paper-implementation/blob/main/03-natural-language-process/DialoGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29ac0a71-ea19-48f5-a192-8b666d41d00d",
      "metadata": {
        "id": "29ac0a71-ea19-48f5-a192-8b666d41d00d"
      },
      "source": [
        "# DialoGPT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04f0f272-2b46-4e56-b435-3f4aa4cee2b6",
      "metadata": {
        "id": "04f0f272-2b46-4e56-b435-3f4aa4cee2b6"
      },
      "source": [
        "## 0. Info"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c435f01-061d-45ed-a552-7b08300c59d2",
      "metadata": {
        "id": "1c435f01-061d-45ed-a552-7b08300c59d2"
      },
      "source": [
        "## Paper\n",
        "* title: DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation\n",
        "* author: Yizhe Zhang et al.\n",
        "* url: https://arxiv.org/abs/1911.00536\n",
        "\n",
        "## Feats\n",
        "* data: AI Hub\n",
        "\n",
        "## Refs\n",
        "* https://huggingface.co/microsoft/DialoGPT-medium\n",
        "* https://github.com/xcapt0/gpt2_chatbot"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c0aed00-7980-4331-88a7-437b06dd058b",
      "metadata": {
        "id": "0c0aed00-7980-4331-88a7-437b06dd058b"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47bca836-a53f-4516-ac9c-3aa6d19024b3",
      "metadata": {
        "id": "47bca836-a53f-4516-ac9c-3aa6d19024b3"
      },
      "outputs": [],
      "source": [
        "import easydict\n",
        "from glob import glob\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import Adafactor\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c9b3566-2717-4b3b-955d-d73855a38c3b",
      "metadata": {
        "id": "9c9b3566-2717-4b3b-955d-d73855a38c3b"
      },
      "outputs": [],
      "source": [
        "cfg = easydict.EasyDict(\n",
        "    model_name = 'EleutherAI/polyglot-ko-1.3b',\n",
        "    device = 'cuda:2',\n",
        "    num_training_steps = 50000,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e24cf4cf-122a-464d-a5b3-7572915735d5",
      "metadata": {
        "id": "e24cf4cf-122a-464d-a5b3-7572915735d5"
      },
      "source": [
        "## 2. Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "184707ec-0128-4a21-90e9-d34841d6ba8d",
      "metadata": {
        "id": "184707ec-0128-4a21-90e9-d34841d6ba8d"
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data[idx]['text']\n",
        "        dialog = text.split(chr(1000))\n",
        "\n",
        "        src, tgt = dialog[0], dialog[1:]\n",
        "        src += self.tokenizer.eos_token\n",
        "        tgt = self.tokenizer.eos_token.join(tgt)\n",
        "\n",
        "        src_input_ids = self.tokenizer(src)['input_ids']\n",
        "        tgt_input_ids = self.tokenizer(tgt)['input_ids']\n",
        "\n",
        "        input_ids = src_input_ids + tgt_input_ids\n",
        "        labels = [-100] * len(src_input_ids) + tgt_input_ids\n",
        "        \n",
        "        return input_ids, labels\n",
        "\n",
        "    \n",
        "def pad_seq(seq, value, max_length):\n",
        "    seq = seq[:max_length]\n",
        "    seq += [value] * (max_length - len(seq))\n",
        "    return seq\n",
        "    \n",
        "    \n",
        "def collate_fn(batch):\n",
        "    input_ids, labels = list(zip(*batch))\n",
        "\n",
        "    _max_length = max([len(i) for i in input_ids])\n",
        "    max_length = min(_max_length, 256)\n",
        "\n",
        "    input_ids = [pad_seq(i, 2, max_length) for i in input_ids]\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "\n",
        "    labels = [pad_seq(l, -100, max_length) for l in labels]\n",
        "    labels = torch.tensor(labels)\n",
        "    \n",
        "    return input_ids, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4c737b4-8da8-46a0-9c3a-aa74ba9e5db9",
      "metadata": {
        "id": "f4c737b4-8da8-46a0-9c3a-aa74ba9e5db9"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73dcfaff-6154-4122-a282-1fb08ff90801",
      "metadata": {
        "id": "73dcfaff-6154-4122-a282-1fb08ff90801"
      },
      "outputs": [],
      "source": [
        "files = glob('/mnt/dialog-ko/*.txt')\n",
        "data = load_dataset('text', data_files=files)['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "211c53e9-58ef-4d81-8896-02d22a0499ef",
      "metadata": {
        "id": "211c53e9-58ef-4d81-8896-02d22a0499ef"
      },
      "outputs": [],
      "source": [
        "dataset = Dataset(data, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cf13c8a-0bfa-4818-a593-18dc02e0ee64",
      "metadata": {
        "id": "5cf13c8a-0bfa-4818-a593-18dc02e0ee64"
      },
      "outputs": [],
      "source": [
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f771db63-6024-4bd6-a385-5e39f62a9dc2",
      "metadata": {
        "id": "f771db63-6024-4bd6-a385-5e39f62a9dc2"
      },
      "outputs": [],
      "source": [
        "input_ids, labels = next(iter(dataloader))\n",
        "input_ids.shape, labels.shape "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "480da017-014d-4b28-bbb1-f31da627dacc",
      "metadata": {
        "id": "480da017-014d-4b28-bbb1-f31da627dacc"
      },
      "source": [
        "## 3. Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8585dee8-a49b-4e6b-afac-7ea2ed899789",
      "metadata": {
        "id": "8585dee8-a49b-4e6b-afac-7ea2ed899789"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(cfg.model_name)\n",
        "_ = model.train().to(cfg.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "790ff0df-baea-4567-83f8-d72eabc948f9",
      "metadata": {
        "id": "790ff0df-baea-4567-83f8-d72eabc948f9"
      },
      "outputs": [],
      "source": [
        "# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-3)\n",
        "optimizer = Adafactor(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9acb956-691e-4d2f-b9b0-26cbb91d0443",
      "metadata": {
        "id": "b9acb956-691e-4d2f-b9b0-26cbb91d0443"
      },
      "outputs": [],
      "source": [
        "dataiter = iter(dataloader)\n",
        "pbar = tqdm(range(1, cfg.num_training_steps+1))\n",
        "for st in pbar:\n",
        "    try: \n",
        "        input_ids, labels = next(dataiter)\n",
        "    except StopIteration:\n",
        "        dataiter = iter(dataloader)\n",
        "        input_ids, labels = next(dataiter)\n",
        "    input_ids, labels = input_ids.to(cfg.device), labels.to(cfg.device)\n",
        "    \n",
        "    outputs = model(input_ids, labels=labels)\n",
        "    loss = outputs.loss\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    pbar.set_postfix({'loss': loss.item()})\n",
        "    if st % 1000 == 0:\n",
        "        tokenizer.save_pretrained('dialogpt')\n",
        "        model.save_pretrained('dialogpt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "960efddb-0d69-4542-bad4-c998648ae4a0",
      "metadata": {
        "id": "960efddb-0d69-4542-bad4-c998648ae4a0"
      },
      "source": [
        "## 4. Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54218957-9918-4639-aa3e-a59fd42308b6",
      "metadata": {
        "id": "54218957-9918-4639-aa3e-a59fd42308b6"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('dialogpt')\n",
        "model = AutoModelForCausalLM.from_pretrained('dialogpt')\n",
        "_ = model.eval().requires_grad_(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "862da2d5-a4e7-4a73-aa1e-5bc14d0b5400",
      "metadata": {
        "id": "862da2d5-a4e7-4a73-aa1e-5bc14d0b5400"
      },
      "outputs": [],
      "source": [
        "for step in range(5):\n",
        "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
        "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
        "    chat_history_ids = model.generate(bot_input_ids, max_new_tokens=32, repetition_penalty=4.0)\n",
        "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "238c0f34-1ac5-4139-b3de-bcdc2159d7bb",
      "metadata": {
        "id": "238c0f34-1ac5-4139-b3de-bcdc2159d7bb"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}