{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/respect5716/deep-learning-paper-implementation/blob/main/03_NLP/Adapter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a4f3118-8966-48bc-92e9-68445c02a3af",
      "metadata": {
        "id": "5a4f3118-8966-48bc-92e9-68445c02a3af"
      },
      "source": [
        "# Adapter"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2988ea2d-25d5-42c1-99cc-6e7ca17e096f",
      "metadata": {
        "id": "2988ea2d-25d5-42c1-99cc-6e7ca17e096f"
      },
      "source": [
        "## 0. Info"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9adcb344-4eb1-433a-b480-e6ebf46714ef",
      "metadata": {
        "id": "9adcb344-4eb1-433a-b480-e6ebf46714ef"
      },
      "source": [
        "### Paper\n",
        "* title: Parameter-Efficient Transfer Learning for NLP\n",
        "* author: Neil Houlsby et al.\n",
        "* url: https://arxiv.org/abs/1902.00751\n",
        "\n",
        "### Features\n",
        "* pretrained: klue/bert-base\n",
        "* dataset: klue ynat\n",
        "\n",
        "### Reference\n",
        "* https://github.com/huggingface/transformers/blob/main/src/transformers/models/bert/modeling_bert.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a0395df-ac68-4865-b60a-0cc1cfdefca1",
      "metadata": {
        "id": "0a0395df-ac68-4865-b60a-0cc1cfdefca1"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "571daf52-ef1e-41ac-a031-5b8d363d3cd8",
      "metadata": {
        "id": "571daf52-ef1e-41ac-a031-5b8d363d3cd8"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import warnings\n",
        "import easydict\n",
        "from tqdm.auto import tqdm\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "import torch.utils.checkpoint\n",
        "from torch import nn\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "\n",
        "from transformers.activations import ACT2FN\n",
        "from transformers.modeling_outputs import (\n",
        "    BaseModelOutputWithPastAndCrossAttentions,\n",
        "    BaseModelOutputWithPoolingAndCrossAttentions,\n",
        "    CausalLMOutputWithCrossAttentions,\n",
        "    MaskedLMOutput,\n",
        "    MultipleChoiceModelOutput,\n",
        "    NextSentencePredictorOutput,\n",
        "    QuestionAnsweringModelOutput,\n",
        "    SequenceClassifierOutput,\n",
        "    TokenClassifierOutput,\n",
        ")\n",
        "from transformers.modeling_utils import PreTrainedModel\n",
        "from transformers.pytorch_utils import (\n",
        "    apply_chunking_to_forward,\n",
        "    find_pruneable_heads_and_indices,\n",
        "    is_torch_greater_than_1_6,\n",
        "    prune_linear_layer,\n",
        ")\n",
        "from transformers.utils import (\n",
        "    ModelOutput,\n",
        "    add_code_sample_docstrings,\n",
        "    add_start_docstrings,\n",
        "    add_start_docstrings_to_model_forward,\n",
        "    logging,\n",
        "    replace_return_docstrings,\n",
        ")\n",
        "\n",
        "from transformers import BertConfig\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1489df0f-f645-4cfa-a480-c1e55eccd321",
      "metadata": {
        "id": "1489df0f-f645-4cfa-a480-c1e55eccd321"
      },
      "outputs": [],
      "source": [
        "cfg = easydict.EasyDict(\n",
        "    pretrained = 'klue/bert-base',\n",
        "    device = 'cuda:0',\n",
        "    batch_size = 16,\n",
        "    num_epochs = 3,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88efb513-da04-415f-bb9b-71ea54cc80a9",
      "metadata": {
        "id": "88efb513-da04-415f-bb9b-71ea54cc80a9"
      },
      "source": [
        "## 2. Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "535ed168-aa7f-4b9c-8372-433bb4e2358f",
      "metadata": {
        "id": "535ed168-aa7f-4b9c-8372-433bb4e2358f",
        "outputId": "3949515a-03bf-4847-cb25-6bbbbc70e3a7",
        "colab": {
          "referenced_widgets": [
            "82a2dfa43f08456390a608f32cde3cc3"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Reusing dataset klue (/root/.cache/huggingface/datasets/klue/ynat/1.0.0/e0fc3bc3de3eb03be2c92d72fd04a60ecc71903f821619cb28ca0e1e29e4233e)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82a2dfa43f08456390a608f32cde3cc3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ds = load_dataset('klue', 'ynat')\n",
        "train_ds = ds['train']\n",
        "eval_ds = ds['validation']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "642ec475-5685-44b9-baf7-0db26521d83a",
      "metadata": {
        "id": "642ec475-5685-44b9-baf7-0db26521d83a"
      },
      "source": [
        "## 3. Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b29e3a50-cd8e-4544-894e-d0347f881b7f",
      "metadata": {
        "id": "b29e3a50-cd8e-4544-894e-d0347f881b7f"
      },
      "outputs": [],
      "source": [
        "class Adapter(nn.Module):\n",
        "    def __init__(self, hidden_size, down_size=64, activation='gelu'):\n",
        "        super().__init__()\n",
        "        self.down = nn.Linear(hidden_size, down_size)\n",
        "        self.actv = ACT2FN[activation]\n",
        "        self.up = nn.Linear(down_size, hidden_size)\n",
        "        self.init_params()\n",
        "        \n",
        "    def init_params(self):\n",
        "        nn.init.zeros_(self.down.weight)\n",
        "        nn.init.zeros_(self.down.bias)\n",
        "        nn.init.zeros_(self.up.weight)\n",
        "        nn.init.zeros_(self.up.bias)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        x = self.up(self.actv(self.down(x)))\n",
        "        x += identity\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5447cab-6699-43fd-873a-1f406969c581",
      "metadata": {
        "id": "e5447cab-6699-43fd-873a-1f406969c581"
      },
      "outputs": [],
      "source": [
        "class BertEmbeddings(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
        "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
        "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
        "        if is_torch_greater_than_1_6:\n",
        "            self.register_buffer(\n",
        "                \"token_type_ids\",\n",
        "                torch.zeros(self.position_ids.size(), dtype=torch.long),\n",
        "                persistent=False,\n",
        "            )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.LongTensor] = None,\n",
        "        token_type_ids: Optional[torch.LongTensor] = None,\n",
        "        position_ids: Optional[torch.LongTensor] = None,\n",
        "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
        "        past_key_values_length: int = 0,\n",
        "    ) -> torch.Tensor:\n",
        "        if input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        else:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "\n",
        "        seq_length = input_shape[1]\n",
        "\n",
        "        if position_ids is None:\n",
        "            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n",
        "\n",
        "        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n",
        "        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n",
        "        # issue #5664\n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self, \"token_type_ids\"):\n",
        "                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.word_embeddings(input_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = inputs_embeds + token_type_embeddings\n",
        "        if self.position_embedding_type == \"absolute\":\n",
        "            position_embeddings = self.position_embeddings(position_ids)\n",
        "            embeddings += position_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config, position_embedding_type=None):\n",
        "        super().__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
        "            raise ValueError(\n",
        "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
        "                f\"heads ({config.num_attention_heads})\"\n",
        "            )\n",
        "\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "        self.position_embedding_type = position_embedding_type or getattr(\n",
        "            config, \"position_embedding_type\", \"absolute\"\n",
        "        )\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            self.max_position_embeddings = config.max_position_embeddings\n",
        "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
        "\n",
        "        self.is_decoder = config.is_decoder\n",
        "\n",
        "    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "\n",
        "        # If this is instantiated as a cross-attention module, the keys\n",
        "        # and values come from an encoder; the attention mask needs to be\n",
        "        # such that the encoder's padding tokens are not attended to.\n",
        "        is_cross_attention = encoder_hidden_states is not None\n",
        "\n",
        "        if is_cross_attention and past_key_value is not None:\n",
        "            # reuse k,v, cross_attentions\n",
        "            key_layer = past_key_value[0]\n",
        "            value_layer = past_key_value[1]\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif is_cross_attention:\n",
        "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
        "            attention_mask = encoder_attention_mask\n",
        "        elif past_key_value is not None:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
        "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
        "        else:\n",
        "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
        "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
        "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
        "            # key/value_states (first \"if\" case)\n",
        "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
        "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
        "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
        "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
        "            past_key_value = (key_layer, value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "\n",
        "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
        "            seq_length = hidden_states.size()[1]\n",
        "            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
        "            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
        "            distance = position_ids_l - position_ids_r\n",
        "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
        "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
        "\n",
        "            if self.position_embedding_type == \"relative_key\":\n",
        "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores\n",
        "            elif self.position_embedding_type == \"relative_key_query\":\n",
        "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
        "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
        "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
        "\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        if attention_mask is not None:\n",
        "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "            attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        # Mask heads if we want to\n",
        "        if head_mask is not None:\n",
        "            attention_probs = attention_probs * head_mask\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(new_context_layer_shape)\n",
        "\n",
        "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (past_key_value,)\n",
        "        return outputs\n",
        "\n",
        "# Add adapter\n",
        "class BertSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.adapter = Adapter(config.hidden_size)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.adapter(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertAttention(nn.Module):\n",
        "    def __init__(self, config, position_embedding_type=None):\n",
        "        super().__init__()\n",
        "        self.self = BertSelfAttention(config, position_embedding_type=position_embedding_type)\n",
        "        self.output = BertSelfOutput(config)\n",
        "        self.pruned_heads = set()\n",
        "\n",
        "    def prune_heads(self, heads):\n",
        "        if len(heads) == 0:\n",
        "            return\n",
        "        heads, index = find_pruneable_heads_and_indices(\n",
        "            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
        "        )\n",
        "\n",
        "        # Prune linear layers\n",
        "        self.self.query = prune_linear_layer(self.self.query, index)\n",
        "        self.self.key = prune_linear_layer(self.self.key, index)\n",
        "        self.self.value = prune_linear_layer(self.self.value, index)\n",
        "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
        "\n",
        "        # Update hyper params and store pruned heads\n",
        "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
        "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
        "        self.pruned_heads = self.pruned_heads.union(heads)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        self_outputs = self.self(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            encoder_hidden_states,\n",
        "            encoder_attention_mask,\n",
        "            past_key_value,\n",
        "            output_attentions,\n",
        "        )\n",
        "        attention_output = self.output(self_outputs[0], hidden_states)\n",
        "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class BertIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        if isinstance(config.hidden_act, str):\n",
        "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states\n",
        "\n",
        "# Add adapter\n",
        "class BertOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.adapter = Adapter(config.hidden_size)\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.adapter(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "\n",
        "class BertLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
        "        self.seq_len_dim = 1\n",
        "        self.attention = BertAttention(config)\n",
        "        self.is_decoder = config.is_decoder\n",
        "        self.add_cross_attention = config.add_cross_attention\n",
        "        if self.add_cross_attention:\n",
        "            if not self.is_decoder:\n",
        "                raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n",
        "            self.crossattention = BertAttention(config, position_embedding_type=\"absolute\")\n",
        "        self.intermediate = BertIntermediate(config)\n",
        "        self.output = BertOutput(config)\n",
        "\n",
        "        \n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "    ) -> Tuple[torch.Tensor]:\n",
        "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
        "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
        "        self_attention_outputs = self.attention(\n",
        "            hidden_states,\n",
        "            attention_mask,\n",
        "            head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "            past_key_value=self_attn_past_key_value,\n",
        "        )\n",
        "        attention_output = self_attention_outputs[0]\n",
        "\n",
        "        # if decoder, the last output is tuple of self-attn cache\n",
        "        if self.is_decoder:\n",
        "            outputs = self_attention_outputs[1:-1]\n",
        "            present_key_value = self_attention_outputs[-1]\n",
        "        else:\n",
        "            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
        "\n",
        "        cross_attn_present_key_value = None\n",
        "        if self.is_decoder and encoder_hidden_states is not None:\n",
        "            if not hasattr(self, \"crossattention\"):\n",
        "                raise ValueError(\n",
        "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n",
        "                    \" by setting `config.add_cross_attention=True`\"\n",
        "                )\n",
        "\n",
        "            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n",
        "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
        "            cross_attention_outputs = self.crossattention(\n",
        "                attention_output,\n",
        "                attention_mask,\n",
        "                head_mask,\n",
        "                encoder_hidden_states,\n",
        "                encoder_attention_mask,\n",
        "                cross_attn_past_key_value,\n",
        "                output_attentions,\n",
        "            )\n",
        "            attention_output = cross_attention_outputs[0]\n",
        "            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n",
        "\n",
        "            # add cross-attn cache to positions 3,4 of present_key_value tuple\n",
        "            cross_attn_present_key_value = cross_attention_outputs[-1]\n",
        "            present_key_value = present_key_value + cross_attn_present_key_value\n",
        "\n",
        "        layer_output = apply_chunking_to_forward(\n",
        "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
        "        )\n",
        "        outputs = (layer_output,) + outputs\n",
        "\n",
        "        # if decoder, return the attn key/values as the last output\n",
        "        if self.is_decoder:\n",
        "            outputs = outputs + (present_key_value,)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def feed_forward_chunk(self, attention_output):\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output\n",
        "\n",
        "\n",
        "class BertEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])\n",
        "        self.gradient_checkpointing = False\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "        output_hidden_states: Optional[bool] = False,\n",
        "        return_dict: Optional[bool] = True,\n",
        "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attentions = () if output_attentions else None\n",
        "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
        "\n",
        "        next_decoder_cache = () if use_cache else None\n",
        "        for i, layer_module in enumerate(self.layer):\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
        "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
        "\n",
        "            if self.gradient_checkpointing and self.training:\n",
        "\n",
        "                if use_cache:\n",
        "                    print(\"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\")\n",
        "                    use_cache = False\n",
        "\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        return module(*inputs, past_key_value, output_attentions)\n",
        "\n",
        "                    return custom_forward\n",
        "\n",
        "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(layer_module),\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                )\n",
        "            else:\n",
        "                layer_outputs = layer_module(\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    layer_head_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                    past_key_value,\n",
        "                    output_attentions,\n",
        "                )\n",
        "\n",
        "            hidden_states = layer_outputs[0]\n",
        "            if use_cache:\n",
        "                next_decoder_cache += (layer_outputs[-1],)\n",
        "            if output_attentions:\n",
        "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
        "                if self.config.add_cross_attention:\n",
        "                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(\n",
        "                v\n",
        "                for v in [\n",
        "                    hidden_states,\n",
        "                    next_decoder_cache,\n",
        "                    all_hidden_states,\n",
        "                    all_self_attentions,\n",
        "                    all_cross_attentions,\n",
        "                ]\n",
        "                if v is not None\n",
        "            )\n",
        "        return BaseModelOutputWithPastAndCrossAttentions(\n",
        "            last_hidden_state=hidden_states,\n",
        "            past_key_values=next_decoder_cache,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attentions,\n",
        "            cross_attentions=all_cross_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "class BertPooler(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
        "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
        "        # to the first token.\n",
        "        first_token_tensor = hidden_states[:, 0]\n",
        "        pooled_output = self.dense(first_token_tensor)\n",
        "        pooled_output = self.activation(pooled_output)\n",
        "        return pooled_output\n",
        "\n",
        "\n",
        "\n",
        "class BertPreTrainedModel(PreTrainedModel):\n",
        "    config_class = BertConfig\n",
        "    base_model_prefix = \"bert\"\n",
        "    supports_gradient_checkpointing = True\n",
        "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        \"\"\"Initialize the weights\"\"\"\n",
        "        if isinstance(module, nn.Linear):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    def _set_gradient_checkpointing(self, module, value=False):\n",
        "        if isinstance(module, BertEncoder):\n",
        "            module.gradient_checkpointing = value\n",
        "\n",
        "\n",
        "\n",
        "class BertModel(BertPreTrainedModel):\n",
        "    def __init__(self, config, add_pooling_layer=True):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "\n",
        "        self.embeddings = BertEmbeddings(config)\n",
        "        self.encoder = BertEncoder(config)\n",
        "\n",
        "        self.pooler = BertPooler(config) if add_pooling_layer else None\n",
        "\n",
        "        # Initialize weights and apply final processing\n",
        "        self.post_init()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embeddings.word_embeddings\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embeddings.word_embeddings = value\n",
        "\n",
        "    def _prune_heads(self, heads_to_prune):\n",
        "        for layer, heads in heads_to_prune.items():\n",
        "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        token_type_ids: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.Tensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
        "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
        "        use_cache: Optional[bool] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if self.config.is_decoder:\n",
        "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        else:\n",
        "            use_cache = False\n",
        "\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        batch_size, seq_length = input_shape\n",
        "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "\n",
        "        # past_key_values_length\n",
        "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
        "\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
        "\n",
        "        if token_type_ids is None:\n",
        "            if hasattr(self.embeddings, \"token_type_ids\"):\n",
        "                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n",
        "                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n",
        "                token_type_ids = buffered_token_type_ids_expanded\n",
        "            else:\n",
        "                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "\n",
        "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
        "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
        "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n",
        "\n",
        "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
        "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
        "        if self.config.is_decoder and encoder_hidden_states is not None:\n",
        "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
        "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
        "            if encoder_attention_mask is None:\n",
        "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
        "            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
        "        else:\n",
        "            encoder_extended_attention_mask = None\n",
        "\n",
        "        # Prepare head mask if needed\n",
        "        # 1.0 in head_mask indicate we keep the head\n",
        "        # attention_probs has shape bsz x n_heads x N x N\n",
        "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
        "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
        "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
        "\n",
        "        embedding_output = self.embeddings(\n",
        "            input_ids=input_ids,\n",
        "            position_ids=position_ids,\n",
        "            token_type_ids=token_type_ids,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            past_key_values_length=past_key_values_length,\n",
        "        )\n",
        "        encoder_outputs = self.encoder(\n",
        "            embedding_output,\n",
        "            attention_mask=extended_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            encoder_hidden_states=encoder_hidden_states,\n",
        "            encoder_attention_mask=encoder_extended_attention_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "        sequence_output = encoder_outputs[0]\n",
        "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
        "\n",
        "        if not return_dict:\n",
        "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
        "\n",
        "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
        "            last_hidden_state=sequence_output,\n",
        "            pooler_output=pooled_output,\n",
        "            past_key_values=encoder_outputs.past_key_values,\n",
        "            hidden_states=encoder_outputs.hidden_states,\n",
        "            attentions=encoder_outputs.attentions,\n",
        "            cross_attentions=encoder_outputs.cross_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "class BertForSequenceClassification(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.config = config\n",
        "\n",
        "        self.bert = BertModel(config)\n",
        "        classifier_dropout = (config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob)\n",
        "        self.dropout = nn.Dropout(classifier_dropout)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "\n",
        "        self.post_init()\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids: Optional[torch.Tensor] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        token_type_ids: Optional[torch.Tensor] = None,\n",
        "        position_ids: Optional[torch.Tensor] = None,\n",
        "        head_mask: Optional[torch.Tensor] = None,\n",
        "        inputs_embeds: Optional[torch.Tensor] = None,\n",
        "        labels: Optional[torch.Tensor] = None,\n",
        "        output_attentions: Optional[bool] = None,\n",
        "        output_hidden_states: Optional[bool] = None,\n",
        "        return_dict: Optional[bool] = None,\n",
        "    ) -> Union[Tuple[torch.Tensor], SequenceClassifierOutput]:\n",
        "\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        pooled_output = outputs[1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            if self.config.problem_type is None:\n",
        "                if self.num_labels == 1:\n",
        "                    self.config.problem_type = \"regression\"\n",
        "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
        "                    self.config.problem_type = \"single_label_classification\"\n",
        "                else:\n",
        "                    self.config.problem_type = \"multi_label_classification\"\n",
        "\n",
        "            if self.config.problem_type == \"regression\":\n",
        "                loss_fct = MSELoss()\n",
        "                if self.num_labels == 1:\n",
        "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
        "                else:\n",
        "                    loss = loss_fct(logits, labels)\n",
        "            elif self.config.problem_type == \"single_label_classification\":\n",
        "                loss_fct = CrossEntropyLoss()\n",
        "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "            elif self.config.problem_type == \"multi_label_classification\":\n",
        "                loss_fct = BCEWithLogitsLoss()\n",
        "                loss = loss_fct(logits, labels)\n",
        "        if not return_dict:\n",
        "            output = (logits,) + outputs[2:]\n",
        "            return ((loss,) + output) if loss is not None else output\n",
        "\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=outputs.attentions,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "212bc24e-2e68-48f0-9900-30ac91b96040",
      "metadata": {
        "id": "212bc24e-2e68-48f0-9900-30ac91b96040"
      },
      "source": [
        "## 4. Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f8adced-9239-4e1e-be20-923977d6325b",
      "metadata": {
        "id": "0f8adced-9239-4e1e-be20-923977d6325b"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch, tokenizer):\n",
        "    inputs = tokenizer(batch['title'], padding=True, return_tensors='pt')\n",
        "    inputs['labels'] = torch.tensor(batch['label'])\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b146e0f4-8bf5-4387-a115-0db6eb01ed11",
      "metadata": {
        "id": "b146e0f4-8bf5-4387-a115-0db6eb01ed11",
        "outputId": "c57d875e-acfa-43c8-e5c9-232817168281"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['bert.encoder.layer.7.output.adapter.down.bias', 'bert.encoder.layer.0.output.adapter.up.bias', 'bert.encoder.layer.4.output.adapter.up.weight', 'bert.encoder.layer.4.output.adapter.down.bias', 'bert.encoder.layer.3.output.adapter.up.weight', 'bert.encoder.layer.4.output.adapter.down.weight', 'bert.encoder.layer.10.output.adapter.up.bias', 'bert.encoder.layer.5.attention.output.adapter.up.weight', 'bert.encoder.layer.11.attention.output.adapter.down.weight', 'bert.encoder.layer.3.attention.output.adapter.down.weight', 'bert.encoder.layer.1.output.adapter.up.weight', 'bert.encoder.layer.1.attention.output.adapter.up.weight', 'bert.encoder.layer.9.attention.output.adapter.down.weight', 'bert.encoder.layer.2.attention.output.adapter.up.weight', 'bert.encoder.layer.10.output.adapter.down.weight', 'bert.encoder.layer.2.attention.output.adapter.up.bias', 'bert.encoder.layer.6.attention.output.adapter.up.weight', 'bert.encoder.layer.7.output.adapter.down.weight', 'bert.encoder.layer.4.attention.output.adapter.down.bias', 'bert.encoder.layer.11.output.adapter.down.weight', 'bert.encoder.layer.2.output.adapter.down.weight', 'bert.encoder.layer.2.output.adapter.up.bias', 'bert.encoder.layer.8.output.adapter.up.weight', 'bert.encoder.layer.8.attention.output.adapter.down.bias', 'bert.encoder.layer.1.output.adapter.up.bias', 'bert.encoder.layer.11.output.adapter.down.bias', 'bert.encoder.layer.1.output.adapter.down.weight', 'bert.encoder.layer.8.output.adapter.down.weight', 'bert.encoder.layer.8.attention.output.adapter.up.bias', 'bert.encoder.layer.1.attention.output.adapter.down.bias', 'bert.encoder.layer.8.output.adapter.down.bias', 'bert.encoder.layer.7.attention.output.adapter.up.bias', 'bert.encoder.layer.0.attention.output.adapter.up.bias', 'bert.encoder.layer.7.output.adapter.up.bias', 'bert.encoder.layer.11.attention.output.adapter.up.weight', 'bert.encoder.layer.7.output.adapter.up.weight', 'bert.encoder.layer.10.attention.output.adapter.down.bias', 'bert.encoder.layer.6.attention.output.adapter.down.weight', 'bert.encoder.layer.3.output.adapter.up.bias', 'bert.encoder.layer.3.attention.output.adapter.down.bias', 'bert.encoder.layer.11.attention.output.adapter.down.bias', 'bert.encoder.layer.10.output.adapter.down.bias', 'bert.encoder.layer.0.attention.output.adapter.down.weight', 'bert.encoder.layer.7.attention.output.adapter.down.weight', 'bert.encoder.layer.5.attention.output.adapter.down.bias', 'bert.encoder.layer.8.attention.output.adapter.down.weight', 'classifier.weight', 'bert.encoder.layer.9.output.adapter.up.bias', 'bert.encoder.layer.0.output.adapter.down.weight', 'bert.encoder.layer.4.output.adapter.up.bias', 'bert.encoder.layer.0.output.adapter.up.weight', 'bert.encoder.layer.6.output.adapter.down.weight', 'bert.encoder.layer.9.output.adapter.down.weight', 'bert.encoder.layer.3.attention.output.adapter.up.weight', 'bert.encoder.layer.2.attention.output.adapter.down.bias', 'bert.encoder.layer.6.attention.output.adapter.down.bias', 'bert.encoder.layer.9.output.adapter.down.bias', 'bert.encoder.layer.10.attention.output.adapter.down.weight', 'bert.encoder.layer.6.output.adapter.down.bias', 'bert.encoder.layer.9.output.adapter.up.weight', 'bert.encoder.layer.1.output.adapter.down.bias', 'bert.encoder.layer.0.attention.output.adapter.up.weight', 'bert.encoder.layer.5.output.adapter.down.weight', 'bert.encoder.layer.4.attention.output.adapter.up.bias', 'bert.encoder.layer.7.attention.output.adapter.up.weight', 'bert.encoder.layer.10.attention.output.adapter.up.bias', 'bert.encoder.layer.0.attention.output.adapter.down.bias', 'bert.encoder.layer.10.output.adapter.up.weight', 'bert.encoder.layer.10.attention.output.adapter.up.weight', 'classifier.bias', 'bert.encoder.layer.11.attention.output.adapter.up.bias', 'bert.encoder.layer.4.attention.output.adapter.down.weight', 'bert.encoder.layer.5.attention.output.adapter.down.weight', 'bert.encoder.layer.11.output.adapter.up.weight', 'bert.encoder.layer.5.attention.output.adapter.up.bias', 'bert.encoder.layer.3.output.adapter.down.bias', 'bert.encoder.layer.3.attention.output.adapter.up.bias', 'bert.encoder.layer.5.output.adapter.up.bias', 'bert.encoder.layer.8.attention.output.adapter.up.weight', 'bert.encoder.layer.1.attention.output.adapter.up.bias', 'bert.encoder.layer.3.output.adapter.down.weight', 'bert.encoder.layer.2.output.adapter.up.weight', 'bert.encoder.layer.2.attention.output.adapter.down.weight', 'bert.encoder.layer.6.output.adapter.up.bias', 'bert.encoder.layer.9.attention.output.adapter.down.bias', 'bert.encoder.layer.6.attention.output.adapter.up.bias', 'bert.encoder.layer.9.attention.output.adapter.up.bias', 'bert.encoder.layer.2.output.adapter.down.bias', 'bert.encoder.layer.6.output.adapter.up.weight', 'bert.encoder.layer.4.attention.output.adapter.up.weight', 'bert.encoder.layer.5.output.adapter.up.weight', 'bert.encoder.layer.5.output.adapter.down.bias', 'bert.encoder.layer.7.attention.output.adapter.down.bias', 'bert.encoder.layer.1.attention.output.adapter.down.weight', 'bert.encoder.layer.0.output.adapter.down.bias', 'bert.encoder.layer.11.output.adapter.up.bias', 'bert.encoder.layer.9.attention.output.adapter.up.weight', 'bert.encoder.layer.8.output.adapter.up.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = BertForSequenceClassification.from_pretrained(cfg.pretrained, num_labels=7).to(cfg.device)\n",
        "\n",
        "params = []\n",
        "for name, param in model.named_parameters():\n",
        "    if 'adapter' in name:\n",
        "        params.append(param)\n",
        "\n",
        "optimizer = torch.optim.AdamW(params, lr=1e-4, weight_decay=1e-2)\n",
        "tokenizer = AutoTokenizer.from_pretrained(cfg.pretrained)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7ac268f-4e54-4d10-9d10-2db0dab8444b",
      "metadata": {
        "tags": [],
        "id": "b7ac268f-4e54-4d10-9d10-2db0dab8444b",
        "outputId": "bd8e50e2-737d-456b-bd79-8fb016f59993",
        "colab": {
          "referenced_widgets": [
            "d9e6649e89774e9184eb100cf2ae152e",
            "d81ec5c6d6714982a509e38aaeaa076f",
            "76b59a520b1a460e86c85bead8321c29",
            "fb7f6c328f2b4dfeb74ef4c50b0530ee",
            "a1bdc86fb70245f3a385427512d7b422",
            "c225df1bdb18484fa8e4f82011f90857"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d9e6649e89774e9184eb100cf2ae152e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2855 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d81ec5c6d6714982a509e38aaeaa076f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/570 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EP 00 | ACC 85.26\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76b59a520b1a460e86c85bead8321c29",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2855 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb7f6c328f2b4dfeb74ef4c50b0530ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/570 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EP 01 | ACC 86.49\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a1bdc86fb70245f3a385427512d7b422",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2855 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c225df1bdb18484fa8e4f82011f90857",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/570 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EP 02 | ACC 86.86\n"
          ]
        }
      ],
      "source": [
        "for ep in range(cfg.num_epochs):\n",
        "    model.train()\n",
        "    pbar = tqdm(range(0, len(train_ds), cfg.batch_size))\n",
        "    for i in pbar:\n",
        "        batch = train_ds[i:i+cfg.batch_size]\n",
        "        batch = collate_fn(batch, tokenizer).to(cfg.device)\n",
        "        outputs = model(**batch)\n",
        "        \n",
        "        loss = outputs.loss\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        pbar.set_postfix({'loss': loss.item()})\n",
        "    \n",
        "    model.eval()\n",
        "    preds, targets = [], []\n",
        "    pbar = tqdm(range(0, len(eval_ds), cfg.batch_size))\n",
        "    for i in pbar:\n",
        "        batch = eval_ds[i:i+cfg.batch_size]\n",
        "        batch = collate_fn(batch, tokenizer).to(cfg.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch)\n",
        "        preds.append(outputs.logits.argmax(dim=-1).cpu())\n",
        "        targets.append(batch.labels.cpu())\n",
        "    \n",
        "    preds = torch.cat(preds, dim=0)\n",
        "    targets = torch.cat(targets, dim=0)\n",
        "    acc = (preds == targets).float().mean() * 100\n",
        "    print(f'EP {ep:02d} | ACC {acc:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b3800dd-2524-49dc-a245-5035f3a2fadc",
      "metadata": {
        "id": "4b3800dd-2524-49dc-a245-5035f3a2fadc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84a0f9e8-380f-4a9d-8cce-403d4c1a6aee",
      "metadata": {
        "id": "84a0f9e8-380f-4a9d-8cce-403d4c1a6aee"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "080b64ea-e938-4491-baca-74d1f9f8ad56",
      "metadata": {
        "id": "080b64ea-e938-4491-baca-74d1f9f8ad56"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}