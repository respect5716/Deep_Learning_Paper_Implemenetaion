{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MAML",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzh7yGxNDluC"
      },
      "source": [
        "# MAML"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iki6sM3EDsjS"
      },
      "source": [
        "## 0. Paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "subFe3E5DttM"
      },
      "source": [
        "### Info\n",
        "* Title: Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\n",
        "* Author: Chelsea Finn\n",
        "* Task: Meta Learning\n",
        "* Link: https://arxiv.org/abs/1703.03400\n",
        "\n",
        "\n",
        "### Features\n",
        "* Dataset: mini-Imagenet, [link](https://drive.google.com/file/d/1HkgrkAwukzEZA0TpO7010PkAOREb2Nuk/view)\n",
        "* Dataset: Omniglot, [link](https://github.com/brendenlake/omniglot)\n",
        "\n",
        "### Reference\n",
        "* https://github.com/dragen1860/MAML-Pytorch\n",
        "* https://github.com/tristandeleu/pytorch-meta\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuaQpOWMD9tz"
      },
      "source": [
        "## 1. Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZizQk84lEC3L"
      },
      "source": [
        "# Libraries\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from glob import glob\n",
        "from collections import OrderedDict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import torchsummary"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsk7XAE2Lz_g"
      },
      "source": [
        "CONFIG = {\n",
        "    'update_lr': 0.01,\n",
        "    'update_step': 5,\n",
        "    'finetune_step': 10,\n",
        "    'meta_lr': 0.01,\n",
        "    'n_way': 5,\n",
        "    'k_shot': 1,\n",
        "    'k_test': 15,\n",
        "    'image_size': 28,\n",
        "    'task_size': 32,\n",
        "    'epoch_size': 100,\n",
        "    'base_dir': '/content/drive/Shared drives/Yoon/Project/Doing/Deep Learning Paper Implementation',\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd9D_aouEVMB"
      },
      "source": [
        "## 2. Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzYjD0NkEzIs"
      },
      "source": [
        "class MiniImageNetDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, task_size, n_way, k_shot, k_test, transform=None):\n",
        "        self.task_size = task_size\n",
        "        self.n_way = n_way\n",
        "        self.k_shot = k_shot\n",
        "        self.k_test = k_test\n",
        "        self.transform = transform\n",
        "\n",
        "        self.image_dir = os.path.join(root, 'images')\n",
        "        self.csv = pd.read_csv(os.path.join(root, f'{mode}.csv'))\n",
        "        self.labels = self.csv['label'].unique()\n",
        "        self.image2label = self.csv.set_index('filename')['label'].to_dict()\n",
        "    \n",
        "\n",
        "    def __len__(self):\n",
        "        return 100\n",
        "    \n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        train_x, test_x = [], []\n",
        "        selected_labels = np.random.choice(self.labels, self.n_way, replace=False)\n",
        "\n",
        "        for label in selected_labels:\n",
        "            images = self.csv.loc[self.csv['label']==label, 'filename'].sample(self.k_shot + self.k_test).tolist()\n",
        "            train_x += images[:self.k_shot]\n",
        "            test_x += images[self.k_shot:]\n",
        "\n",
        "        np.random.shuffle(train_x)\n",
        "        np.random.shuffle(test_x)\n",
        "\n",
        "        train_y = [self.image2label[i] for i in train_x]\n",
        "        test_y = [self.image2label[i] for i in test_x]\n",
        "        relative_label = np.unique(train_y)\n",
        "        np.random.shuffle(relative_label)\n",
        "        label2idx = {i:idx for idx, i in enumerate(relative_label)}\n",
        "        train_y = torch.LongTensor([label2idx[i] for i in train_y])\n",
        "        test_y = torch.LongTensor([label2idx[i] for i in test_y])\n",
        "\n",
        "        train_x = [os.path.join(self.image_dir, i) for i in train_x]\n",
        "        test_x = [os.path.join(self.image_dir, i) for i in test_x]\n",
        "        train_x = torch.stack([self.transform(x) for x in train_x], dim=0)\n",
        "        test_x = torch.stack([self.transform(x) for x in test_x], dim=0)\n",
        "\n",
        "        return train_x, train_y, test_x, test_y\n",
        "\n",
        "\n",
        "class OmniglotDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, labels, task_size, n_way, k_shot, k_test, transform=None):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.task_size = task_size\n",
        "        self.n_way = n_way\n",
        "        self.k_shot = k_shot\n",
        "        self.k_test = k_test\n",
        "        self.transform = transform\n",
        "        self.image2label = self.data.set_index('file')['label'].to_dict()\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return 320\n",
        "\n",
        "    @staticmethod\n",
        "    def load(root, task_size, n_way, k_shot, k_test, train_transform=None, test_transform=None):\n",
        "        files = glob(os.path.join(root, '*/*/*/*.png'))\n",
        "        labels = ['/'.join(i.split('/')[4:6]) for i in files]\n",
        "        data = pd.DataFrame({'file':files, 'label':labels})\n",
        "\n",
        "        unique_labels = data['label'].unique()\n",
        "        random_idx = np.random.permutation(len(unique_labels))\n",
        "        train_labels = unique_labels[random_idx[:1200]]\n",
        "        test_labels = unique_labels[random_idx[1200:]]\n",
        "\n",
        "        return OmniglotDataset(data, train_labels, task_size, n_way, k_shot, k_test, train_transform), OmniglotDataset(data, test_labels, task_size, n_way, k_shot, k_test, test_transform)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        train_x, test_x = [], []\n",
        "        selected_labels = np.random.choice(self.labels, self.n_way, replace=False)\n",
        "\n",
        "        for label in selected_labels:\n",
        "            images = self.data.loc[self.data['label']==label, 'file'].sample(self.k_shot + self.k_test).tolist()\n",
        "            train_x += images[:self.k_shot]\n",
        "            test_x += images[self.k_shot:]\n",
        "\n",
        "        np.random.shuffle(train_x)\n",
        "        np.random.shuffle(test_x)\n",
        "\n",
        "        train_y = [self.image2label[i] for i in train_x]\n",
        "        test_y = [self.image2label[i] for i in test_x]\n",
        "        relative_label = np.unique(train_y)\n",
        "        np.random.shuffle(relative_label)\n",
        "        label2idx = {i:idx for idx, i in enumerate(relative_label)}\n",
        "        train_y = torch.LongTensor([label2idx[i] for i in train_y])\n",
        "        test_y = torch.LongTensor([label2idx[i] for i in test_y])\n",
        "\n",
        "        train_x = torch.stack([self.transform(x) for x in train_x], dim=0)\n",
        "        test_x = torch.stack([self.transform(x) for x in test_x], dim=0)\n",
        "\n",
        "        return train_x, train_y, test_x, test_y"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSIJSakAgmQi"
      },
      "source": [
        "# data_path = os.path.join(CONFIG['base_dir'], 'data/mini_imagenet.zip')\n",
        "data_path = os.path.join(CONFIG['base_dir'], 'data/omniglot.zip')\n",
        "!unzip -q \"{data_path}\" -d 'data'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7iKtbEMpR-p"
      },
      "source": [
        "train_transform = transforms.Compose([\n",
        "    lambda x : Image.open(x).convert('L'),\n",
        "    transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    lambda x : Image.open(x).convert('L'),\n",
        "    transforms.Resize((CONFIG['image_size'], CONFIG['image_size'])),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkSi2ZxXnFWW"
      },
      "source": [
        "train_data, test_data = OmniglotDataset.load('/content/data', CONFIG['task_size'], CONFIG['n_way'], CONFIG['k_shot'], CONFIG['k_test'], train_transform, test_transform)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXiB_l9l3W7s"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_data, CONFIG['task_size'], shuffle=True, pin_memory=True, drop_last=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, CONFIG['task_size'], shuffle=True, pin_memory=True, drop_last=True)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsXOEAqKt8bC",
        "outputId": "c75897eb-1816-4eba-c30e-80142f1f04f5"
      },
      "source": [
        "train_x, train_y, test_x, test_y = next(iter(train_loader))\n",
        "train_x.size(), train_y.size(), test_x.size(), test_y.size()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 5, 1, 28, 28]),\n",
              " torch.Size([32, 5]),\n",
              " torch.Size([32, 75, 1, 28, 28]),\n",
              " torch.Size([32, 75]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "xpyDWSVnudHr",
        "outputId": "9e68827b-715d-4f78-83df-c03f5af8fbfe"
      },
      "source": [
        "plt.imshow(torchvision.utils.make_grid(train_x[0], nrow=5).permute(1, 2, 0))\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLljDsOoFoLN"
      },
      "source": [
        "## 3. Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opxs2hKBJXr4"
      },
      "source": [
        "class MetaLinear(nn.Linear):\n",
        "    def forward(self, x, params=None):\n",
        "        if params is None:\n",
        "            params = OrderedDict(self.named_parameters())\n",
        "        bias = params.get('bias', None)\n",
        "        return F.linear(x, params['weight'], bias)\n",
        "\n",
        "\n",
        "class MetaConv2d(nn.Conv2d):\n",
        "    def forward(self, x, params=None):\n",
        "        if params is None:\n",
        "            params = OrderedDict(self.named_parameters())\n",
        "        bias = params.get('bias', None)\n",
        "        return F.conv2d(x, params['weight'], bias, self.stride, self.padding, self.dilation, self.groups)\n",
        "\n",
        "\n",
        "class MetaBatchNorm2d(nn.modules.BatchNorm2d):\n",
        "    def forward(self, x, params=None):\n",
        "        if params is None:\n",
        "            params = OrderedDict(self.named_parameters())\n",
        "        \n",
        "        if self.momentum is None:\n",
        "            exponential_average_factor = 0.0\n",
        "        else:\n",
        "            exponential_average_factor = self.momentum\n",
        "        \n",
        "        if self.training and self.track_running_stats:\n",
        "            if self.num_batches_tracked is not None:\n",
        "                self.num_batches_tracked += 1\n",
        "                if self.momentum is None:  # use cumulative moving average\n",
        "                    exponential_average_factor = 1.0 / float(self.num_batches_tracked)\n",
        "                else:  # use exponential moving average\n",
        "                    exponential_average_factor = self.momentum\n",
        "        \n",
        "        weight = params.get('weight', None)\n",
        "        bias = params.get('bias', None)\n",
        "        training = self.training or not self.track_running_stats\n",
        "        return F.batch_norm(x, self.running_mean, self.running_var, weight, bias, training, exponential_average_factor, self.eps)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "If-lc75zWLyI"
      },
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self, n_way):\n",
        "        super(Network, self).__init__()\n",
        "        self.layers = {\n",
        "            'meta_conv1': MetaConv2d(1, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            'meta_bn1': MetaBatchNorm2d(64),\n",
        "            'relu1': nn.ReLU(),\n",
        "            'pool1': nn.MaxPool2d(2),\n",
        "            'meta_conv2': MetaConv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            'meta_bn2': MetaBatchNorm2d(64),\n",
        "            'relu2': nn.ReLU(),\n",
        "            'pool2': nn.MaxPool2d(2),\n",
        "            'meta_conv3': MetaConv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            'meta_bn3': MetaBatchNorm2d(64),\n",
        "            'relu3': nn.ReLU(),\n",
        "            'pool3': nn.MaxPool2d(2),\n",
        "            'meta_conv4': MetaConv2d(64, 64, kernel_size=2, stride=1, padding=0, bias=False),\n",
        "            'meta_bn4': MetaBatchNorm2d(64),\n",
        "            'relu4': nn.ReLU(),\n",
        "            'pool4': nn.MaxPool2d(2),\n",
        "            'flatten': nn.Flatten(),\n",
        "            'meta_linear': MetaLinear(64, n_way)\n",
        "        }\n",
        "        self.layers = nn.ModuleDict(self.layers)\n",
        "    \n",
        "    def get_named_params(self):\n",
        "        return OrderedDict(self.named_parameters())\n",
        "    \n",
        "    def forward(self, x, params={}):\n",
        "        for layer_name, layer in self.layers.items():\n",
        "            if 'meta' in layer_name:\n",
        "                p = {k.split('.')[-1]:v for k,v in params.items() if layer_name in k} if params else None\n",
        "                x = layer(x, params=p)\n",
        "            else:\n",
        "                x = layer(x)\n",
        "        return x"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niEWQOF8qZO2"
      },
      "source": [
        "## 4. Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Al-KpR_Hqa4O"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "        self.avg = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = f'{self.name:10s} {self.avg:.3f}'\n",
        "        return fmtstr\n",
        "\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, meters, loader_length, prefix=\"\"):\n",
        "        self.meters = [AverageMeter(i) for i in meters]\n",
        "        self.loader_length = loader_length\n",
        "        self.prefix = prefix\n",
        "    \n",
        "    def reset(self):\n",
        "        for m in self.meters:\n",
        "            m.reset()\n",
        "    \n",
        "    def update(self, values, n=1):\n",
        "        for m, v in zip(self.meters, values):\n",
        "            m.update(v, n)\n",
        "            self.__setattr__(m.name, m.avg)\n",
        "\n",
        "    def display(self, batch_idx, postfix=\"\"):\n",
        "        batch_info = f'[{batch_idx+1:03d}/{self.loader_length:03d}]'\n",
        "        msg = [self.prefix + ' ' + batch_info]\n",
        "        msg += [str(meter) for meter in self.meters]\n",
        "        msg = ' | '.join(msg)\n",
        "\n",
        "        sys.stdout.write('\\r')\n",
        "        sys.stdout.write(msg + postfix)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "\n",
        "def accuracy(logits, targets):\n",
        "    _, pred = logits.max(1)\n",
        "    acc = pred.eq(targets).float().mean().item()\n",
        "    return acc"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFdlTj8tqdgQ"
      },
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self, model, criterion, optimizer, device):\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "    \n",
        "    def train(self, train_loader, epoch):\n",
        "        progress = ProgressMeter([\"train_loss\", \"train_acc\"], len(train_loader), prefix=f'EPOCH {epoch:03d}')\n",
        "        self.model.train()\n",
        "        start_time = time.time()\n",
        "\n",
        "        for idx, (train_x, train_y, test_x, test_y) in enumerate(train_loader):\n",
        "            train_x, train_y, test_x, test_y = train_x.to(self.device), train_y.to(self.device), test_x.to(self.device), test_y.to(self.device)\n",
        "\n",
        "            meta_loss = torch.tensor(0.).to(device)\n",
        "            meta_acc = torch.tensor(0.).to(device)\n",
        "\n",
        "            for i in range(CONFIG['task_size']):\n",
        "                fast_params = self.model.get_named_params()\n",
        "\n",
        "                for j in range(CONFIG['update_step']):\n",
        "                    logits = self.model(train_x[i], fast_params)\n",
        "                    loss = self.criterion(logits, train_y[i])\n",
        "                    grads = torch.autograd.grad(loss, fast_params.values())\n",
        "                    fast_params = OrderedDict({n:p - CONFIG['update_lr'] * g for (n, p), g in zip(fast_params.items(), grads)})\n",
        "                \n",
        "                test_logits = self.model(test_x[i], fast_params)\n",
        "                test_loss = criterion(test_logits, test_y[i])\n",
        "                meta_loss += test_loss\n",
        "                _, pred = test_logits.max(1)\n",
        "                acc = pred.eq(test_y[i]).float().mean()\n",
        "                meta_acc += acc\n",
        "\n",
        "            meta_loss.div_(CONFIG['task_size'])\n",
        "            meta_acc.div_(CONFIG['task_size'])\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            meta_loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            progress.update([meta_loss.item(), meta_acc.item()], n=CONFIG['task_size'])\n",
        "            if idx % 10 == 0:\n",
        "                progress.display(idx)\n",
        "\n",
        "        finish_time = time.time()\n",
        "        epoch_time = finish_time - start_time\n",
        "        progress.display(idx, f' | {epoch_time:.0f}s' + '\\n')\n",
        "\n",
        "    \n",
        "    def test(self, test_loader, prefix):\n",
        "        progress = ProgressMeter([\"test loss\", \"test acc\"], len(test_loader), prefix=prefix)\n",
        "        self.model.train()\n",
        "        start_time = time.time()\n",
        "\n",
        "        for idx, (train_x, train_y, test_x, test_y) in enumerate(test_loader):\n",
        "            train_x, train_y, test_x, test_y = train_x.to(self.device), train_y.to(self.device), test_x.to(self.device), test_y.to(self.device)\n",
        "\n",
        "            meta_loss = torch.tensor(0.).to(device)\n",
        "            meta_acc = torch.tensor(0.).to(device)\n",
        "\n",
        "            for i in range(CONFIG['task_size']):\n",
        "                fast_params = self.model.get_named_params()\n",
        "\n",
        "                for j in range(CONFIG['finetune_step']):\n",
        "                    logits = self.model(train_x[i], fast_params)\n",
        "                    loss = self.criterion(logits, train_y[i])\n",
        "                    grads = torch.autograd.grad(loss, fast_params.values())\n",
        "                    fast_params = OrderedDict({n:p - CONFIG['update_lr'] * g for (n, p), g in zip(fast_params.items(), grads)})\n",
        "\n",
        "                test_logits = self.model(test_x[i], fast_params)\n",
        "                test_loss = criterion(test_logits, test_y[i])\n",
        "                meta_loss += test_loss\n",
        "\n",
        "                _, pred = test_logits.max(1)\n",
        "                acc = pred.eq(test_y[i]).float().mean()\n",
        "                meta_acc += acc\n",
        "\n",
        "            meta_loss.div_(CONFIG['task_size'])\n",
        "            meta_acc.div_(CONFIG['task_size'])\n",
        "\n",
        "            progress.update([meta_loss.item(), meta_acc.item()], n=CONFIG['task_size'])\n",
        "            if idx % 10 == 0:\n",
        "                progress.display(idx)\n",
        "\n",
        "        finish_time = time.time()\n",
        "        epoch_time = finish_time - start_time\n",
        "        progress.display(idx, f' | {epoch_time:.0f}s' + '\\n')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3kfhSd_VMmL"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = Network(CONFIG['n_way']).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['meta_lr'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdkFS1l938Pu"
      },
      "source": [
        "trainer = Trainer(model, criterion, optimizer, device)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9g_veplre78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f006c9e-8a2c-4c71-c736-601cf4a2a184"
      },
      "source": [
        "for ep in range(CONFIG['epoch_size']):\n",
        "    print('=' * 65)\n",
        "    trainer.train(train_loader, ep)\n",
        "    if ep % 5 == 0:\n",
        "        trainer.test(test_loader, prefix=f'TEST  {ep:03d}')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "EPOCH 000 [010/010] | train_loss 1.315 | train_acc  0.493 | 14s\n",
            "TEST  000 [010/010] | test loss  1.160 | test acc   0.623 | 17s\n",
            "=================================================================\n",
            "EPOCH 001 [010/010] | train_loss 1.234 | train_acc  0.561 | 14s\n",
            "=================================================================\n",
            "EPOCH 002 [010/010] | train_loss 1.132 | train_acc  0.602 | 14s\n",
            "=================================================================\n",
            "EPOCH 003 [010/010] | train_loss 1.091 | train_acc  0.618 | 14s\n",
            "=================================================================\n",
            "EPOCH 004 [010/010] | train_loss 1.062 | train_acc  0.610 | 14s\n",
            "=================================================================\n",
            "EPOCH 005 [010/010] | train_loss 0.970 | train_acc  0.656 | 14s\n",
            "TEST  005 [010/010] | test loss  0.855 | test acc   0.699 | 17s\n",
            "=================================================================\n",
            "EPOCH 006 [010/010] | train_loss 0.813 | train_acc  0.718 | 14s\n",
            "=================================================================\n",
            "EPOCH 007 [010/010] | train_loss 0.708 | train_acc  0.754 | 14s\n",
            "=================================================================\n",
            "EPOCH 008 [010/010] | train_loss 0.659 | train_acc  0.771 | 14s\n",
            "=================================================================\n",
            "EPOCH 009 [010/010] | train_loss 0.603 | train_acc  0.785 | 14s\n",
            "=================================================================\n",
            "EPOCH 010 [010/010] | train_loss 0.522 | train_acc  0.817 | 14s\n",
            "TEST  010 [010/010] | test loss  0.520 | test acc   0.814 | 17s\n",
            "=================================================================\n",
            "EPOCH 011 [010/010] | train_loss 0.482 | train_acc  0.831 | 14s\n",
            "=================================================================\n",
            "EPOCH 012 [010/010] | train_loss 0.458 | train_acc  0.839 | 14s\n",
            "=================================================================\n",
            "EPOCH 013 [010/010] | train_loss 0.462 | train_acc  0.839 | 14s\n",
            "=================================================================\n",
            "EPOCH 014 [010/010] | train_loss 0.448 | train_acc  0.841 | 14s\n",
            "=================================================================\n",
            "EPOCH 015 [010/010] | train_loss 0.464 | train_acc  0.836 | 14s\n",
            "TEST  015 [010/010] | test loss  0.409 | test acc   0.858 | 17s\n",
            "=================================================================\n",
            "EPOCH 016 [010/010] | train_loss 0.441 | train_acc  0.845 | 14s\n",
            "=================================================================\n",
            "EPOCH 017 [010/010] | train_loss 0.432 | train_acc  0.850 | 14s\n",
            "=================================================================\n",
            "EPOCH 018 [010/010] | train_loss 0.404 | train_acc  0.859 | 14s\n",
            "=================================================================\n",
            "EPOCH 019 [010/010] | train_loss 0.384 | train_acc  0.868 | 14s\n",
            "=================================================================\n",
            "EPOCH 020 [010/010] | train_loss 0.346 | train_acc  0.877 | 14s\n",
            "TEST  020 [010/010] | test loss  0.362 | test acc   0.873 | 17s\n",
            "=================================================================\n",
            "EPOCH 021 [010/010] | train_loss 0.356 | train_acc  0.875 | 14s\n",
            "=================================================================\n",
            "EPOCH 022 [010/010] | train_loss 0.386 | train_acc  0.864 | 14s\n",
            "=================================================================\n",
            "EPOCH 023 [010/010] | train_loss 0.355 | train_acc  0.877 | 14s\n",
            "=================================================================\n",
            "EPOCH 024 [010/010] | train_loss 0.342 | train_acc  0.883 | 14s\n",
            "=================================================================\n",
            "EPOCH 025 [010/010] | train_loss 0.363 | train_acc  0.874 | 14s\n",
            "TEST  025 [010/010] | test loss  0.331 | test acc   0.887 | 17s\n",
            "=================================================================\n",
            "EPOCH 026 [010/010] | train_loss 0.359 | train_acc  0.872 | 14s\n",
            "=================================================================\n",
            "EPOCH 027 [010/010] | train_loss 0.340 | train_acc  0.882 | 14s\n",
            "=================================================================\n",
            "EPOCH 028 [010/010] | train_loss 0.330 | train_acc  0.882 | 14s\n",
            "=================================================================\n",
            "EPOCH 029 [010/010] | train_loss 0.341 | train_acc  0.879 | 14s\n",
            "=================================================================\n",
            "EPOCH 030 [010/010] | train_loss 0.373 | train_acc  0.871 | 14s\n",
            "TEST  030 [010/010] | test loss  0.337 | test acc   0.882 | 17s\n",
            "=================================================================\n",
            "EPOCH 031 [010/010] | train_loss 0.350 | train_acc  0.877 | 14s\n",
            "=================================================================\n",
            "EPOCH 032 [010/010] | train_loss 0.346 | train_acc  0.880 | 14s\n",
            "=================================================================\n",
            "EPOCH 033 [010/010] | train_loss 0.333 | train_acc  0.884 | 14s\n",
            "=================================================================\n",
            "EPOCH 034 [010/010] | train_loss 0.329 | train_acc  0.886 | 14s\n",
            "=================================================================\n",
            "EPOCH 035 [010/010] | train_loss 0.331 | train_acc  0.886 | 14s\n",
            "TEST  035 [010/010] | test loss  0.298 | test acc   0.897 | 17s\n",
            "=================================================================\n",
            "EPOCH 036 [010/010] | train_loss 0.315 | train_acc  0.894 | 14s\n",
            "=================================================================\n",
            "EPOCH 037 [010/010] | train_loss 0.282 | train_acc  0.906 | 14s\n",
            "=================================================================\n",
            "EPOCH 038 [010/010] | train_loss 0.296 | train_acc  0.898 | 14s\n",
            "=================================================================\n",
            "EPOCH 039 [010/010] | train_loss 0.281 | train_acc  0.902 | 14s\n",
            "=================================================================\n",
            "EPOCH 040 [010/010] | train_loss 0.319 | train_acc  0.891 | 14s\n",
            "TEST  040 [010/010] | test loss  0.281 | test acc   0.904 | 17s\n",
            "=================================================================\n",
            "EPOCH 041 [010/010] | train_loss 0.265 | train_acc  0.912 | 14s\n",
            "=================================================================\n",
            "EPOCH 042 [010/010] | train_loss 0.286 | train_acc  0.898 | 14s\n",
            "=================================================================\n",
            "EPOCH 043 [010/010] | train_loss 0.274 | train_acc  0.906 | 14s\n",
            "=================================================================\n",
            "EPOCH 044 [010/010] | train_loss 0.267 | train_acc  0.907 | 14s\n",
            "=================================================================\n",
            "EPOCH 045 [010/010] | train_loss 0.257 | train_acc  0.915 | 14s\n",
            "TEST  045 [010/010] | test loss  0.297 | test acc   0.898 | 17s\n",
            "=================================================================\n",
            "EPOCH 046 [010/010] | train_loss 0.299 | train_acc  0.895 | 14s\n",
            "=================================================================\n",
            "EPOCH 047 [010/010] | train_loss 0.303 | train_acc  0.896 | 14s\n",
            "=================================================================\n",
            "EPOCH 048 [010/010] | train_loss 0.313 | train_acc  0.890 | 14s\n",
            "=================================================================\n",
            "EPOCH 049 [010/010] | train_loss 0.274 | train_acc  0.905 | 14s\n",
            "=================================================================\n",
            "EPOCH 050 [010/010] | train_loss 0.298 | train_acc  0.897 | 14s\n",
            "TEST  050 [010/010] | test loss  0.283 | test acc   0.905 | 18s\n",
            "=================================================================\n",
            "EPOCH 051 [010/010] | train_loss 0.279 | train_acc  0.906 | 14s\n",
            "=================================================================\n",
            "EPOCH 052 [010/010] | train_loss 0.261 | train_acc  0.912 | 14s\n",
            "=================================================================\n",
            "EPOCH 053 [010/010] | train_loss 0.246 | train_acc  0.915 | 14s\n",
            "=================================================================\n",
            "EPOCH 054 [010/010] | train_loss 0.264 | train_acc  0.907 | 14s\n",
            "=================================================================\n",
            "EPOCH 055 [010/010] | train_loss 0.252 | train_acc  0.915 | 14s\n",
            "TEST  055 [010/010] | test loss  0.259 | test acc   0.911 | 17s\n",
            "=================================================================\n",
            "EPOCH 056 [010/010] | train_loss 0.229 | train_acc  0.923 | 14s\n",
            "=================================================================\n",
            "EPOCH 057 [010/010] | train_loss 0.239 | train_acc  0.917 | 14s\n",
            "=================================================================\n",
            "EPOCH 058 [010/010] | train_loss 0.273 | train_acc  0.904 | 14s\n",
            "=================================================================\n",
            "EPOCH 059 [010/010] | train_loss 0.284 | train_acc  0.903 | 14s\n",
            "=================================================================\n",
            "EPOCH 060 [010/010] | train_loss 0.261 | train_acc  0.909 | 14s\n",
            "TEST  060 [010/010] | test loss  0.274 | test acc   0.906 | 17s\n",
            "=================================================================\n",
            "EPOCH 061 [010/010] | train_loss 0.252 | train_acc  0.916 | 14s\n",
            "=================================================================\n",
            "EPOCH 062 [010/010] | train_loss 0.275 | train_acc  0.904 | 14s\n",
            "=================================================================\n",
            "EPOCH 063 [010/010] | train_loss 0.251 | train_acc  0.915 | 14s\n",
            "=================================================================\n",
            "EPOCH 064 [010/010] | train_loss 0.225 | train_acc  0.924 | 14s\n",
            "=================================================================\n",
            "EPOCH 065 [010/010] | train_loss 0.257 | train_acc  0.914 | 14s\n",
            "TEST  065 [010/010] | test loss  0.284 | test acc   0.904 | 17s\n",
            "=================================================================\n",
            "EPOCH 066 [010/010] | train_loss 0.223 | train_acc  0.923 | 14s\n",
            "=================================================================\n",
            "EPOCH 067 [010/010] | train_loss 0.205 | train_acc  0.932 | 15s\n",
            "=================================================================\n",
            "EPOCH 068 [010/010] | train_loss 0.218 | train_acc  0.923 | 14s\n",
            "=================================================================\n",
            "EPOCH 069 [010/010] | train_loss 0.233 | train_acc  0.921 | 15s\n",
            "=================================================================\n",
            "EPOCH 070 [010/010] | train_loss 0.228 | train_acc  0.924 | 14s\n",
            "TEST  070 [010/010] | test loss  0.249 | test acc   0.917 | 17s\n",
            "=================================================================\n",
            "EPOCH 071 [010/010] | train_loss 0.243 | train_acc  0.914 | 14s\n",
            "=================================================================\n",
            "EPOCH 072 [010/010] | train_loss 0.230 | train_acc  0.921 | 14s\n",
            "=================================================================\n",
            "EPOCH 073 [010/010] | train_loss 0.225 | train_acc  0.922 | 14s\n",
            "=================================================================\n",
            "EPOCH 074 [010/010] | train_loss 0.236 | train_acc  0.918 | 14s\n",
            "=================================================================\n",
            "EPOCH 075 [010/010] | train_loss 0.228 | train_acc  0.920 | 14s\n",
            "TEST  075 [010/010] | test loss  0.261 | test acc   0.914 | 17s\n",
            "=================================================================\n",
            "EPOCH 076 [010/010] | train_loss 0.234 | train_acc  0.924 | 14s\n",
            "=================================================================\n",
            "EPOCH 077 [010/010] | train_loss 0.236 | train_acc  0.919 | 14s\n",
            "=================================================================\n",
            "EPOCH 078 [010/010] | train_loss 0.224 | train_acc  0.927 | 14s\n",
            "=================================================================\n",
            "EPOCH 079 [010/010] | train_loss 0.246 | train_acc  0.919 | 14s\n",
            "=================================================================\n",
            "EPOCH 080 [010/010] | train_loss 0.233 | train_acc  0.922 | 14s\n",
            "TEST  080 [010/010] | test loss  0.277 | test acc   0.912 | 18s\n",
            "=================================================================\n",
            "EPOCH 081 [010/010] | train_loss 0.238 | train_acc  0.921 | 15s\n",
            "=================================================================\n",
            "EPOCH 082 [010/010] | train_loss 0.238 | train_acc  0.919 | 15s\n",
            "=================================================================\n",
            "EPOCH 083 [010/010] | train_loss 0.215 | train_acc  0.928 | 15s\n",
            "=================================================================\n",
            "EPOCH 084 [010/010] | train_loss 0.235 | train_acc  0.920 | 14s\n",
            "=================================================================\n",
            "EPOCH 085 [010/010] | train_loss 0.236 | train_acc  0.920 | 14s\n",
            "TEST  085 [010/010] | test loss  0.274 | test acc   0.910 | 18s\n",
            "=================================================================\n",
            "EPOCH 086 [010/010] | train_loss 0.228 | train_acc  0.924 | 15s\n",
            "=================================================================\n",
            "EPOCH 087 [010/010] | train_loss 0.215 | train_acc  0.926 | 14s\n",
            "=================================================================\n",
            "EPOCH 088 [010/010] | train_loss 0.227 | train_acc  0.922 | 14s\n",
            "=================================================================\n",
            "EPOCH 089 [010/010] | train_loss 0.250 | train_acc  0.915 | 14s\n",
            "=================================================================\n",
            "EPOCH 090 [010/010] | train_loss 0.244 | train_acc  0.917 | 15s\n",
            "TEST  090 [010/010] | test loss  0.286 | test acc   0.906 | 18s\n",
            "=================================================================\n",
            "EPOCH 091 [010/010] | train_loss 0.231 | train_acc  0.923 | 14s\n",
            "=================================================================\n",
            "EPOCH 092 [010/010] | train_loss 0.284 | train_acc  0.906 | 14s\n",
            "=================================================================\n",
            "EPOCH 093 [010/010] | train_loss 0.272 | train_acc  0.906 | 14s\n",
            "=================================================================\n",
            "EPOCH 094 [010/010] | train_loss 0.245 | train_acc  0.916 | 14s\n",
            "=================================================================\n",
            "EPOCH 095 [010/010] | train_loss 0.253 | train_acc  0.915 | 14s\n",
            "TEST  095 [010/010] | test loss  0.276 | test acc   0.908 | 18s\n",
            "=================================================================\n",
            "EPOCH 096 [010/010] | train_loss 0.240 | train_acc  0.920 | 14s\n",
            "=================================================================\n",
            "EPOCH 097 [010/010] | train_loss 0.247 | train_acc  0.916 | 15s\n",
            "=================================================================\n",
            "EPOCH 098 [010/010] | train_loss 0.245 | train_acc  0.921 | 14s\n",
            "=================================================================\n",
            "EPOCH 099 [010/010] | train_loss 0.244 | train_acc  0.921 | 14s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhyQw7zyFq8B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b02fc16-e9cb-4e06-c6fd-8ccb0ca48706"
      },
      "source": [
        "trainer.test(test_loader, prefix='TEST')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TEST [010/010] | test loss  0.254 | test acc   0.914 | 17s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plyP-mLUljJ0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}