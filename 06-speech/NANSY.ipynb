{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yongsun-yoon/deep-learning-paper-implementation/blob/main/06-speech/NANSY.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2183de15-abcb-4bef-b4b9-e2840e480989",
      "metadata": {
        "id": "2183de15-abcb-4bef-b4b9-e2840e480989"
      },
      "source": [
        "# NANSY"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db955bc2-8294-4420-966c-daa6512288e1",
      "metadata": {
        "id": "db955bc2-8294-4420-966c-daa6512288e1"
      },
      "source": [
        "## 0. Info"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be1ffbf7-4b50-4cda-a614-94c47ca83558",
      "metadata": {
        "id": "be1ffbf7-4b50-4cda-a614-94c47ca83558"
      },
      "source": [
        "### paper\n",
        "* title: Neural Analysis and Synthesis: Reconstructing Speech from Self-Supervised Representations\n",
        "* authors: Hyeong-Seok Choi et al.\n",
        "* url: https://arxiv.org/abs/2110.14513\n",
        "\n",
        "### feats\n",
        "* dataset: AI Hub\n",
        "\n",
        "### refs\n",
        "* https://github.com/dhchoi99/NANSY\n",
        "* https://github.com/jik876/hifi-gan"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2d57470-93bc-4736-b616-c9ca03c139d7",
      "metadata": {
        "tags": [],
        "id": "b2d57470-93bc-4736-b616-c9ca03c139d7"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebb1b705-0887-4c2e-aa07-0aa83ddda9dc",
      "metadata": {
        "id": "ebb1b705-0887-4c2e-aa07-0aa83ddda9dc"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c33e988-c6c2-4407-a87f-a4135f217f1e",
      "metadata": {
        "id": "6c33e988-c6c2-4407-a87f-a4135f217f1e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import wandb\n",
        "import random\n",
        "import IPython\n",
        "import easydict\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from tqdm.auto import tqdm\n",
        "from omegaconf import OmegaConf, DictConfig\n",
        "\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from librosa.util import normalize\n",
        "from librosa.filters import mel as librosa_mel_fn\n",
        "import parselmouth\n",
        "import scipy.signal\n",
        "import torchaudio.functional as AF\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import Wav2Vec2ForPreTraining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f65776d-ee63-4a0c-a4b4-fa4b3f772876",
      "metadata": {
        "id": "6f65776d-ee63-4a0c-a4b4-fa4b3f772876"
      },
      "outputs": [],
      "source": [
        "cfg = easydict.EasyDict(\n",
        "    device = 'cuda:2'\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c592809-795b-46a4-bb68-8094d6f91ab8",
      "metadata": {
        "tags": [],
        "id": "0c592809-795b-46a4-bb68-8094d6f91ab8"
      },
      "source": [
        "## 2. Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "727dfbfe-53e0-4130-8788-96336d3c077c",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "727dfbfe-53e0-4130-8788-96336d3c077c"
      },
      "source": [
        "### 2.1. functional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6b1dc40-d11f-490d-91c8-47bc32b159c6",
      "metadata": {
        "id": "f6b1dc40-d11f-490d-91c8-47bc32b159c6"
      },
      "outputs": [],
      "source": [
        "PRAAT_CHANGEGENDER_PITCHMEDIAN_DEFAULT = 0.0\n",
        "PRAAT_CHANGEGENDER_FORMANTSHIFTRATIO_DEFAULT = 1.0\n",
        "PRAAT_CHANGEGENDER_PITCHSHIFTRATIO_DEFAULT = 1.0\n",
        "PRAAT_CHANGEGENDER_PITCHRANGERATIO_DEFAULT = 1.0\n",
        "PRAAT_CHANGEGENDER_DURATIONFACTOR_DEFAULT = 1.0\n",
        "\n",
        "\n",
        "def wav_to_Sound(wav, sampling_frequency: int = 22050) -> parselmouth.Sound:\n",
        "    r\"\"\" load wav file to parselmouth Sound file\n",
        "    # __init__(self: parselmouth.Sound, other: parselmouth.Sound) -> None \\\n",
        "    # __init__(self: parselmouth.Sound, values: numpy.ndarray[numpy.float64], sampling_frequency: Positive[float] = 44100.0, start_time: float = 0.0) -> None \\\n",
        "    # __init__(self: parselmouth.Sound, file_path: str) -> None\n",
        "    returns:\n",
        "        sound: parselmouth.Sound\n",
        "    \"\"\"\n",
        "    if isinstance(wav, parselmouth.Sound):\n",
        "        sound = wav\n",
        "    elif isinstance(wav, np.ndarray):\n",
        "        sound = parselmouth.Sound(wav, sampling_frequency=sampling_frequency)\n",
        "    elif isinstance(wav, list):\n",
        "        wav_np = np.asarray(wav)\n",
        "        sound = parselmouth.Sound(np.asarray(wav_np), sampling_frequency=sampling_frequency)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    return sound\n",
        "\n",
        "\n",
        "def wav_to_Tensor(wav) -> torch.Tensor:\n",
        "    if isinstance(wav, np.ndarray):\n",
        "        wav_tensor = torch.from_numpy(wav)\n",
        "    elif isinstance(wav, torch.Tensor):\n",
        "        wav_tensor = wav\n",
        "    elif isinstance(wav, parselmouth.Sound):\n",
        "        wav_np = wav.values\n",
        "        wav_tensor = torch.from_numpy(wav_np)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    return wav_tensor\n",
        "\n",
        "\n",
        "def get_pitch_median(wav, sr: int = None):\n",
        "    sound = wav_to_Sound(wav, sr)\n",
        "    pitch = None\n",
        "    pitch_median = PRAAT_CHANGEGENDER_PITCHMEDIAN_DEFAULT\n",
        "\n",
        "    try:\n",
        "        pitch = parselmouth.praat.call(sound, \"To Pitch\", 0.8 / 75, 75, 600)\n",
        "        pitch_median = parselmouth.praat.call(pitch, \"Get quantile\", 0.0, 0.0, 0.5, \"Hertz\")\n",
        "    except Exception as e:\n",
        "        raise e\n",
        "        pass\n",
        "\n",
        "    return pitch, pitch_median\n",
        "\n",
        "\n",
        "def change_gender(\n",
        "        sound, pitch=None,\n",
        "        formant_shift_ratio: float = PRAAT_CHANGEGENDER_FORMANTSHIFTRATIO_DEFAULT,\n",
        "        new_pitch_median: float = PRAAT_CHANGEGENDER_PITCHMEDIAN_DEFAULT,\n",
        "        pitch_range_ratio: float = PRAAT_CHANGEGENDER_PITCHRANGERATIO_DEFAULT,\n",
        "        duration_factor: float = PRAAT_CHANGEGENDER_DURATIONFACTOR_DEFAULT, ) -> parselmouth.Sound:\n",
        "    try:\n",
        "        if pitch is None:\n",
        "            new_sound = parselmouth.praat.call(\n",
        "                sound, \"Change gender\", 75, 600,\n",
        "                formant_shift_ratio,\n",
        "                new_pitch_median,\n",
        "                pitch_range_ratio,\n",
        "                duration_factor\n",
        "            )\n",
        "        else:\n",
        "            new_sound = parselmouth.praat.call(\n",
        "                (sound, pitch), \"Change gender\",\n",
        "                formant_shift_ratio,\n",
        "                new_pitch_median,\n",
        "                pitch_range_ratio,\n",
        "                duration_factor\n",
        "            )\n",
        "    except Exception as e:\n",
        "        raise e\n",
        "\n",
        "    return new_sound\n",
        "\n",
        "\n",
        "def apply_formant_and_pitch_shift(\n",
        "        sound: parselmouth.Sound,\n",
        "        formant_shift_ratio: float = PRAAT_CHANGEGENDER_FORMANTSHIFTRATIO_DEFAULT,\n",
        "        pitch_shift_ratio: float = PRAAT_CHANGEGENDER_PITCHSHIFTRATIO_DEFAULT,\n",
        "        pitch_range_ratio: float = PRAAT_CHANGEGENDER_PITCHRANGERATIO_DEFAULT,\n",
        "        duration_factor: float = PRAAT_CHANGEGENDER_DURATIONFACTOR_DEFAULT) -> parselmouth.Sound:\n",
        "    r\"\"\"uses praat 'Change Gender' backend to manipulate pitch and formant\n",
        "        'Change Gender' function: praat -> Sound Object -> Convert -> Change Gender\n",
        "        see Help of Praat for more details\n",
        "        # https://github.com/YannickJadoul/Parselmouth/issues/25#issuecomment-608632887 might help\n",
        "    \"\"\"\n",
        "\n",
        "    # pitch = sound.to_pitch()\n",
        "    pitch = None\n",
        "    new_pitch_median = PRAAT_CHANGEGENDER_PITCHMEDIAN_DEFAULT\n",
        "    if pitch_shift_ratio != 1.:\n",
        "        try:\n",
        "            pitch, pitch_median = get_pitch_median(sound, None)\n",
        "            new_pitch_median = pitch_median * pitch_shift_ratio\n",
        "\n",
        "            # https://github.com/praat/praat/issues/1926#issuecomment-974909408\n",
        "            pitch_minimum = parselmouth.praat.call(pitch, \"Get minimum\", 0.0, 0.0, \"Hertz\", \"Parabolic\")\n",
        "            newMedian = pitch_median * pitch_shift_ratio\n",
        "            scaledMinimum = pitch_minimum * pitch_shift_ratio\n",
        "            resultingMinimum = newMedian + (scaledMinimum - newMedian) * pitch_range_ratio\n",
        "            if resultingMinimum < 0:\n",
        "                new_pitch_median = PRAAT_CHANGEGENDER_PITCHMEDIAN_DEFAULT\n",
        "                pitch_range_ratio = PRAAT_CHANGEGENDER_PITCHRANGERATIO_DEFAULT\n",
        "\n",
        "            if math.isnan(new_pitch_median):\n",
        "                new_pitch_median = PRAAT_CHANGEGENDER_PITCHMEDIAN_DEFAULT\n",
        "                pitch_range_ratio = PRAAT_CHANGEGENDER_PITCHRANGERATIO_DEFAULT\n",
        "\n",
        "        except Exception as e:\n",
        "            raise e\n",
        "\n",
        "    new_sound = change_gender(\n",
        "        sound, pitch,\n",
        "        formant_shift_ratio, new_pitch_median,\n",
        "        pitch_range_ratio, duration_factor)\n",
        "\n",
        "    return new_sound\n",
        "\n",
        "\n",
        "# fs & pr\n",
        "def formant_and_pitch_shift(sound: parselmouth.Sound) -> parselmouth.Sound:\n",
        "    r\"\"\"calculate random factors and apply formant and pitch shift\n",
        "    designed for formant shifting(fs) and pitch randomization(pr) in the paper\n",
        "    \"\"\"\n",
        "    formant_shifting_ratio = random.uniform(1, 1.4)\n",
        "    use_reciprocal = random.uniform(-1, 1) > 0\n",
        "    if use_reciprocal:\n",
        "        formant_shifting_ratio = 1 / formant_shifting_ratio\n",
        "\n",
        "    pitch_shift_ratio = random.uniform(1, 2)\n",
        "    use_reciprocal = random.uniform(-1, 1) > 0\n",
        "    if use_reciprocal:\n",
        "        pitch_shift_ratio = 1 / pitch_shift_ratio\n",
        "\n",
        "    pitch_range_ratio = random.uniform(1, 1.5)\n",
        "    use_reciprocal = random.uniform(-1, 1) > 0\n",
        "    if use_reciprocal:\n",
        "        pitch_range_ratio = 1 / pitch_range_ratio\n",
        "\n",
        "    sound_new = apply_formant_and_pitch_shift(\n",
        "        sound,\n",
        "        formant_shift_ratio=formant_shifting_ratio,\n",
        "        pitch_shift_ratio=pitch_shift_ratio,\n",
        "        pitch_range_ratio=pitch_range_ratio,\n",
        "        duration_factor=1.\n",
        "    )\n",
        "    return sound_new\n",
        "\n",
        "\n",
        "# fs\n",
        "def formant_shift(sound: parselmouth.Sound) -> parselmouth.Sound:\n",
        "    \"\"\"designed for formant shifting(fs) in the paper\n",
        "    Args:\n",
        "        sound: parselmouth Sound object\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    formant_shifting_ratio = random.uniform(1, 1.4)\n",
        "    use_reciprocal = random.uniform(-1, 1) > 0\n",
        "    if use_reciprocal:\n",
        "        formant_shifting_ratio = 1 / formant_shifting_ratio\n",
        "\n",
        "    sound_new = apply_formant_and_pitch_shift(\n",
        "        sound,\n",
        "        formant_shift_ratio=formant_shifting_ratio,\n",
        "    )\n",
        "    return sound_new\n",
        "\n",
        "\n",
        "def power_ratio(r: float, a: float, b: float):\n",
        "    return a * math.pow((b / a), r)\n",
        "\n",
        "\n",
        "# peq\n",
        "def parametric_equalizer(wav: torch.Tensor, sr: int) -> torch.Tensor:\n",
        "    cutoff_low_freq = 60.\n",
        "    cutoff_high_freq = 10000.\n",
        "\n",
        "    q_min = 2\n",
        "    q_max = 5\n",
        "\n",
        "    num_filters = 8 + 2  # 8 for peak, 2 for high/low\n",
        "    key_freqs = [\n",
        "        power_ratio(float(z) / (num_filters), cutoff_low_freq, cutoff_high_freq)\n",
        "        for z in range(num_filters)\n",
        "    ]\n",
        "    Qs = [\n",
        "        power_ratio(random.uniform(0, 1), q_min, q_max)\n",
        "        for _ in range(num_filters)\n",
        "    ]\n",
        "    gains = [random.uniform(-12, 12) for _ in range(num_filters)]\n",
        "\n",
        "    # peak filters\n",
        "    for i in range(1, 9):\n",
        "        wav = apply_iir_filter(\n",
        "            wav,\n",
        "            ftype='peak',\n",
        "            dBgain=gains[i],\n",
        "            cutoff_freq=key_freqs[i],\n",
        "            sample_rate=sr,\n",
        "            Q=Qs[i]\n",
        "        )\n",
        "\n",
        "    # high-shelving filter\n",
        "    wav = apply_iir_filter(\n",
        "        wav,\n",
        "        ftype='high',\n",
        "        dBgain=gains[-1],\n",
        "        cutoff_freq=key_freqs[-1],\n",
        "        sample_rate=sr,\n",
        "        Q=Qs[-1]\n",
        "    )\n",
        "\n",
        "    # low-shelving filter\n",
        "    wav = apply_iir_filter(\n",
        "        wav,\n",
        "        ftype='low',\n",
        "        dBgain=gains[0],\n",
        "        cutoff_freq=key_freqs[0],\n",
        "        sample_rate=sr,\n",
        "        Q=Qs[0]\n",
        "    )\n",
        "\n",
        "    return wav\n",
        "\n",
        "\n",
        "# implemented using the cookbook https://webaudio.github.io/Audio-EQ-Cookbook/audio-eq-cookbook.html\n",
        "def lowShelf_coeffs(dBgain, cutoff_freq, sample_rate, Q):\n",
        "    A = math.pow(10, dBgain / 40.)\n",
        "\n",
        "    w0 = 2 * math.pi * cutoff_freq / sample_rate\n",
        "    alpha = math.sin(w0) / 2 / Q\n",
        "    # alpha = alpha / math.sqrt(2) * math.sqrt(A + 1 / A)\n",
        "\n",
        "    b0 = A * ((A + 1) - (A - 1) * math.cos(w0) + 2 * math.sqrt(A) * alpha)\n",
        "    b1 = 2 * A * ((A - 1) - (A + 1) * math.cos(w0))\n",
        "    b2 = A * ((A + 1) - (A - 1) * math.cos(w0) - 2 * math.sqrt(A) * alpha)\n",
        "\n",
        "    a0 = (A + 1) + (A - 1) * math.cos(w0) + 2 * math.sqrt(A) * alpha\n",
        "    a1 = -2 * ((A - 1) + (A + 1) * math.cos(w0))\n",
        "    a2 = (A + 1) + (A - 1) * math.cos(w0) - 2 * math.sqrt(A) * alpha\n",
        "    return b0, b1, b2, a0, a1, a2\n",
        "\n",
        "\n",
        "def highShelf_coeffs(dBgain, cutoff_freq, sample_rate, Q):\n",
        "    A = math.pow(10, dBgain / 40.)\n",
        "\n",
        "    w0 = 2 * math.pi * cutoff_freq / sample_rate\n",
        "    alpha = math.sin(w0) / 2 / Q\n",
        "    # alpha = alpha / math.sqrt(2) * math.sqrt(A + 1 / A)\n",
        "\n",
        "    b0 = A * ((A + 1) + (A - 1) * math.cos(w0) + 2 * math.sqrt(A) * alpha)\n",
        "    b1 = -2 * A * ((A - 1) + (A + 1) * math.cos(w0))\n",
        "    b2 = A * ((A + 1) + (A - 1) * math.cos(w0) - 2 * math.sqrt(A) * alpha)\n",
        "\n",
        "    a0 = (A + 1) - (A - 1) * math.cos(w0) + 2 * math.sqrt(A) * alpha\n",
        "    a1 = 2 * ((A - 1) - (A + 1) * math.cos(w0))\n",
        "    a2 = (A + 1) - (A - 1) * math.cos(w0) - 2 * math.sqrt(A) * alpha\n",
        "    return b0, b1, b2, a0, a1, a2\n",
        "\n",
        "\n",
        "def peaking_coeffs(dBgain, cutoff_freq, sample_rate, Q):\n",
        "    A = math.pow(10, dBgain / 40.)\n",
        "\n",
        "    w0 = 2 * math.pi * cutoff_freq / sample_rate\n",
        "    alpha = math.sin(w0) / 2 / Q\n",
        "    # alpha = alpha / math.sqrt(2) * math.sqrt(A + 1 / A)\n",
        "\n",
        "    b0 = 1 + alpha * A\n",
        "    b1 = -2 * math.cos(w0)\n",
        "    b2 = 1 - alpha * A\n",
        "\n",
        "    a0 = 1 + alpha / A\n",
        "    a1 = -2 * math.cos(w0)\n",
        "    a2 = 1 - alpha / A\n",
        "    return b0, b1, b2, a0, a1, a2\n",
        "\n",
        "\n",
        "def apply_iir_filter(wav: torch.Tensor, ftype, dBgain, cutoff_freq, sample_rate, Q, torch_backend=True):\n",
        "    if ftype == 'low':\n",
        "        b0, b1, b2, a0, a1, a2 = lowShelf_coeffs(dBgain, cutoff_freq, sample_rate, Q)\n",
        "    elif ftype == 'high':\n",
        "        b0, b1, b2, a0, a1, a2 = highShelf_coeffs(dBgain, cutoff_freq, sample_rate, Q)\n",
        "    elif ftype == 'peak':\n",
        "        b0, b1, b2, a0, a1, a2 = peaking_coeffs(dBgain, cutoff_freq, sample_rate, Q)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    if torch_backend:\n",
        "        return_wav = AF.biquad(wav, b0, b1, b2, a0, a1, a2)\n",
        "    else:\n",
        "        # https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.lfilter_zi.html\n",
        "        wav_numpy = wav.numpy()\n",
        "        b = np.asarray([b0, b1, b2])\n",
        "        a = np.asarray([a0, a1, a2])\n",
        "        zi = scipy.signal.lfilter_zi(b, a) * wav_numpy[0]\n",
        "        return_wav, _ = scipy.signal.lfilter(b, a, wav_numpy, zi=zi)\n",
        "        return_wav = torch.from_numpy(return_wav)\n",
        "    return return_wav\n",
        "\n",
        "\n",
        "peq = parametric_equalizer\n",
        "fs = formant_shift\n",
        "\n",
        "\n",
        "def g(wav: torch.Tensor, sr: int) -> torch.Tensor:\n",
        "    r\"\"\"sequentially apply peq and fs\n",
        "    \"\"\"\n",
        "    wav = peq(wav, sr)\n",
        "    wav_numpy = wav.numpy()\n",
        "\n",
        "    sound = wav_to_Sound(wav_numpy, sampling_frequency=sr)\n",
        "    sound = formant_shift(sound)\n",
        "\n",
        "    wav = torch.from_numpy(sound.values).float().squeeze(0)\n",
        "    return wav\n",
        "\n",
        "\n",
        "def f(wav: torch.Tensor, sr: int) -> torch.Tensor:\n",
        "    r\"\"\"sequentially apply peq, pr and fs\n",
        "    \"\"\"\n",
        "    wav = peq(wav, sr)\n",
        "    wav_numpy = wav.numpy()\n",
        "\n",
        "    sound = wav_to_Sound(wav_numpy, sampling_frequency=sr)\n",
        "    sound = formant_and_pitch_shift(sound)\n",
        "\n",
        "    wav = torch.from_numpy(sound.values).float().squeeze(0)\n",
        "    return wav"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d257851-c065-447c-928d-230249493678",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "5d257851-c065-447c-928d-230249493678"
      },
      "source": [
        "### 2.2. mel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09951883-f605-413d-8f7e-dcc958d32b2c",
      "metadata": {
        "id": "09951883-f605-413d-8f7e-dcc958d32b2c"
      },
      "outputs": [],
      "source": [
        "MAX_WAV_VALUE = 32768.0\n",
        "mel_basis = {}\n",
        "hann_window = {}\n",
        "\n",
        "def dynamic_range_compression(x, C=1, clip_val=1e-5):\n",
        "    return np.log(np.clip(x, a_min=clip_val, a_max=None) * C)\n",
        "\n",
        "\n",
        "def dynamic_range_decompression(x, C=1):\n",
        "    return np.exp(x) / C\n",
        "\n",
        "\n",
        "def dynamic_range_compression_torch(x, C=1, clip_val=1e-5):\n",
        "    return torch.log(torch.clamp(x, min=clip_val) * C)\n",
        "    # return torch.clamp(x, min=clip_val) * C\n",
        "\n",
        "\n",
        "def dynamic_range_decompression_torch(x, C=1):\n",
        "    return torch.exp(x) / C\n",
        "\n",
        "\n",
        "def spectral_normalize_torch(magnitudes):\n",
        "    output = dynamic_range_compression_torch(magnitudes)\n",
        "    return output\n",
        "\n",
        "\n",
        "def spectral_de_normalize_torch(magnitudes):\n",
        "    output = dynamic_range_decompression_torch(magnitudes)\n",
        "    return output\n",
        "\n",
        "\n",
        "def mel_spectrogram(y, n_fft=1024, num_mels=80, sampling_rate=22050, hop_size=256, win_size=1024, fmin=0, fmax=8000, center=False):\n",
        "    global mel_basis, hann_window\n",
        "    if fmax not in mel_basis:\n",
        "        mel = librosa_mel_fn(sr=sampling_rate, n_fft=n_fft, n_mels=num_mels, fmin=fmin, fmax=fmax)\n",
        "        mel_basis[str(fmax) + '_' + str(y.device)] = torch.from_numpy(mel).float().to(y.device)\n",
        "        hann_window[str(y.device)] = torch.hann_window(win_size).to(y.device)\n",
        "\n",
        "    y = torch.nn.functional.pad(y.unsqueeze(1), (int((n_fft - hop_size) / 2), int((n_fft - hop_size) / 2)),\n",
        "                                mode='reflect')\n",
        "    y = y.squeeze(1)\n",
        "\n",
        "    spec = torch.stft(y, n_fft=n_fft, hop_length=hop_size, win_length=win_size, window=hann_window[str(y.device)],\n",
        "                      center=center, pad_mode='reflect', normalized=False, onesided=True)\n",
        "\n",
        "    spec = torch.sqrt(spec.pow(2).sum(-1) + (1e-9))\n",
        "\n",
        "    spec = torch.matmul(mel_basis[str(fmax) + '_' + str(y.device)], spec)\n",
        "    spec = spectral_normalize_torch(spec)\n",
        "\n",
        "    return spec"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56bf4d0b-17a5-47b3-813e-f6ed8d7847c9",
      "metadata": {
        "id": "56bf4d0b-17a5-47b3-813e-f6ed8d7847c9"
      },
      "source": [
        "### 2.3. dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d1aa934-baf7-4049-8306-58adf118ec9b",
      "metadata": {
        "id": "3d1aa934-baf7-4049-8306-58adf118ec9b"
      },
      "outputs": [],
      "source": [
        "def pad_audio(x: torch.Tensor, length: int, value: float = 0., pad_at: str = 'end') -> torch.Tensor:\n",
        "    r\"\"\"pads value to audio data, at last dimension\n",
        "    params:\n",
        "        x: torch.Tensor of shape (..., T)\n",
        "        length: int, length to pad\n",
        "        value: float, value to pad\n",
        "        pad_at: str, 'start' or 'end'\n",
        "    returns:\n",
        "        y: padded torch.Tensor of shape (..., T+length)\n",
        "    \"\"\"\n",
        "    # x: (..., T)\n",
        "    pad_at = pad_at.strip().lower()\n",
        "    if pad_at == 'end':\n",
        "        y = torch.cat([\n",
        "            x, torch.ones(*x.shape[:-1], length) * value\n",
        "        ], dim=-1)\n",
        "    elif pad_at == 'start':\n",
        "        y = torch.cat([\n",
        "            torch.ones(*x.shape[:-1], length) * value, x\n",
        "        ], dim=-1)\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    return y\n",
        "\n",
        "\n",
        "def crop_audio(x: torch.Tensor, start: int, end: int, padding_value: float = 0.) -> torch.Tensor:\n",
        "    r\"\"\"crop audio data at last dimension from start to end, automatically pad with padding_value\n",
        "    params:\n",
        "        x: torch.Tensor of shape (..., T)\n",
        "        start: int, position to crop\n",
        "        end: int, position to crop\n",
        "        padding_value: float, value for padding when needed\n",
        "    returns:\n",
        "        y: torch.Tensor of shape (..., end-start)\n",
        "    \"\"\"\n",
        "    if start < 0:\n",
        "        if end < 0:\n",
        "            y = torch.ones(size=(*x.shape[:-1], end - start), dtype=torch.float, device=x.device) * padding_value\n",
        "        elif end > x.shape[-1]:\n",
        "            y = x\n",
        "            y = pad_audio(y, -start, padding_value, pad_at='start')\n",
        "            y = pad_audio(y, end - x.shape[-1], padding_value, pad_at='end')\n",
        "        else:\n",
        "            y = x[..., :end]\n",
        "            y = pad_audio(y, -start, padding_value, pad_at='start')\n",
        "    elif end > x.shape[-1]:\n",
        "        if start > x.shape[-1]:\n",
        "            y = torch.ones(size=(*x.shape[:-1], end - start), dtype=torch.float, device=x.device) * padding_value\n",
        "        else:\n",
        "            y = x[..., start:]\n",
        "            y = pad_audio(y, end - x.shape[-1], padding_value, pad_at='end')\n",
        "    else:\n",
        "        y = x[..., start:end]\n",
        "    assert y.shape[-1] == end - start, f'{x.shape}, {start}, {end}, {y.shape}'\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70cbe3d5-6b38-4bf3-aac2-0e2bb1312afa",
      "metadata": {
        "id": "70cbe3d5-6b38-4bf3-aac2-0e2bb1312afa"
      },
      "outputs": [],
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    yin_window_22k = 34816\n",
        "    minimum_audio_length = yin_window_22k\n",
        "    minimum_mel_length = 136\n",
        "    mel_window = 128\n",
        "    hop_size = 256\n",
        "    audio_window_22k = 32768 # mel_window * hop_size\n",
        "    audio_window_16k = 23778\n",
        "    \n",
        "    \n",
        "    def __init__(self, datadir, speaker_to_files={}):\n",
        "        self.datadir = datadir\n",
        "        self.speakers = os.listdir(datadir)\n",
        "        self.speaker_to_files = speaker_to_files\n",
        "    \n",
        "    def __len__(self):\n",
        "        return 1000\n",
        "    \n",
        "    def get_files(self, speaker):\n",
        "        files = self.speaker_to_files.get(speaker)\n",
        "        if files is None:\n",
        "            files = glob(f'{self.datadir}/{speaker}/*/*.wav')\n",
        "            self.speaker_to_files[speaker] = files\n",
        "        return files\n",
        "    \n",
        "    \n",
        "    def get_time_idxs(self, mel_start: int):\n",
        "        mel_end = mel_start + self.mel_window\n",
        "        t_start = mel_start * self.hop_size / 22050.\n",
        "        w_start_22k = int(t_start * 22050)\n",
        "        w_start_16k = int(t_start * 16000)\n",
        "        w_end_22k = w_start_22k + self.audio_window_22k\n",
        "        w_end_22k_yin = w_start_22k + self.yin_window_22k\n",
        "        w_end_16k = w_start_16k + self.audio_window_16k\n",
        "        return mel_start, mel_end, t_start, w_start_16k, w_start_22k, w_end_16k, w_end_22k, w_end_22k_yin\n",
        "    \n",
        "    \n",
        "    def get_pos(self, speaker):\n",
        "        data = {}\n",
        "        files = self.get_files(speaker)\n",
        "        file = np.random.choice(files)\n",
        "        \n",
        "        wav_22k, sr = librosa.load(file, sr=22050)\n",
        "        wav_16k = librosa.resample(wav_22k, orig_sr=22050, target_sr=16000)\n",
        "\n",
        "        wav_22k = torch.from_numpy(wav_22k).float()\n",
        "        wav_16k = torch.from_numpy(wav_16k).float()\n",
        "\n",
        "        if wav_22k.shape[-1] < self.minimum_audio_length:\n",
        "            wav_22k = F.pad(wav_22k, (0, self.minimum_audio_length - wav_22k.size(-1)), mode='constant', value=0.0)\n",
        "\n",
        "        _, pitch_median = get_pitch_median(wav_22k.numpy(), sr=22050)\n",
        "        data['ptich_median_pos'] = pitch_median\n",
        "\n",
        "        mel_22k = mel_spectrogram(wav_22k.unsqueeze(0))[0] # (80, T)\n",
        "        mel_start = random.randint(0, mel_22k.size(-1) - self.minimum_mel_length)\n",
        "        time_idxs = self.get_time_idxs(mel_start)\n",
        "        data['gt_mel_22k'] = crop_audio(mel_22k, time_idxs[0], time_idxs[1])\n",
        "\n",
        "        data['gt_audio_16k'] = crop_audio(wav_16k, time_idxs[3], time_idxs[5])\n",
        "        wav_16k_f = f(wav_16k, sr=16000)\n",
        "        data['gt_audio_16k_f'] = crop_audio(wav_16k_f, time_idxs[3], time_idxs[5])\n",
        "\n",
        "        data['gt_audio_22k'] = crop_audio(wav_22k, time_idxs[4], time_idxs[6])\n",
        "        wav_22k_g = g(wav_22k, sr=22050)\n",
        "        data['gt_audio_22k_g'] = crop_audio(wav_22k_g, time_idxs[4], time_idxs[6])\n",
        "        return data\n",
        "    \n",
        "\n",
        "    def get_neg(self, speaker):\n",
        "        data = {}\n",
        "        files = self.get_files(speaker)\n",
        "        file = np.random.choice(files)\n",
        "        \n",
        "        wav_22k, sr = librosa.load(file, sr=22050)\n",
        "        wav_16k = librosa.resample(wav_22k, orig_sr=22050, target_sr=16000)\n",
        "\n",
        "        wav_22k = torch.from_numpy(wav_22k).float()\n",
        "        wav_16k = torch.from_numpy(wav_16k).float()\n",
        "\n",
        "        if wav_22k.shape[-1] < self.minimum_audio_length:\n",
        "            wav_22k = F.pad(wav_22k, (0, self.minimum_audio_length - wav_22k.size(-1)), mode='constant', value=0.0)\n",
        "\n",
        "        _, pitch_median = get_pitch_median(wav_22k.numpy(), sr=22050)\n",
        "        data['ptich_median_neg'] = pitch_median\n",
        "\n",
        "        mel_22k = mel_spectrogram(wav_22k.unsqueeze(0))[0] # (80, T)\n",
        "        mel_start = random.randint(0, mel_22k.size(-1) - self.minimum_mel_length)\n",
        "        time_idxs = self.get_time_idxs(mel_start)\n",
        "\n",
        "        data['gt_audio_16k_negative'] = crop_audio(wav_16k, time_idxs[3], time_idxs[5])\n",
        "        data['gt_audio_22k_negative'] = crop_audio(wav_22k, time_idxs[4], time_idxs[6])\n",
        "        return data\n",
        "        \n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        pos_speaker = np.random.choice(self.speakers)\n",
        "        neg_speaker = pos_speaker\n",
        "        while neg_speaker == pos_speaker:\n",
        "            neg_speaker = np.random.choice(self.speakers)\n",
        "            \n",
        "        pos_item = self.get_pos(pos_speaker)\n",
        "        neg_item = self.get_neg(neg_speaker)\n",
        "        item = dict(**pos_item, **neg_item)\n",
        "        return item\n",
        "    \n",
        "    def get_data(self, fpath):\n",
        "        data = {}\n",
        "        wav_22k, sr = librosa.load(fpath, sr=22050)\n",
        "        wav_16k = librosa.resample(wav_22k, orig_sr=22050, target_sr=16000)\n",
        "\n",
        "        wav_22k = torch.from_numpy(wav_22k).float()\n",
        "        wav_16k = torch.from_numpy(wav_16k).float()\n",
        "\n",
        "        if wav_22k.shape[-1] < self.minimum_audio_length:\n",
        "            wav_22k = F.pad(wav_22k, (0, self.minimum_audio_length - wav_22k.size(-1)), mode='constant', value=0.0)\n",
        "\n",
        "        mel_22k = mel_spectrogram(wav_22k.unsqueeze(0))[0] # (80, T)\n",
        "        time_idxs = self.get_time_idxs(0)\n",
        "        \n",
        "        data['gt_mel_22k'] = crop_audio(mel_22k, time_idxs[0], time_idxs[1]).unsqueeze(0)\n",
        "        data['gt_audio_16k'] = crop_audio(wav_16k, time_idxs[3], time_idxs[5]).unsqueeze(0)\n",
        "        data['gt_audio_22k'] = crop_audio(wav_22k, time_idxs[4], time_idxs[6]).unsqueeze(0)\n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9211f23-d415-4ea6-b695-1a9416827682",
      "metadata": {
        "id": "b9211f23-d415-4ea6-b695-1a9416827682"
      },
      "outputs": [],
      "source": [
        "datadir = '/mnt/tts/ko-aihub/ko-aihub-emotion2021/'\n",
        "dataset = Dataset(datadir)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True, num_workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14638bc1-3ff2-41c2-a31d-3c3d78d071b7",
      "metadata": {
        "id": "14638bc1-3ff2-41c2-a31d-3c3d78d071b7"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(dataloader))\n",
        "for k,v in batch.items():\n",
        "    print(k, v.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50e78c36-9160-432c-9eb3-5fdd9a6be0a6",
      "metadata": {
        "tags": [],
        "id": "50e78c36-9160-432c-9eb3-5fdd9a6be0a6"
      },
      "source": [
        "## 3. Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d73b255-ab4e-4781-89c8-e01e15d60293",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "3d73b255-ab4e-4781-89c8-e01e15d60293"
      },
      "source": [
        "### 3.1. Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00a77e7b-1127-4b21-b67f-5aeefa077f30",
      "metadata": {
        "id": "00a77e7b-1127-4b21-b67f-5aeefa077f30"
      },
      "outputs": [],
      "source": [
        "# yin\n",
        "def differenceFunction(x, N, tau_max):\n",
        "    \"\"\"\n",
        "    Compute difference function of data x. This corresponds to equation (6) in [1]\n",
        "    This solution is implemented directly with Numpy fft.\n",
        "    :param x: audio data\n",
        "    :param N: length of data\n",
        "    :param tau_max: integration window size\n",
        "    :return: difference function\n",
        "    :rtype: list\n",
        "    \"\"\"\n",
        "\n",
        "    x = np.array(x, np.float64)\n",
        "    w = x.size\n",
        "    tau_max = min(tau_max, w)\n",
        "    x_cumsum = np.concatenate((np.array([0.]), (x * x).cumsum()))\n",
        "    size = w + tau_max\n",
        "    p2 = (size // 32).bit_length()\n",
        "    nice_numbers = (16, 18, 20, 24, 25, 27, 30, 32)\n",
        "    size_pad = min(x * 2 ** p2 for x in nice_numbers if x * 2 ** p2 >= size)\n",
        "    fc = np.fft.rfft(x, size_pad)\n",
        "    conv = np.fft.irfft(fc * fc.conjugate())[:tau_max]\n",
        "    return x_cumsum[w:w - tau_max:-1] + x_cumsum[w] - x_cumsum[:tau_max] - 2 * conv\n",
        "\n",
        "\n",
        "def cumulativeMeanNormalizedDifferenceFunction(df, N, eps=1e-8):\n",
        "    \"\"\"\n",
        "    Compute cumulative mean normalized difference function (CMND).\n",
        "    This corresponds to equation (8) in [1]\n",
        "    :param df: Difference function\n",
        "    :param N: length of data\n",
        "    :return: cumulative mean normalized difference function\n",
        "    :rtype: list\n",
        "    \"\"\"\n",
        "    np.seterr(divide='ignore', invalid='ignore')\n",
        "    # scipy method, assert df>0 for all element\n",
        "    cmndf = df[1:] * np.asarray(list(range(1, N))) / (np.cumsum(df[1:]).astype(float) + eps)\n",
        "    return np.insert(cmndf, 0, 1)\n",
        "\n",
        "\n",
        "def differenceFunctionBatch(xs: np.ndarray, N, tau_max):\n",
        "    \"\"\"numpy backend batch-wise differenceFunction\n",
        "    Args:\n",
        "        xs: audio segments, np.ndarray of shape (B x t)\n",
        "        N:\n",
        "        tau_max:\n",
        "    Returns:\n",
        "        y: dF. np.ndarray of shape (B x tau_max)\n",
        "    \"\"\"\n",
        "    xs = xs.astype(np.float64)\n",
        "    w = xs.shape[-1]\n",
        "    tau_max = min(tau_max, w)\n",
        "    zeros = np.zeros((xs.shape[0], 1))\n",
        "    x_cumsum = np.concatenate((np.zeros((xs.shape[0], 1)), (xs * xs).cumsum(axis=-1)), axis=-1)  # B x w\n",
        "    size = w + tau_max\n",
        "    p2 = (size // 32).bit_length()\n",
        "    nice_numbers = (16, 18, 20, 24, 25, 27, 30, 32)\n",
        "    size_pad = min(x * 2 ** p2 for x in nice_numbers if x * 2 ** p2 >= size)\n",
        "\n",
        "    convs = []\n",
        "    for i in range(xs.shape[0]):\n",
        "        x = xs[i]\n",
        "        fc = np.fft.rfft(x, size_pad)\n",
        "        conv = np.fft.irfft(fc * fc.conjugate())[:tau_max]\n",
        "        convs.append(conv)\n",
        "    convs = np.asarray(convs)\n",
        "\n",
        "    y = x_cumsum[:, w:w - tau_max:-1] + x_cumsum[:, w, np.newaxis] - x_cumsum[:, :tau_max] - 2 * convs\n",
        "    return y\n",
        "\n",
        "\n",
        "def cumulativeMeanNormalizedDifferenceFunctionBatch(dFs, N, eps=1e-8):\n",
        "    \"\"\"numpy backend batch-wise cumulative Mean Normalized Difference Functions\n",
        "    Args:\n",
        "        dFs: differenceFunctions. np.ndarray of shape (B x tau_max)\n",
        "        N:\n",
        "        eps:\n",
        "    Returns:\n",
        "        cMNDFs: np.ndarray of shape (B x tau_max)\n",
        "    \"\"\"\n",
        "    arange = np.asarray(list(range(1, N)))[np.newaxis, ...]\n",
        "    cumsum = np.cumsum(dFs[:, 1:], axis=-1).astype(float)\n",
        "    cMNDFs = dFs[:, 1:] * arange / (cumsum + eps)\n",
        "    cMNDFs = np.concatenate((np.zeros((cMNDFs.shape[0], 1)), cMNDFs), axis=1)\n",
        "    return cMNDFs\n",
        "\n",
        "\n",
        "def differenceFunctionTorch(xs: torch.Tensor, N, tau_max) -> torch.Tensor:\n",
        "    \"\"\"pytorch backend batch-wise differenceFunction\n",
        "    has 1e-4 level error with input shape of (32, 22050*1.5)\n",
        "    Args:\n",
        "        xs:\n",
        "        N:\n",
        "        tau_max:\n",
        "    Returns:\n",
        "    \"\"\"\n",
        "    xs = xs.double()\n",
        "    w = xs.shape[-1]\n",
        "    tau_max = min(tau_max, w)\n",
        "    zeros = torch.zeros((xs.shape[0], 1))\n",
        "    x_cumsum = torch.cat(\n",
        "        (torch.zeros((xs.shape[0], 1), device=xs.device), (xs * xs).cumsum(dim=-1, dtype=torch.double)),\n",
        "        dim=-1)  # B x w\n",
        "    size = w + tau_max\n",
        "    p2 = (size // 32).bit_length()\n",
        "    nice_numbers = (16, 18, 20, 24, 25, 27, 30, 32)\n",
        "    size_pad = min(x * 2 ** p2 for x in nice_numbers if x * 2 ** p2 >= size)\n",
        "\n",
        "    fcs = torch.fft.rfft(xs, n=size_pad, dim=-1)\n",
        "    convs = torch.fft.irfft(fcs * fcs.conj())[:, :tau_max]\n",
        "    y1 = torch.flip(x_cumsum[:, w - tau_max + 1:w + 1], dims=[-1])\n",
        "    y = y1 + x_cumsum[:, w, np.newaxis] - x_cumsum[:, :tau_max] - 2 * convs\n",
        "    return y\n",
        "\n",
        "\n",
        "def cumulativeMeanNormalizedDifferenceFunctionTorch(dfs: torch.Tensor, N, eps=1e-8) -> torch.Tensor:\n",
        "    arange = torch.arange(1, N, device=dfs.device, dtype=torch.float64)\n",
        "    cumsum = torch.cumsum(dfs[:, 1:], dim=-1, dtype=torch.float64).to(dfs.device)\n",
        "\n",
        "    cmndfs = dfs[:, 1:] * arange / (cumsum + eps)\n",
        "    cmndfs = torch.cat(\n",
        "        (torch.ones(cmndfs.shape[0], 1, device=dfs.device), cmndfs),\n",
        "        dim=-1)\n",
        "    return cmndfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b842ed31-14e8-4dc1-b8cc-f7e19e36635f",
      "metadata": {
        "id": "b842ed31-14e8-4dc1-b8cc-f7e19e36635f"
      },
      "outputs": [],
      "source": [
        "# ecapa\n",
        "class Conv1D_ReLU_BN(nn.Module):\n",
        "    def __init__(self, c_in, c_out, ks, stride, padding, dilation):\n",
        "        super(Conv1D_ReLU_BN, self).__init__()\n",
        "\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv1d(c_in, c_out, ks, stride, padding, dilation),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm1d(c_out),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.network(x)\n",
        "        return y\n",
        "\n",
        "\n",
        "class Res2_Conv1D(nn.Module):\n",
        "    def __init__(self, c, scale, ks, stride, padding, dilation):\n",
        "        super(Res2_Conv1D, self).__init__()\n",
        "        assert c % scale == 0\n",
        "        self.c = c\n",
        "        self.scale = scale\n",
        "        self.width = c // scale\n",
        "\n",
        "        self.convs = []\n",
        "        self.bns = []\n",
        "\n",
        "        for i in range(scale - 1):\n",
        "            self.convs.append(nn.Conv1d(self.width, self.width, ks, stride, padding, dilation))\n",
        "            self.bns.append(nn.BatchNorm1d(self.width))\n",
        "        self.convs = nn.ModuleList(self.convs)\n",
        "        self.bns = nn.ModuleList(self.bns)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        param x: (B x c x d)\n",
        "        \"\"\"\n",
        "\n",
        "        xs = torch.split(x, self.width, dim=1)  # channel-wise split\n",
        "        ys = []\n",
        "\n",
        "        for i in range(self.scale):\n",
        "            if i == 0:\n",
        "                x_ = xs[i]\n",
        "                y_ = x_\n",
        "            elif i == 1:\n",
        "                x_ = xs[i]\n",
        "                y_ = self.bns[i - 1](self.convs[i - 1](x_))\n",
        "            else:\n",
        "                x_ = xs[i] + ys[i - 1]\n",
        "                y_ = self.bns[i - 1](self.convs[i - 1](x_))\n",
        "            ys.append(y_)\n",
        "\n",
        "        y = torch.cat(ys, dim=1)  # channel-wise concat\n",
        "        return y\n",
        "\n",
        "\n",
        "class Res2_Conv1D_ReLU_BN(nn.Module):\n",
        "    def __init__(self, channel, scale, ks, stride, padding, dilation):\n",
        "        super(Res2_Conv1D_ReLU_BN, self).__init__()\n",
        "\n",
        "        self.network = nn.Sequential(\n",
        "            Res2_Conv1D(channel, scale, ks, stride, padding, dilation),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.BatchNorm1d(channel),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.network(x)\n",
        "        return y\n",
        "\n",
        "\n",
        "class SE_Block(nn.Module):\n",
        "    def __init__(self, c_in, c_mid):\n",
        "        super(SE_Block, self).__init__()\n",
        "\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(c_in, c_mid),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(c_mid, c_in),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        s = self.network(x.mean(dim=-1))\n",
        "        y = x * s.unsqueeze(-1)\n",
        "        return y\n",
        "\n",
        "\n",
        "class SE_Res2_Block(nn.Module):\n",
        "    def __init__(self, channel, scale, ks, stride, padding, dilation):\n",
        "        super(SE_Res2_Block, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            Conv1D_ReLU_BN(channel, channel, 1, 1, 0, 1),\n",
        "            Res2_Conv1D_ReLU_BN(channel, scale, ks, stride, padding, dilation),\n",
        "            Conv1D_ReLU_BN(channel, channel, 1, 1, 0, 1),\n",
        "            SE_Block(channel, channel)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.network(x) + x\n",
        "        return y\n",
        "\n",
        "\n",
        "class AttentiveStatisticPool(nn.Module):\n",
        "    def __init__(self, c_in, c_mid):\n",
        "        super(AttentiveStatisticPool, self).__init__()\n",
        "\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv1d(c_in, c_mid, kernel_size=1),\n",
        "            nn.Tanh(),  # seems like most implementations uses tanh?\n",
        "            nn.Conv1d(c_mid, c_in, kernel_size=1),\n",
        "            nn.Softmax(dim=-1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.shape: B x C x t\n",
        "        alpha = self.network(x)\n",
        "        mu_hat = torch.sum(alpha * x, dim=-1)\n",
        "        var = torch.sum(alpha * x ** 2, dim=-1) - mu_hat ** 2\n",
        "        std_hat = torch.sqrt(var.clamp(min=1e-9))\n",
        "        y = torch.cat([mu_hat, std_hat], dim=-1)\n",
        "        # y.shape: B x (c_in*2)\n",
        "        return y\n",
        "\n",
        "\n",
        "class ECAPA_TDNN(nn.Module):\n",
        "    def __init__(self, c_in=80, c_mid=512, c_out=192):\n",
        "        super(ECAPA_TDNN, self).__init__()\n",
        "\n",
        "        self.layer1 = Conv1D_ReLU_BN(c_in, c_mid, 5, 1, 2, 1)\n",
        "        self.layer2 = SE_Res2_Block(c_mid, 8, 3, 1, 2, 2)\n",
        "        self.layer3 = SE_Res2_Block(c_mid, 8, 3, 1, 3, 3)\n",
        "        self.layer4 = SE_Res2_Block(c_mid, 8, 3, 1, 4, 4)\n",
        "\n",
        "        self.network = nn.Sequential(\n",
        "            # Figure 2 in https://arxiv.org/pdf/2005.07143.pdf seems like groupconv?\n",
        "            nn.Conv1d(c_mid * 3, 1536, kernel_size=1, groups=3),\n",
        "            AttentiveStatisticPool(1536, 128),\n",
        "        )\n",
        "\n",
        "        self.bn1 = nn.BatchNorm1d(3072)\n",
        "        self.linear = nn.Linear(3072, c_out)\n",
        "        self.bn2 = nn.BatchNorm1d(c_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.shape: B x C x t\n",
        "        y1 = self.layer1(x)\n",
        "        y2 = self.layer2(y1) + y1\n",
        "        y3 = self.layer3(y1 + y2) + y1 + y2\n",
        "        y4 = self.layer4(y1 + y2 + y3) + y1 + y2 + y3\n",
        "\n",
        "        y = torch.cat([y2, y3, y4], dim=1)  # channel-wise concat\n",
        "        y = self.network(y)\n",
        "\n",
        "        y = self.linear(self.bn1(y.unsqueeze(-1)).squeeze(-1))\n",
        "        y = self.bn2(y.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1ab64be-ccb6-4a24-a10f-fb03877ac588",
      "metadata": {
        "id": "f1ab64be-ccb6-4a24-a10f-fb03877ac588"
      },
      "outputs": [],
      "source": [
        "class Linguistic(torch.nn.Module):\n",
        "    def __init__(self, wav2vec2):\n",
        "        super().__init__()\n",
        "        self.wav2vec2 = wav2vec2\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: torch.Tensor of shape (B x t)\n",
        "        Returns:\n",
        "            y: torch.Tensor of shape(B x C x t)\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = self.wav2vec2(x, output_hidden_states=True)\n",
        "        y = outputs.hidden_states[12]  # B x t x C(1024)\n",
        "        y = y.permute((0, 2, 1))  # B x t x C -> B x C x t\n",
        "        return y\n",
        "    \n",
        "    def train(self, mode: bool = True):\n",
        "        if not isinstance(mode, bool):\n",
        "            raise ValueError(\"training mode is expected to be boolean\")\n",
        "        self.training = mode\n",
        "        # for module in self.children():\n",
        "        #     module.train(mode)\n",
        "        return self\n",
        "\n",
        "\n",
        "class Speaker(torch.nn.Module):\n",
        "    def __init__(self, wav2vec2):\n",
        "        super(Speaker, self).__init__()\n",
        "        self.wav2vec2 = wav2vec2\n",
        "        self.spk = ECAPA_TDNN(c_in=1024, c_mid=512, c_out=192)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: torch.Tensor of shape (B x t)\n",
        "        Returns:\n",
        "            y: torch.Tensor of shape (B x 192)\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = self.wav2vec2(x, output_hidden_states=True)\n",
        "        y = outputs.hidden_states[1]  # B x t x C(1024)\n",
        "        y = y.permute((0, 2, 1))  # B x t x C -> B x C x t\n",
        "        y = self.spk(y)  # B x C(1024) x t -> B x D(192)\n",
        "        y = torch.nn.functional.normalize(y, p=2, dim=-1)\n",
        "        return y\n",
        "\n",
        "    def train(self, mode: bool = True):\n",
        "        if not isinstance(mode, bool):\n",
        "            raise ValueError(\"training mode is expected to be boolean\")\n",
        "        self.training = mode\n",
        "        # for module in self.children():\n",
        "        #     module.train(mode)\n",
        "        self.spk.train(mode)\n",
        "        return self\n",
        "\n",
        "\n",
        "class Energy(torch.nn.Module):\n",
        "    def forward(self, mel):\n",
        "        \"\"\"For the energy feature, we simply took an average from a log-mel spectrogram along the frequency axis.\n",
        "        Args:\n",
        "            mel: torch.Tensor of shape (B x t x C)\n",
        "        Returns:\n",
        "            y: torch.Tensor of shape (B x 1 x C)\n",
        "        \"\"\"\n",
        "        y = torch.mean(mel, dim=1, keepdim=True)  # B x 1(channel) x t\n",
        "        return y\n",
        "\n",
        "\n",
        "class Pitch(torch.nn.Module):\n",
        "    @staticmethod\n",
        "    def midi_to_lag(m: int, sr: int, semitone_range: float = 12):\n",
        "        \"\"\"converts midi-to-lag, eq. (4)\n",
        "        Args:\n",
        "            m: midi\n",
        "            sr: sample_rate\n",
        "            semitone_range:\n",
        "        Returns:\n",
        "            lag: time lag(tau, c(m)) calculated from midi, eq. (4)\n",
        "        \"\"\"\n",
        "        f = 440 * math.pow(2, (m - 69) / semitone_range)\n",
        "        lag = sr / f\n",
        "        return lag\n",
        "\n",
        "    @staticmethod\n",
        "    def yingram_from_cmndf(cmndfs: torch.Tensor, ms: list, sr: int = 22050) -> torch.Tensor:\n",
        "        \"\"\" yingram calculator from cMNDFs(cumulative Mean Normalized Difference Functions)\n",
        "        Args:\n",
        "            cmndfs: torch.Tensor\n",
        "                calculated cumulative mean normalized difference function\n",
        "                for details, see models/yin.py or eq. (1) and (2)\n",
        "            ms: list of midi(int)\n",
        "            sr: sampling rate\n",
        "        Returns:\n",
        "            y:\n",
        "                calculated batch yingram\n",
        "        \"\"\"\n",
        "        c_ms = np.asarray([Pitch.midi_to_lag(m, sr) for m in ms])\n",
        "        c_ms = torch.from_numpy(c_ms).to(cmndfs.device)\n",
        "        c_ms_ceil = torch.ceil(c_ms).long().to(cmndfs.device)\n",
        "        c_ms_floor = torch.floor(c_ms).long().to(cmndfs.device)\n",
        "\n",
        "        y = (cmndfs[:, c_ms_ceil] - cmndfs[:, c_ms_floor]) / (c_ms_ceil - c_ms_floor).unsqueeze(0) * (\n",
        "                c_ms - c_ms_floor).unsqueeze(0) + cmndfs[:, c_ms_floor]\n",
        "        return y\n",
        "\n",
        "    @staticmethod\n",
        "    def yingram(x: torch.Tensor, W: int = 2048, tau_max: int = 2048, sr: int = 22050, w_step: int = 256):\n",
        "        \"\"\"calculates yingram from raw audio (multi segment)\n",
        "        Args:\n",
        "            x: raw audio, torch.Tensor of shape (t)\n",
        "            W: yingram Window Size\n",
        "            tau_max:\n",
        "            sr: sampling rate\n",
        "            w_step: yingram bin step size\n",
        "        Returns:\n",
        "            yingram: yingram. torch.Tensor of shape (80 x t')\n",
        "        \"\"\"\n",
        "        # x.shape: t\n",
        "        w_len = W\n",
        "\n",
        "        startFrames = list(range(0, x.shape[-1] - w_len, w_step))\n",
        "        startFrames = np.asarray(startFrames)\n",
        "        # times = startFrames / sr\n",
        "        frames = [x[..., t:t + W] for t in startFrames]\n",
        "        frames_torch = torch.stack(frames, dim=0).to(x.device)\n",
        "\n",
        "        # If not using gpu, or torch not compatible, implemented numpy batch function is still fine\n",
        "        dfs = differenceFunctionTorch(frames_torch, frames_torch.shape[-1], tau_max)\n",
        "        cmndfs = cumulativeMeanNormalizedDifferenceFunctionTorch(dfs, tau_max)\n",
        "\n",
        "        midis = list(range(5, 85))\n",
        "        yingram = Pitch.yingram_from_cmndf(cmndfs, midis, sr)\n",
        "        return yingram\n",
        "\n",
        "    @staticmethod\n",
        "    def yingram_batch(x: torch.Tensor, W: int = 2048, tau_max: int = 2048, sr: int = 22050, w_step: int = 256):\n",
        "        \"\"\"calculates yingram from batch raw audio.\n",
        "        currently calculates batch-wise through for loop, but seems it can be implemented to act batch-wise\n",
        "        Args:\n",
        "            x: torch.Tensor of shape (B x t)\n",
        "            W:\n",
        "            tau_max:\n",
        "            sr:\n",
        "            w_step:\n",
        "        Returns:\n",
        "            yingram: yingram. torch.Tensor of shape (B x 80 x t')\n",
        "        \"\"\"\n",
        "        batch_results = []\n",
        "        for i in range(len(x)):\n",
        "            yingram = Pitch.yingram(x[i], W, tau_max, sr, w_step)\n",
        "            batch_results.append(yingram)\n",
        "        result = torch.stack(batch_results, dim=0).float()\n",
        "        result = result.permute((0, 2, 1)).to(x.device)\n",
        "        return result\n",
        "\n",
        "\n",
        "class Analysis(torch.nn.Module):\n",
        "    def __init__(self, conf=None):\n",
        "        \"\"\"joins all analysis modules into one\n",
        "        Args:\n",
        "            conf:\n",
        "        \"\"\"\n",
        "        super(Analysis, self).__init__()\n",
        "        wav2vec2 = Wav2Vec2ForPreTraining.from_pretrained(\"facebook/wav2vec2-large-xlsr-53\")\n",
        "        _ = wav2vec2.eval().requires_grad_(False)\n",
        "        self.wav2vec2 = wav2vec2\n",
        "\n",
        "        self.linguistic = Linguistic(wav2vec2)\n",
        "        self.speaker = Speaker(wav2vec2)\n",
        "        self.energy = Energy()\n",
        "        self.pitch = Pitch()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4794c0ee-6c06-4d12-bd8e-d46fcbf9cf53",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "4794c0ee-6c06-4d12-bd8e-d46fcbf9cf53"
      },
      "source": [
        "### 3.2. Synthesis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e25c02f5-7688-4b00-98a0-2bb85e53798b",
      "metadata": {
        "id": "e25c02f5-7688-4b00-98a0-2bb85e53798b"
      },
      "outputs": [],
      "source": [
        "# hifigan\n",
        "\n",
        "from torch.nn import Conv1d, ConvTranspose1d, AvgPool1d, Conv2d\n",
        "from torch.nn.utils import weight_norm, remove_weight_norm, spectral_norm\n",
        "\n",
        "# from utils import init_weights, get_padding\n",
        "\n",
        "LRELU_SLOPE = 0.1\n",
        "\n",
        "\n",
        "def init_weights(m, mean=0.0, std=0.01):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        m.weight.data.normal_(mean, std)\n",
        "\n",
        "\n",
        "def get_padding(kernel_size, dilation=1):\n",
        "    return int((kernel_size * dilation - dilation) / 2)\n",
        "\n",
        "\n",
        "class ResBlock1(torch.nn.Module):\n",
        "    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3, 5)):\n",
        "        super(ResBlock1, self).__init__()\n",
        "        self.h = h\n",
        "        self.convs1 = nn.ModuleList([\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
        "                               padding=get_padding(kernel_size, dilation[0]))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
        "                               padding=get_padding(kernel_size, dilation[1]))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],\n",
        "                               padding=get_padding(kernel_size, dilation[2])))\n",
        "        ])\n",
        "        self.convs1.apply(init_weights)\n",
        "\n",
        "        self.convs2 = nn.ModuleList([\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
        "                               padding=get_padding(kernel_size, 1)))\n",
        "        ])\n",
        "        self.convs2.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for c1, c2 in zip(self.convs1, self.convs2):\n",
        "            xt = F.leaky_relu(x, LRELU_SLOPE)\n",
        "            xt = c1(xt)\n",
        "            xt = F.leaky_relu(xt, LRELU_SLOPE)\n",
        "            xt = c2(xt)\n",
        "            x = xt + x\n",
        "        return x\n",
        "\n",
        "    def remove_weight_norm(self):\n",
        "        for l in self.convs1:\n",
        "            remove_weight_norm(l)\n",
        "        for l in self.convs2:\n",
        "            remove_weight_norm(l)\n",
        "\n",
        "\n",
        "class ResBlock2(torch.nn.Module):\n",
        "    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3)):\n",
        "        super(ResBlock2, self).__init__()\n",
        "        self.h = h\n",
        "        self.convs = nn.ModuleList([\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
        "                               padding=get_padding(kernel_size, dilation[0]))),\n",
        "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
        "                               padding=get_padding(kernel_size, dilation[1])))\n",
        "        ])\n",
        "        self.convs.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for c in self.convs:\n",
        "            xt = F.leaky_relu(x, LRELU_SLOPE)\n",
        "            xt = c(xt)\n",
        "            x = xt + x\n",
        "        return x\n",
        "\n",
        "    def remove_weight_norm(self):\n",
        "        for l in self.convs:\n",
        "            remove_weight_norm(l)\n",
        "\n",
        "\n",
        "class HiFiGANGenerator(torch.nn.Module):\n",
        "    def __init__(self, h):\n",
        "        super().__init__()\n",
        "        self.h = h\n",
        "        self.num_kernels = len(h.resblock_kernel_sizes)\n",
        "        self.num_upsamples = len(h.upsample_rates)\n",
        "        self.conv_pre = weight_norm(Conv1d(80, h.upsample_initial_channel, 7, 1, padding=3))\n",
        "        resblock = ResBlock1 if h.resblock == '1' else ResBlock2\n",
        "\n",
        "        self.ups = nn.ModuleList()\n",
        "        for i, (u, k) in enumerate(zip(h.upsample_rates, h.upsample_kernel_sizes)):\n",
        "            self.ups.append(weight_norm(\n",
        "                ConvTranspose1d(h.upsample_initial_channel // (2 ** i), h.upsample_initial_channel // (2 ** (i + 1)),\n",
        "                                k, u, padding=(k - u) // 2)))\n",
        "\n",
        "        self.resblocks = nn.ModuleList()\n",
        "        for i in range(len(self.ups)):\n",
        "            ch = h.upsample_initial_channel // (2 ** (i + 1))\n",
        "            for j, (k, d) in enumerate(zip(h.resblock_kernel_sizes, h.resblock_dilation_sizes)):\n",
        "                self.resblocks.append(resblock(h, ch, k, d))\n",
        "\n",
        "        self.conv_post = weight_norm(Conv1d(ch, 1, 7, 1, padding=3))\n",
        "        self.ups.apply(init_weights)\n",
        "        self.conv_post.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv_pre(x)\n",
        "        for i in range(self.num_upsamples):\n",
        "            x = F.leaky_relu(x, LRELU_SLOPE)\n",
        "            x = self.ups[i](x)\n",
        "            xs = None\n",
        "            for j in range(self.num_kernels):\n",
        "                if xs is None:\n",
        "                    xs = self.resblocks[i * self.num_kernels + j](x)\n",
        "                else:\n",
        "                    xs += self.resblocks[i * self.num_kernels + j](x)\n",
        "            x = xs / self.num_kernels\n",
        "        x = F.leaky_relu(x)\n",
        "        x = self.conv_post(x)\n",
        "        x = torch.tanh(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def remove_weight_norm(self):\n",
        "        print('Removing weight norm...')\n",
        "        for l in self.ups:\n",
        "            remove_weight_norm(l)\n",
        "        for l in self.resblocks:\n",
        "            l.remove_weight_norm()\n",
        "        remove_weight_norm(self.conv_pre)\n",
        "        remove_weight_norm(self.conv_post)\n",
        "\n",
        "\n",
        "class DiscriminatorP(torch.nn.Module):\n",
        "    def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):\n",
        "        super(DiscriminatorP, self).__init__()\n",
        "        self.period = period\n",
        "        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n",
        "        self.convs = nn.ModuleList([\n",
        "            norm_f(Conv2d(1, 32, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n",
        "            norm_f(Conv2d(32, 128, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n",
        "            norm_f(Conv2d(128, 512, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n",
        "            norm_f(Conv2d(512, 1024, (kernel_size, 1), (stride, 1), padding=(get_padding(5, 1), 0))),\n",
        "            norm_f(Conv2d(1024, 1024, (kernel_size, 1), 1, padding=(2, 0))),\n",
        "        ])\n",
        "        self.conv_post = norm_f(Conv2d(1024, 1, (3, 1), 1, padding=(1, 0)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmap = []\n",
        "\n",
        "        # 1d to 2d\n",
        "        b, c, t = x.shape\n",
        "        if t % self.period != 0:  # pad first\n",
        "            n_pad = self.period - (t % self.period)\n",
        "            x = F.pad(x, (0, n_pad), \"reflect\")\n",
        "            t = t + n_pad\n",
        "        x = x.view(b, c, t // self.period, self.period)\n",
        "\n",
        "        for l in self.convs:\n",
        "            x = l(x)\n",
        "            x = F.leaky_relu(x, LRELU_SLOPE)\n",
        "            fmap.append(x)\n",
        "        x = self.conv_post(x)\n",
        "        fmap.append(x)\n",
        "        x = torch.flatten(x, 1, -1)\n",
        "\n",
        "        return x, fmap\n",
        "\n",
        "\n",
        "class MultiPeriodDiscriminator(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiPeriodDiscriminator, self).__init__()\n",
        "        self.discriminators = nn.ModuleList([\n",
        "            DiscriminatorP(2),\n",
        "            DiscriminatorP(3),\n",
        "            DiscriminatorP(5),\n",
        "            DiscriminatorP(7),\n",
        "            DiscriminatorP(11),\n",
        "        ])\n",
        "\n",
        "    def forward(self, y, y_hat):\n",
        "        y_d_rs = []\n",
        "        y_d_gs = []\n",
        "        fmap_rs = []\n",
        "        fmap_gs = []\n",
        "        for i, d in enumerate(self.discriminators):\n",
        "            y_d_r, fmap_r = d(y)\n",
        "            y_d_g, fmap_g = d(y_hat)\n",
        "            y_d_rs.append(y_d_r)\n",
        "            fmap_rs.append(fmap_r)\n",
        "            y_d_gs.append(y_d_g)\n",
        "            fmap_gs.append(fmap_g)\n",
        "\n",
        "        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n",
        "\n",
        "\n",
        "class DiscriminatorS(torch.nn.Module):\n",
        "    def __init__(self, use_spectral_norm=False):\n",
        "        super(DiscriminatorS, self).__init__()\n",
        "        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n",
        "        self.convs = nn.ModuleList([\n",
        "            norm_f(Conv1d(1, 128, 15, 1, padding=7)),\n",
        "            norm_f(Conv1d(128, 128, 41, 2, groups=4, padding=20)),\n",
        "            norm_f(Conv1d(128, 256, 41, 2, groups=16, padding=20)),\n",
        "            norm_f(Conv1d(256, 512, 41, 4, groups=16, padding=20)),\n",
        "            norm_f(Conv1d(512, 1024, 41, 4, groups=16, padding=20)),\n",
        "            norm_f(Conv1d(1024, 1024, 41, 1, groups=16, padding=20)),\n",
        "            norm_f(Conv1d(1024, 1024, 5, 1, padding=2)),\n",
        "        ])\n",
        "        self.conv_post = norm_f(Conv1d(1024, 1, 3, 1, padding=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        fmap = []\n",
        "        for l in self.convs:\n",
        "            x = l(x)\n",
        "            x = F.leaky_relu(x, LRELU_SLOPE)\n",
        "            fmap.append(x)\n",
        "        x = self.conv_post(x)\n",
        "        fmap.append(x)\n",
        "        x = torch.flatten(x, 1, -1)\n",
        "\n",
        "        return x, fmap\n",
        "\n",
        "\n",
        "class MultiScaleDiscriminator(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiScaleDiscriminator, self).__init__()\n",
        "        self.discriminators = nn.ModuleList([\n",
        "            DiscriminatorS(use_spectral_norm=True),\n",
        "            DiscriminatorS(),\n",
        "            DiscriminatorS(),\n",
        "        ])\n",
        "        self.meanpools = nn.ModuleList([\n",
        "            AvgPool1d(4, 2, padding=2),\n",
        "            AvgPool1d(4, 2, padding=2)\n",
        "        ])\n",
        "\n",
        "    def forward(self, y, y_hat):\n",
        "        y_d_rs = []\n",
        "        y_d_gs = []\n",
        "        fmap_rs = []\n",
        "        fmap_gs = []\n",
        "        for i, d in enumerate(self.discriminators):\n",
        "            if i != 0:\n",
        "                y = self.meanpools[i - 1](y)\n",
        "                y_hat = self.meanpools[i - 1](y_hat)\n",
        "            y_d_r, fmap_r = d(y)\n",
        "            y_d_g, fmap_g = d(y_hat)\n",
        "            y_d_rs.append(y_d_r)\n",
        "            fmap_rs.append(fmap_r)\n",
        "            y_d_gs.append(y_d_g)\n",
        "            fmap_gs.append(fmap_g)\n",
        "\n",
        "        return y_d_rs, y_d_gs, fmap_rs, fmap_gs\n",
        "\n",
        "\n",
        "def feature_loss(fmap_r, fmap_g):\n",
        "    loss = 0\n",
        "    for dr, dg in zip(fmap_r, fmap_g):\n",
        "        for rl, gl in zip(dr, dg):\n",
        "            loss += torch.mean(torch.abs(rl - gl))\n",
        "\n",
        "    return loss * 2\n",
        "\n",
        "\n",
        "def discriminator_loss(disc_real_outputs, disc_generated_outputs):\n",
        "    loss = 0\n",
        "    r_losses = []\n",
        "    g_losses = []\n",
        "    for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n",
        "        r_loss = torch.mean((1 - dr) ** 2)\n",
        "        g_loss = torch.mean(dg ** 2)\n",
        "        loss += (r_loss + g_loss)\n",
        "        r_losses.append(r_loss.item())\n",
        "        g_losses.append(g_loss.item())\n",
        "\n",
        "    return loss, r_losses, g_losses\n",
        "\n",
        "\n",
        "def generator_loss(disc_outputs):\n",
        "    loss = 0\n",
        "    gen_losses = []\n",
        "    for dg in disc_outputs:\n",
        "        l = torch.mean((1 - dg) ** 2)\n",
        "        gen_losses.append(l)\n",
        "        loss += l\n",
        "\n",
        "    return loss, gen_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "296f0c80-10f5-4b43-a13c-844f73641452",
      "metadata": {
        "id": "296f0c80-10f5-4b43-a13c-844f73641452"
      },
      "outputs": [],
      "source": [
        "class ConditionalLayerNorm(nn.Module):\n",
        "    def __init__(self, embedding_dim: int, normalize_embedding: bool = True):\n",
        "        super(ConditionalLayerNorm, self).__init__()\n",
        "        self.normalize_embedding = normalize_embedding\n",
        "\n",
        "        self.linear_scale = nn.Linear(embedding_dim, 1)\n",
        "        self.linear_bias = nn.Linear(embedding_dim, 1)\n",
        "\n",
        "    def forward(self, x, embedding):\n",
        "        if self.normalize_embedding:\n",
        "            embedding = torch.nn.functional.normalize(embedding, p=2, dim=-1)\n",
        "        scale = self.linear_scale(embedding).unsqueeze(-1)  # shape: (B, 1, 1)\n",
        "        bias = self.linear_bias(embedding).unsqueeze(-1)  # shape: (B, 1, 1)\n",
        "\n",
        "        out = (x - torch.mean(x, dim=-1, keepdim=True)) / torch.var(x, dim=-1, keepdim=True)\n",
        "        out = scale * out + bias\n",
        "        return out\n",
        "\n",
        "\n",
        "class ConvGLU(nn.Module):\n",
        "    def __init__(self, channel, ks, dilation, embedding_dim=192, use_cLN=False):\n",
        "        super(ConvGLU, self).__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout()\n",
        "        self.conv = nn.Conv1d(channel, channel * 2, kernel_size=ks, stride=1, padding=(ks - 1) // 2 * dilation,\n",
        "                              dilation=dilation)\n",
        "        self.glu = nn.GLU(dim=1)  # channel-wise\n",
        "\n",
        "        self.use_cLN = use_cLN\n",
        "        if self.use_cLN:\n",
        "            self.norm = ConditionalLayerNorm(embedding_dim)\n",
        "\n",
        "    def forward(self, x, speaker_embedding=None):\n",
        "        y = self.dropout(x)\n",
        "        y = self.conv(y)\n",
        "        y = self.glu(y)\n",
        "        y = y + x\n",
        "\n",
        "        if self.use_cLN and speaker_embedding is not None:\n",
        "            y = self.norm(y, speaker_embedding)\n",
        "        return y\n",
        "\n",
        "\n",
        "class PreConv(nn.Module):\n",
        "    def __init__(self, c_in, c_mid, c_out):\n",
        "        super(PreConv, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv1d(c_in, c_mid, kernel_size=1, dilation=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(),\n",
        "\n",
        "            nn.Conv1d(c_mid, c_mid, kernel_size=1, dilation=1),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(),\n",
        "\n",
        "            nn.Conv1d(c_mid, c_out, kernel_size=1, dilation=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.network(x)\n",
        "        return y\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, c_in=1024, c_preconv=512, c_mid=512, c_out=80):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.network1 = nn.Sequential(\n",
        "            PreConv(c_in, c_preconv, c_mid),\n",
        "\n",
        "            ConvGLU(c_mid, ks=3, dilation=1, use_cLN=False),\n",
        "            ConvGLU(c_mid, ks=3, dilation=3, use_cLN=False),\n",
        "            ConvGLU(c_mid, ks=3, dilation=9, use_cLN=False),\n",
        "            ConvGLU(c_mid, ks=3, dilation=27, use_cLN=False),\n",
        "\n",
        "            ConvGLU(c_mid, ks=3, dilation=1, use_cLN=False),\n",
        "            ConvGLU(c_mid, ks=3, dilation=3, use_cLN=False),\n",
        "            ConvGLU(c_mid, ks=3, dilation=9, use_cLN=False),\n",
        "            ConvGLU(c_mid, ks=3, dilation=27, use_cLN=False),\n",
        "\n",
        "            # ConvGLU(c_mid, ks=3, dilation=1, use_cLN=False),\n",
        "            # ConvGLU(c_mid, ks=3, dilation=3, use_cLN=False),\n",
        "            # ConvGLU(c_mid, ks=3, dilation=9, use_cLN=False),\n",
        "            # ConvGLU(c_mid, ks=3, dilation=27, use_cLN=False),\n",
        "\n",
        "            ConvGLU(c_mid, ks=3, dilation=1, use_cLN=False),\n",
        "            ConvGLU(c_mid, ks=3, dilation=3, use_cLN=False),\n",
        "        )\n",
        "\n",
        "        self.LR = nn.Sequential(\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Conv1d(c_mid + 1, c_mid + 1, kernel_size=1, stride=1))\n",
        "\n",
        "        self.network3 = nn.ModuleList([\n",
        "            ConvGLU(c_mid + 1, ks=3, dilation=1, use_cLN=True),\n",
        "            ConvGLU(c_mid + 1, ks=3, dilation=3, use_cLN=True),\n",
        "            ConvGLU(c_mid + 1, ks=3, dilation=9, use_cLN=True),\n",
        "            ConvGLU(c_mid + 1, ks=3, dilation=27, use_cLN=True),\n",
        "\n",
        "            ConvGLU(c_mid + 1, ks=3, dilation=1, use_cLN=True),\n",
        "            ConvGLU(c_mid + 1, ks=3, dilation=3, use_cLN=True),\n",
        "            ConvGLU(c_mid + 1, ks=3, dilation=9, use_cLN=True),\n",
        "            ConvGLU(c_mid + 1, ks=3, dilation=27, use_cLN=True),\n",
        "\n",
        "            # ConvGLU(c_mid + 1, ks=3, dilation=1, use_cLN=True),\n",
        "            # ConvGLU(c_mid + 1, ks=3, dilation=3, use_cLN=True),\n",
        "            # ConvGLU(c_mid + 1, ks=3, dilation=9, use_cLN=True),\n",
        "            # ConvGLU(c_mid + 1, ks=3, dilation=27, use_cLN=True),\n",
        "\n",
        "            ConvGLU(c_mid + 1, ks=3, dilation=1, use_cLN=True),\n",
        "            ConvGLU(c_mid + 1, ks=3, dilation=3, use_cLN=True),\n",
        "        ])\n",
        "\n",
        "        self.lastConv = nn.Conv1d(c_mid + 1, c_out, kernel_size=1, dilation=1)\n",
        "\n",
        "    def forward(self, x, energy, speaker_embedding):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: wav2vec feature or yingram. torch.Tensor of shape (B x C x t)\n",
        "            energy: energy. torch.Tensor of shape (B x 1 x t)\n",
        "            speaker_embedding: embedding. torch.Tensor of shape (B x d x 1)\n",
        "        Returns:\n",
        "        \"\"\"\n",
        "        y = self.network1(x)\n",
        "        B, C, _ = y.shape\n",
        "\n",
        "        y = F.interpolate(y, energy.shape[-1])  # B x C x d\n",
        "        y = torch.cat((y, energy), dim=1)  # channel-wise concat\n",
        "        y = self.LR(y)\n",
        "\n",
        "        for module in self.network3:  # doing this since sequential takes only 1 input\n",
        "            y = module(y, speaker_embedding)\n",
        "        y = self.lastConv(y)\n",
        "        return y\n",
        "\n",
        "\n",
        "class Synthesis(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Synthesis, self).__init__()\n",
        "        self.filter_generator = Generator(1024, 512, 128, 80)\n",
        "        self.source_generator = Generator(50, 512, 128, 80)\n",
        "        self.set_vocoder()\n",
        "\n",
        "    def set_vocoder(self):\n",
        "        path_config = './hifi-gan/UNIVERSAL_V1/config.json'\n",
        "        path_ckpt = './hifi-gan/UNIVERSAL_V1/g_02500000'\n",
        "\n",
        "        hifigan_config = OmegaConf.load(path_config)\n",
        "        self.vocoder = HiFiGANGenerator(hifigan_config)\n",
        "\n",
        "        state_dict_g = torch.load(path_ckpt)\n",
        "        self.vocoder.load_state_dict(state_dict_g['generator'])\n",
        "        self.vocoder.eval()\n",
        "\n",
        "        for param in self.vocoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        zero_audio = torch.zeros(44100).float()\n",
        "        zero_mel = mel_spectrogram(\n",
        "            zero_audio.unsqueeze(0),\n",
        "            1024, 80, 22050, 256, 1024, 0, 8000\n",
        "        )\n",
        "        self.mel_padding_value = torch.min(zero_mel)\n",
        "\n",
        "    def _denormalize(self, spec):\n",
        "        return spec * -self.mel_padding_value + self.mel_padding_value\n",
        "\n",
        "    def forward(self, lps, s, e, ps):\n",
        "        result = {}\n",
        "        result['mel_filter'] = self.filter_generator(lps, e, s)\n",
        "        result['mel_source'] = self.source_generator(ps, e, s)\n",
        "        result['gen_mel'] = result['mel_filter'] + result['mel_source']\n",
        "        with torch.no_grad():\n",
        "            # hifigan_mel = self._denormalize(result['gen_mel'])\n",
        "            hifigan_mel = result['gen_mel']\n",
        "            result['audio_gen'] = self.vocoder(hifigan_mel)\n",
        "        return result\n",
        "\n",
        "    def train(self, mode: bool = True):\n",
        "        if not isinstance(mode, bool):\n",
        "            raise ValueError(\"training mode is expected to be boolean\")\n",
        "        self.training = mode\n",
        "        # for module in self.children():\n",
        "        #     module.train(mode)\n",
        "        self.filter_generator.train(mode)\n",
        "        self.source_generator.train(mode)\n",
        "        return self"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "074addbd-9112-4583-a49a-a79c50178293",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "074addbd-9112-4583-a49a-a79c50178293"
      },
      "source": [
        "### 3.3 Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90ac1dbb-62a6-4ed8-a333-42cee1d74896",
      "metadata": {
        "id": "90ac1dbb-62a6-4ed8-a333-42cee1d74896"
      },
      "outputs": [],
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, c_in, c_mid=128, c_out=128):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.leaky_relu1 = nn.LeakyReLU()\n",
        "        self.conv1 = nn.Conv1d(c_in, c_mid, kernel_size=3, stride=1, padding=(3 - 1) // 2 * 3, dilation=3)\n",
        "\n",
        "        self.leaky_relu2 = nn.LeakyReLU()\n",
        "        self.conv2 = nn.Conv1d(c_mid, c_out, kernel_size=3, stride=1, padding=(3 - 1) // 2 * 3, dilation=3)\n",
        "\n",
        "        self.conv3 = nn.Conv1d(c_in, c_out, kernel_size=1, dilation=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.conv1(self.leaky_relu1(x))\n",
        "        y = self.conv2(self.leaky_relu2(y))\n",
        "        y = y + self.conv3(x)\n",
        "        return y\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        c_in = 80\n",
        "        c_mid = 128\n",
        "        c_out = 192\n",
        "\n",
        "        self.phi = nn.Sequential(\n",
        "            nn.Conv1d(c_in, c_mid, kernel_size=3, stride=1, padding=1, dilation=1),\n",
        "            ResBlock(c_mid, c_mid, c_mid),\n",
        "            ResBlock(c_mid, c_mid, c_mid),\n",
        "            ResBlock(c_mid, c_mid, c_mid),\n",
        "            ResBlock(c_mid, c_mid, c_mid),\n",
        "            ResBlock(c_mid, c_mid, c_mid),\n",
        "        )\n",
        "        self.res = ResBlock(c_mid, c_mid, c_out)\n",
        "\n",
        "        self.psi = nn.Conv1d(c_mid, 1, kernel_size=3, stride=1, padding=1, dilation=1)\n",
        "\n",
        "    def forward(self, mel, positive, negative):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            mel: mel spectrogram, torch.Tensor of shape (B x C x T)\n",
        "            positive: positive speaker embedding, torch.Tensor of shape (B x d)\n",
        "            negative: negative speaker embedding, torch.Tensor of shape (B x d)\n",
        "        Returns:\n",
        "Nsi\n",
        "        \"\"\"\n",
        "        pred1 = self.psi(self.phi(mel))\n",
        "        pred = self.res(self.phi(mel))\n",
        "        pred2 = torch.bmm(positive.unsqueeze(1), pred)\n",
        "        pred3 = torch.bmm(negative.unsqueeze(1), pred)\n",
        "        result = pred1 + pred2 - pred3\n",
        "        result = result.squeeze(1)\n",
        "        # result = torch.mean(result, dim=-1)\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09b9712f-2399-4367-b518-930c6b53ef30",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "09b9712f-2399-4367-b518-930c6b53ef30"
      },
      "source": [
        "### 3.4. Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae45085d-4f0b-4245-a738-e123584f409a",
      "metadata": {
        "id": "ae45085d-4f0b-4245-a738-e123584f409a"
      },
      "outputs": [],
      "source": [
        "class GANLoss(nn.Module):\n",
        "    def __init__(self, conf):\n",
        "        self.conf = conf\n",
        "        super(GANLoss, self).__init__()\n",
        "        self.register_buffer('real_label', torch.tensor(self.conf.real))\n",
        "        self.register_buffer('fake_label', torch.tensor(self.conf.fake))\n",
        "        gan_mode = self.conf.gan_mode\n",
        "        self.gan_mode = gan_mode\n",
        "        if gan_mode == 'lsgan':\n",
        "            self.loss = nn.MSELoss()\n",
        "        elif gan_mode == 'vanilla':\n",
        "            self.loss = nn.BCEWithLogitsLoss()\n",
        "        elif gan_mode in ['wgangp', 'hinge']:\n",
        "            self.loss = None\n",
        "        else:\n",
        "            raise NotImplementedError('gan mode %s not implemented' % gan_mode)\n",
        "\n",
        "    def get_target_tensor(self, prediction, target_is_real):\n",
        "        \"\"\"Create label tensors with the same size as the input.\n",
        "        Parameters:\n",
        "            prediction (tensor) - - tpyically the prediction from a discriminator\n",
        "            target_is_real (bool) - - if the ground truth label is for real images or fake images\n",
        "        Returns:\n",
        "            A label tensor filled with ground truth label, and with the size of the input\n",
        "        \"\"\"\n",
        "\n",
        "        if target_is_real:\n",
        "            target_tensor = self.real_label\n",
        "        else:\n",
        "            target_tensor = self.fake_label\n",
        "        return target_tensor.type_as(prediction).expand_as(prediction)\n",
        "\n",
        "    def __call__(self, prediction, target_is_real, for_discriminator=True):\n",
        "        \"\"\"Calculate loss given Discriminator's output and grount truth labels.\n",
        "        Parameters:\n",
        "            prediction (tensor) - - tpyically the prediction output from a discriminator\n",
        "            target_is_real (bool) - - if the ground truth label is for real images or fake images\n",
        "        Returns:\n",
        "            the calculated loss.\n",
        "        \"\"\"\n",
        "        if self.gan_mode in ['lsgan', 'vanilla']:\n",
        "            target_tensor = self.get_target_tensor(prediction, target_is_real)\n",
        "            loss = self.loss(prediction, target_tensor)\n",
        "        elif self.gan_mode == 'wgangp':\n",
        "            if target_is_real:\n",
        "                loss = -prediction.mean()\n",
        "            else:\n",
        "                loss = prediction.mean()\n",
        "        elif self.gan_mode == 'hinge':\n",
        "            if not for_discriminator:\n",
        "                loss = -prediction.mean()\n",
        "            else:\n",
        "                if target_is_real:\n",
        "                    loss = torch.nn.ReLU()(1.0 - prediction).mean()\n",
        "                else:\n",
        "                    loss = torch.nn.ReLU()(1.0 + prediction).mean()\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99784a2e-13b1-49ad-b3aa-d248dd7cc4b1",
      "metadata": {
        "id": "99784a2e-13b1-49ad-b3aa-d248dd7cc4b1"
      },
      "source": [
        "## 4. Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f9440a0-642f-4c5f-a8bc-eb9804a8d51a",
      "metadata": {
        "id": "1f9440a0-642f-4c5f-a8bc-eb9804a8d51a"
      },
      "outputs": [],
      "source": [
        "def step(batch):\n",
        "    logs = {}\n",
        "    loss = {}\n",
        "    logs.update(batch)\n",
        "    \n",
        "    logs['lps'] = analysis.linguistic(batch['gt_audio_16k_f'])\n",
        "    logs['s_pos'] = analysis.speaker(batch['gt_audio_16k'])\n",
        "    logs['s_neg'] = analysis.speaker(batch['gt_audio_16k_negative'])\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logs['e'] = analysis.energy(batch['gt_mel_22k'])\n",
        "        logs['ps'] = analysis.pitch.yingram_batch(batch['gt_audio_22k_g'])\n",
        "        logs['ps'] = logs['ps'][:, 19:69]\n",
        "\n",
        "    result = synthesis(logs['lps'], logs['s_pos'], logs['e'], logs['ps'])\n",
        "    logs.update(result)\n",
        "\n",
        "    loss['mel'] = F.l1_loss(logs['gen_mel'], logs['gt_mel_22k'])\n",
        "    loss['backward'] = loss['mel']\n",
        "\n",
        "    # G step\n",
        "    pred_gen = discriminator(logs['gen_mel'], logs['s_pos'], logs['s_neg'])\n",
        "    loss['D_gen_forG'] = gan_loss(pred_gen, True, False)\n",
        "    loss['backward'] = loss['backward'] + 1 * loss['D_gen_forG']\n",
        "\n",
        "    # D step\n",
        "    logs['gen_mel'] = logs['gen_mel'].detach()\n",
        "    logs['s_pos'] = logs['s_pos'].detach()\n",
        "    logs['s_neg'] = logs['s_neg'].detach()\n",
        "    pred_gen = discriminator(logs['gen_mel'], logs['s_pos'], logs['s_neg'])\n",
        "    pred_gt = discriminator(logs['gt_mel_22k'], logs['s_pos'], logs['s_neg'])\n",
        "\n",
        "    loss['D_gen_forD'] = gan_loss(pred_gen, False, True)\n",
        "    loss['D_gt_forD'] = gan_loss(pred_gt, True, True)\n",
        "    loss['D_backward'] = loss['D_gen_forD'] + loss['D_gt_forD']\n",
        "    \n",
        "    return loss, logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d5b8ec5-5e5a-46f3-a9a4-e7b06dde63d5",
      "metadata": {
        "id": "2d5b8ec5-5e5a-46f3-a9a4-e7b06dde63d5"
      },
      "outputs": [],
      "source": [
        "analysis = Analysis()\n",
        "_ = analysis.to(cfg.device)\n",
        "analysis_optimizer = torch.optim.Adam(analysis.parameters(), lr=1e-4, betas=[0.5, 0.9])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8173492-63ee-4205-a986-a15373755310",
      "metadata": {
        "id": "d8173492-63ee-4205-a986-a15373755310"
      },
      "outputs": [],
      "source": [
        "synthesis = Synthesis()\n",
        "_ = synthesis.to(cfg.device)\n",
        "synthesis_optimizer = torch.optim.Adam(synthesis.parameters(), lr=1e-4, betas=[0.5, 0.9])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4e357a0-650e-402a-bab5-0fdd3325a865",
      "metadata": {
        "id": "e4e357a0-650e-402a-bab5-0fdd3325a865"
      },
      "outputs": [],
      "source": [
        "discriminator = Discriminator()\n",
        "_ = discriminator.to(cfg.device)\n",
        "discriminator_optimizer = torch.optim.Adam(discriminator.parameters(), lr=1e-4, betas=[0.5, 0.9])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71db456e-8227-46d1-b4a1-8d7e50ce1f5f",
      "metadata": {
        "id": "71db456e-8227-46d1-b4a1-8d7e50ce1f5f"
      },
      "outputs": [],
      "source": [
        "gan_loss = GANLoss(DictConfig({'gan_mode': 'lsgan', 'real': 1., 'fake': 0.}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb685eb8-74cb-4763-a898-e78ca5ca71db",
      "metadata": {
        "id": "eb685eb8-74cb-4763-a898-e78ca5ca71db"
      },
      "outputs": [],
      "source": [
        "wandb.init(project='voice-conversion')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc11ad43-ca0f-48d1-8846-78f87587e46c",
      "metadata": {
        "id": "bc11ad43-ca0f-48d1-8846-78f87587e46c"
      },
      "outputs": [],
      "source": [
        "for ep in range(50):\n",
        "    pbar = tqdm(dataloader)\n",
        "    for batch in pbar:\n",
        "        batch = {k:v.to(cfg.device) for k,v in batch.items()}\n",
        "\n",
        "        loss, _ = step(batch)\n",
        "\n",
        "        analysis_optimizer.zero_grad()\n",
        "        synthesis_optimizer.zero_grad()\n",
        "        discriminator_optimizer.zero_grad()\n",
        "\n",
        "        loss['backward'].backward()\n",
        "\n",
        "        analysis_optimizer.step()\n",
        "        synthesis_optimizer.step()\n",
        "        discriminator_optimizer.step()\n",
        "        \n",
        "        log = {k:v.item() for k,v in loss.items()}\n",
        "        pbar.set_postfix(log)\n",
        "        wandb.log(log)\n",
        "        \n",
        "    state_dict = {\n",
        "        'analysis': analysis.state_dict(),\n",
        "        'synthesis': synthesis.state_dict(),\n",
        "        'discriminator': discriminator.state_dict(),\n",
        "    }\n",
        "    torch.save(state_dict, 'nansy.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "121c8215-80a7-4f1d-88de-222dffde631b",
      "metadata": {
        "id": "121c8215-80a7-4f1d-88de-222dffde631b"
      },
      "source": [
        "## 5. Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6d2acdb-051e-4385-9f6b-d0de9ab6b094",
      "metadata": {
        "id": "b6d2acdb-051e-4385-9f6b-d0de9ab6b094"
      },
      "outputs": [],
      "source": [
        "state_dict = torch.load('nansy.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92f579f1-aff6-4649-a79b-684fbd968b44",
      "metadata": {
        "id": "92f579f1-aff6-4649-a79b-684fbd968b44"
      },
      "outputs": [],
      "source": [
        "analysis = Analysis()\n",
        "analysis.load_state_dict(state_dict['analysis'])\n",
        "_ = analysis.eval().requires_grad_(False).to(cfg.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69a3043e-6893-4a62-99af-85a1bd62aaf2",
      "metadata": {
        "id": "69a3043e-6893-4a62-99af-85a1bd62aaf2"
      },
      "outputs": [],
      "source": [
        "synthesis = Synthesis()\n",
        "synthesis.load_state_dict(state_dict['synthesis'])\n",
        "_ = synthesis.eval().requires_grad_(False).to(cfg.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd451762-25db-41aa-a669-80a022a402c6",
      "metadata": {
        "id": "cd451762-25db-41aa-a669-80a022a402c6"
      },
      "outputs": [],
      "source": [
        "src_audios = glob('/mnt/tts/vctk')\n",
        "tgt_audios = glob('/mnt/tts/kss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "069f3f24-4da0-4e5c-9f0b-4cca89848123",
      "metadata": {
        "id": "069f3f24-4da0-4e5c-9f0b-4cca89848123"
      },
      "outputs": [],
      "source": [
        "src_file = src_audios[10]\n",
        "tgt_file = tgt_audios[20]\n",
        "\n",
        "src = dataset.get_data(src_file)\n",
        "tgt = dataset.get_data(tgt_file)\n",
        "\n",
        "src = {k:v.to(cfg.device) for k,v in src.items()}\n",
        "tgt = {k:v.to(cfg.device) for k,v in tgt.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d321399-2286-4cf7-bb1d-fa37b971c42c",
      "metadata": {
        "id": "3d321399-2286-4cf7-bb1d-fa37b971c42c"
      },
      "outputs": [],
      "source": [
        "lps = analysis.linguistic(src['gt_audio_16k'])\n",
        "s = analysis.speaker(tgt['gt_audio_16k'])\n",
        "e = analysis.energy(tgt['gt_mel_22k'])\n",
        "ps = analysis.pitch.yingram_batch(tgt['gt_audio_22k'])\n",
        "ps = ps[:, 19:69]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "967a8efe-3bdf-439f-b37d-0a2b2c1efe1a",
      "metadata": {
        "id": "967a8efe-3bdf-439f-b37d-0a2b2c1efe1a"
      },
      "outputs": [],
      "source": [
        "result = synthesis(lps, s, e, ps)\n",
        "audio = result['audio_gen']\n",
        "audio = audio.squeeze()\n",
        "audio = audio * 32768.0\n",
        "audio = audio.cpu().numpy().astype('int16')\n",
        "\n",
        "sf.write('output.wav', audio, 22050)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a0ca2ea-ecad-4b83-938c-c7022d301398",
      "metadata": {
        "id": "9a0ca2ea-ecad-4b83-938c-c7022d301398"
      },
      "outputs": [],
      "source": [
        "IPython.display.Audio(src_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8615816-445a-47cf-9eaf-889ccba5640d",
      "metadata": {
        "id": "b8615816-445a-47cf-9eaf-889ccba5640d"
      },
      "outputs": [],
      "source": [
        "IPython.display.Audio(tgt_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a67395a-4711-4149-b438-a404b08de839",
      "metadata": {
        "id": "7a67395a-4711-4149-b438-a404b08de839"
      },
      "outputs": [],
      "source": [
        "IPython.display.Audio('output.wav')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}