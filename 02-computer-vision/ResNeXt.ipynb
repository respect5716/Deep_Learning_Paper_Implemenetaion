{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNeXt",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzh7yGxNDluC"
      },
      "source": [
        "# RexNeXt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iki6sM3EDsjS"
      },
      "source": [
        "## 0. Paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "subFe3E5DttM"
      },
      "source": [
        "### Info\n",
        "* Title: Aggregated Residual Transformations for Deep Neural Networks\n",
        "* Author: Saining Xie\n",
        "* Task: Image Classification\n",
        "* Link: https://arxiv.org/abs/1611.05431\n",
        "\n",
        "\n",
        "### Features\n",
        "* Dataset: CIFAR-10\n",
        "\n",
        "\n",
        "### Reference\n",
        "* https://github.com/kuangliu/pytorch-cifar\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuaQpOWMD9tz"
      },
      "source": [
        "## 1. Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZizQk84lEC3L"
      },
      "source": [
        "# Libraries\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from glob import glob\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import torchsummary"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsk7XAE2Lz_g"
      },
      "source": [
        "CONFIG = {\n",
        "    'lr': 0.1,\n",
        "    'momentum': 0.9,\n",
        "    'weight_decay': 5e-4,\n",
        "    'batch_size': 128,\n",
        "    'epoch_size': 200,\n",
        "    'base_dir': '/content/drive/Shared drives/Yoon/Project/Doing/Deep Learning Paper Implementation',\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd9D_aouEVMB"
      },
      "source": [
        "## 2. Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-LDNaypRX0c"
      },
      "source": [
        "def create_dataset():\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2439, 0.2616)),\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2439, 0.2616)),\n",
        "    ])\n",
        "\n",
        "    train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "    test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "\n",
        "def create_dataloader(batch_size):\n",
        "    train_dataset, val_dataset, test_dataset = create_dataset()\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzYjD0NkEzIs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc0d59c3-4894-4d7c-9271-2b8bfa823c30"
      },
      "source": [
        "train_loader, val_loader, test_loader = create_dataloader(CONFIG['batch_size'])\n",
        "inputs, targets = next(iter(train_loader))\n",
        "inputs.size(), targets.size()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([128, 3, 32, 32]), torch.Size([128]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLljDsOoFoLN"
      },
      "source": [
        "## 3. Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NbRhIkmFo47"
      },
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    expansion = 2\n",
        "    \n",
        "    def __init__(self, in_C, bottleneck_width, groups, stride):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        bn_C = bottleneck_width * groups\n",
        "        out_C = bn_C * self.expansion\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride > 1 or in_C != out_C:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_C, out_C, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_C)\n",
        "            )\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_C, bn_C, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(bn_C)\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        self.conv2 = nn.Conv2d(bn_C, bn_C, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(bn_C)\n",
        "        self.relu2 = nn.ReLU()\n",
        "\n",
        "        self.conv3 = nn.Conv2d(bn_C, out_C, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(out_C)\n",
        "        self.relu3 = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = self.shortcut(x)\n",
        "        x = self.relu1(self.bn1(self.conv1(x)))\n",
        "        x = self.relu2(self.bn2(self.conv2(x)))\n",
        "        x = self.bn3(self.conv3(x))\n",
        "        x += shortcut\n",
        "        x = self.relu3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNeXt(nn.Module):\n",
        "    def __init__(self, cfg, num_classes=10):\n",
        "        super(ResNeXt, self).__init__()\n",
        "        self.bottleneck_width = cfg['bottleneck_width']\n",
        "        self.groups = cfg['groups']\n",
        "        self.in_C = cfg['in_C']\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(3, self.in_C, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(self.in_C),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.layer1 = self._make_layer(cfg['num_blocks'][0], 1)\n",
        "        self.layer2 = self._make_layer(cfg['num_blocks'][1], 2)\n",
        "        self.layer3 = self._make_layer(cfg['num_blocks'][2], 2)\n",
        "        self.layer4 = self._make_layer(cfg['num_blocks'][3], 2)\n",
        "        \n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(self.in_C, num_classes)\n",
        "        )\n",
        "\n",
        "    def _make_layer(self, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for st in strides:\n",
        "            layers.append(Bottleneck(self.in_C, self.bottleneck_width, self.groups, st))\n",
        "            self.in_C = Bottleneck.expansion * self.groups * self.bottleneck_width\n",
        "        self.bottleneck_width *= 2\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.head(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def ResNeXt50_2x32d():\n",
        "    cfg = {\n",
        "        'in_C': 32,\n",
        "        'groups': 2,\n",
        "        'bottleneck_width': 32,\n",
        "        'num_blocks': [3, 4, 6, 3]\n",
        "    }\n",
        "    return ResNeXt(cfg)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_7_PUNrFpCS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ec00a2a-6578-4115-9745-b28d6d180dac"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)\n",
        "model = ResNeXt50_2x32d().to(device)\n",
        "\n",
        "optimizer = optimizer = torch.optim.SGD(model.parameters(), lr=CONFIG['lr'], \n",
        "    momentum=CONFIG['momentum'], weight_decay=CONFIG['weight_decay'])\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epoch_size'])\n",
        "\n",
        "torchsummary.summary(model, input_size=inputs.size()[1:], device=device)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device:  cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 32, 32]             864\n",
            "       BatchNorm2d-2           [-1, 32, 32, 32]              64\n",
            "              ReLU-3           [-1, 32, 32, 32]               0\n",
            "            Conv2d-4          [-1, 128, 32, 32]           4,096\n",
            "       BatchNorm2d-5          [-1, 128, 32, 32]             256\n",
            "            Conv2d-6           [-1, 64, 32, 32]           2,048\n",
            "       BatchNorm2d-7           [-1, 64, 32, 32]             128\n",
            "              ReLU-8           [-1, 64, 32, 32]               0\n",
            "            Conv2d-9           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-10           [-1, 64, 32, 32]             128\n",
            "             ReLU-11           [-1, 64, 32, 32]               0\n",
            "           Conv2d-12          [-1, 128, 32, 32]           8,192\n",
            "      BatchNorm2d-13          [-1, 128, 32, 32]             256\n",
            "             ReLU-14          [-1, 128, 32, 32]               0\n",
            "       Bottleneck-15          [-1, 128, 32, 32]               0\n",
            "           Conv2d-16           [-1, 64, 32, 32]           8,192\n",
            "      BatchNorm2d-17           [-1, 64, 32, 32]             128\n",
            "             ReLU-18           [-1, 64, 32, 32]               0\n",
            "           Conv2d-19           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-20           [-1, 64, 32, 32]             128\n",
            "             ReLU-21           [-1, 64, 32, 32]               0\n",
            "           Conv2d-22          [-1, 128, 32, 32]           8,192\n",
            "      BatchNorm2d-23          [-1, 128, 32, 32]             256\n",
            "             ReLU-24          [-1, 128, 32, 32]               0\n",
            "       Bottleneck-25          [-1, 128, 32, 32]               0\n",
            "           Conv2d-26           [-1, 64, 32, 32]           8,192\n",
            "      BatchNorm2d-27           [-1, 64, 32, 32]             128\n",
            "             ReLU-28           [-1, 64, 32, 32]               0\n",
            "           Conv2d-29           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-30           [-1, 64, 32, 32]             128\n",
            "             ReLU-31           [-1, 64, 32, 32]               0\n",
            "           Conv2d-32          [-1, 128, 32, 32]           8,192\n",
            "      BatchNorm2d-33          [-1, 128, 32, 32]             256\n",
            "             ReLU-34          [-1, 128, 32, 32]               0\n",
            "       Bottleneck-35          [-1, 128, 32, 32]               0\n",
            "           Conv2d-36          [-1, 256, 16, 16]          32,768\n",
            "      BatchNorm2d-37          [-1, 256, 16, 16]             512\n",
            "           Conv2d-38          [-1, 128, 32, 32]          16,384\n",
            "      BatchNorm2d-39          [-1, 128, 32, 32]             256\n",
            "             ReLU-40          [-1, 128, 32, 32]               0\n",
            "           Conv2d-41          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-42          [-1, 128, 16, 16]             256\n",
            "             ReLU-43          [-1, 128, 16, 16]               0\n",
            "           Conv2d-44          [-1, 256, 16, 16]          32,768\n",
            "      BatchNorm2d-45          [-1, 256, 16, 16]             512\n",
            "             ReLU-46          [-1, 256, 16, 16]               0\n",
            "       Bottleneck-47          [-1, 256, 16, 16]               0\n",
            "           Conv2d-48          [-1, 128, 16, 16]          32,768\n",
            "      BatchNorm2d-49          [-1, 128, 16, 16]             256\n",
            "             ReLU-50          [-1, 128, 16, 16]               0\n",
            "           Conv2d-51          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-52          [-1, 128, 16, 16]             256\n",
            "             ReLU-53          [-1, 128, 16, 16]               0\n",
            "           Conv2d-54          [-1, 256, 16, 16]          32,768\n",
            "      BatchNorm2d-55          [-1, 256, 16, 16]             512\n",
            "             ReLU-56          [-1, 256, 16, 16]               0\n",
            "       Bottleneck-57          [-1, 256, 16, 16]               0\n",
            "           Conv2d-58          [-1, 128, 16, 16]          32,768\n",
            "      BatchNorm2d-59          [-1, 128, 16, 16]             256\n",
            "             ReLU-60          [-1, 128, 16, 16]               0\n",
            "           Conv2d-61          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-62          [-1, 128, 16, 16]             256\n",
            "             ReLU-63          [-1, 128, 16, 16]               0\n",
            "           Conv2d-64          [-1, 256, 16, 16]          32,768\n",
            "      BatchNorm2d-65          [-1, 256, 16, 16]             512\n",
            "             ReLU-66          [-1, 256, 16, 16]               0\n",
            "       Bottleneck-67          [-1, 256, 16, 16]               0\n",
            "           Conv2d-68          [-1, 128, 16, 16]          32,768\n",
            "      BatchNorm2d-69          [-1, 128, 16, 16]             256\n",
            "             ReLU-70          [-1, 128, 16, 16]               0\n",
            "           Conv2d-71          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-72          [-1, 128, 16, 16]             256\n",
            "             ReLU-73          [-1, 128, 16, 16]               0\n",
            "           Conv2d-74          [-1, 256, 16, 16]          32,768\n",
            "      BatchNorm2d-75          [-1, 256, 16, 16]             512\n",
            "             ReLU-76          [-1, 256, 16, 16]               0\n",
            "       Bottleneck-77          [-1, 256, 16, 16]               0\n",
            "           Conv2d-78            [-1, 512, 8, 8]         131,072\n",
            "      BatchNorm2d-79            [-1, 512, 8, 8]           1,024\n",
            "           Conv2d-80          [-1, 256, 16, 16]          65,536\n",
            "      BatchNorm2d-81          [-1, 256, 16, 16]             512\n",
            "             ReLU-82          [-1, 256, 16, 16]               0\n",
            "           Conv2d-83            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-84            [-1, 256, 8, 8]             512\n",
            "             ReLU-85            [-1, 256, 8, 8]               0\n",
            "           Conv2d-86            [-1, 512, 8, 8]         131,072\n",
            "      BatchNorm2d-87            [-1, 512, 8, 8]           1,024\n",
            "             ReLU-88            [-1, 512, 8, 8]               0\n",
            "       Bottleneck-89            [-1, 512, 8, 8]               0\n",
            "           Conv2d-90            [-1, 256, 8, 8]         131,072\n",
            "      BatchNorm2d-91            [-1, 256, 8, 8]             512\n",
            "             ReLU-92            [-1, 256, 8, 8]               0\n",
            "           Conv2d-93            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-94            [-1, 256, 8, 8]             512\n",
            "             ReLU-95            [-1, 256, 8, 8]               0\n",
            "           Conv2d-96            [-1, 512, 8, 8]         131,072\n",
            "      BatchNorm2d-97            [-1, 512, 8, 8]           1,024\n",
            "             ReLU-98            [-1, 512, 8, 8]               0\n",
            "       Bottleneck-99            [-1, 512, 8, 8]               0\n",
            "          Conv2d-100            [-1, 256, 8, 8]         131,072\n",
            "     BatchNorm2d-101            [-1, 256, 8, 8]             512\n",
            "            ReLU-102            [-1, 256, 8, 8]               0\n",
            "          Conv2d-103            [-1, 256, 8, 8]         589,824\n",
            "     BatchNorm2d-104            [-1, 256, 8, 8]             512\n",
            "            ReLU-105            [-1, 256, 8, 8]               0\n",
            "          Conv2d-106            [-1, 512, 8, 8]         131,072\n",
            "     BatchNorm2d-107            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-108            [-1, 512, 8, 8]               0\n",
            "      Bottleneck-109            [-1, 512, 8, 8]               0\n",
            "          Conv2d-110            [-1, 256, 8, 8]         131,072\n",
            "     BatchNorm2d-111            [-1, 256, 8, 8]             512\n",
            "            ReLU-112            [-1, 256, 8, 8]               0\n",
            "          Conv2d-113            [-1, 256, 8, 8]         589,824\n",
            "     BatchNorm2d-114            [-1, 256, 8, 8]             512\n",
            "            ReLU-115            [-1, 256, 8, 8]               0\n",
            "          Conv2d-116            [-1, 512, 8, 8]         131,072\n",
            "     BatchNorm2d-117            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-118            [-1, 512, 8, 8]               0\n",
            "      Bottleneck-119            [-1, 512, 8, 8]               0\n",
            "          Conv2d-120            [-1, 256, 8, 8]         131,072\n",
            "     BatchNorm2d-121            [-1, 256, 8, 8]             512\n",
            "            ReLU-122            [-1, 256, 8, 8]               0\n",
            "          Conv2d-123            [-1, 256, 8, 8]         589,824\n",
            "     BatchNorm2d-124            [-1, 256, 8, 8]             512\n",
            "            ReLU-125            [-1, 256, 8, 8]               0\n",
            "          Conv2d-126            [-1, 512, 8, 8]         131,072\n",
            "     BatchNorm2d-127            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-128            [-1, 512, 8, 8]               0\n",
            "      Bottleneck-129            [-1, 512, 8, 8]               0\n",
            "          Conv2d-130            [-1, 256, 8, 8]         131,072\n",
            "     BatchNorm2d-131            [-1, 256, 8, 8]             512\n",
            "            ReLU-132            [-1, 256, 8, 8]               0\n",
            "          Conv2d-133            [-1, 256, 8, 8]         589,824\n",
            "     BatchNorm2d-134            [-1, 256, 8, 8]             512\n",
            "            ReLU-135            [-1, 256, 8, 8]               0\n",
            "          Conv2d-136            [-1, 512, 8, 8]         131,072\n",
            "     BatchNorm2d-137            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-138            [-1, 512, 8, 8]               0\n",
            "      Bottleneck-139            [-1, 512, 8, 8]               0\n",
            "          Conv2d-140           [-1, 1024, 4, 4]         524,288\n",
            "     BatchNorm2d-141           [-1, 1024, 4, 4]           2,048\n",
            "          Conv2d-142            [-1, 512, 8, 8]         262,144\n",
            "     BatchNorm2d-143            [-1, 512, 8, 8]           1,024\n",
            "            ReLU-144            [-1, 512, 8, 8]               0\n",
            "          Conv2d-145            [-1, 512, 4, 4]       2,359,296\n",
            "     BatchNorm2d-146            [-1, 512, 4, 4]           1,024\n",
            "            ReLU-147            [-1, 512, 4, 4]               0\n",
            "          Conv2d-148           [-1, 1024, 4, 4]         524,288\n",
            "     BatchNorm2d-149           [-1, 1024, 4, 4]           2,048\n",
            "            ReLU-150           [-1, 1024, 4, 4]               0\n",
            "      Bottleneck-151           [-1, 1024, 4, 4]               0\n",
            "          Conv2d-152            [-1, 512, 4, 4]         524,288\n",
            "     BatchNorm2d-153            [-1, 512, 4, 4]           1,024\n",
            "            ReLU-154            [-1, 512, 4, 4]               0\n",
            "          Conv2d-155            [-1, 512, 4, 4]       2,359,296\n",
            "     BatchNorm2d-156            [-1, 512, 4, 4]           1,024\n",
            "            ReLU-157            [-1, 512, 4, 4]               0\n",
            "          Conv2d-158           [-1, 1024, 4, 4]         524,288\n",
            "     BatchNorm2d-159           [-1, 1024, 4, 4]           2,048\n",
            "            ReLU-160           [-1, 1024, 4, 4]               0\n",
            "      Bottleneck-161           [-1, 1024, 4, 4]               0\n",
            "          Conv2d-162            [-1, 512, 4, 4]         524,288\n",
            "     BatchNorm2d-163            [-1, 512, 4, 4]           1,024\n",
            "            ReLU-164            [-1, 512, 4, 4]               0\n",
            "          Conv2d-165            [-1, 512, 4, 4]       2,359,296\n",
            "     BatchNorm2d-166            [-1, 512, 4, 4]           1,024\n",
            "            ReLU-167            [-1, 512, 4, 4]               0\n",
            "          Conv2d-168           [-1, 1024, 4, 4]         524,288\n",
            "     BatchNorm2d-169           [-1, 1024, 4, 4]           2,048\n",
            "            ReLU-170           [-1, 1024, 4, 4]               0\n",
            "      Bottleneck-171           [-1, 1024, 4, 4]               0\n",
            "AdaptiveAvgPool2d-172           [-1, 1024, 1, 1]               0\n",
            "         Flatten-173                 [-1, 1024]               0\n",
            "          Linear-174                   [-1, 10]          10,250\n",
            "================================================================\n",
            "Total params: 16,734,378\n",
            "Trainable params: 16,734,378\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 56.58\n",
            "Params size (MB): 63.84\n",
            "Estimated Total Size (MB): 120.43\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AILnMW_tFpOG"
      },
      "source": [
        "## 4. Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DevYm6PQhuM1"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "        self.avg = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = f'{self.name:10s} {self.avg:.3f}'\n",
        "        return fmtstr\n",
        "\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, meters, loader_length, prefix=\"\"):\n",
        "        self.meters = [AverageMeter(i) for i in meters]\n",
        "        self.loader_length = loader_length\n",
        "        self.prefix = prefix\n",
        "    \n",
        "    def reset(self):\n",
        "        for m in self.meters:\n",
        "            m.reset()\n",
        "    \n",
        "    def update(self, values, n=1):\n",
        "        for m, v in zip(self.meters, values):\n",
        "            m.update(v, n)\n",
        "            self.__setattr__(m.name, m.avg)\n",
        "\n",
        "    def display(self, batch_idx, postfix=\"\"):\n",
        "        batch_info = f'[{batch_idx+1:03d}/{self.loader_length:03d}]'\n",
        "        msg = [self.prefix + ' ' + batch_info]\n",
        "        msg += [str(meter) for meter in self.meters]\n",
        "        msg = ' | '.join(msg)\n",
        "\n",
        "        sys.stdout.write('\\r')\n",
        "        sys.stdout.write(msg + postfix)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "\n",
        "def accuracy(logits, targets):\n",
        "    _, pred = logits.max(1)\n",
        "    acc = pred.eq(targets).float().mean().item()\n",
        "    return acc\n",
        "\n",
        "def criterion(logits, targets):\n",
        "    return F.cross_entropy(logits, targets)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgWgO7V2g4qS"
      },
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self, model, optimizer, device, scheduler=None):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.device = device\n",
        "        self.best_epoch, self.best_acc = 0, 0\n",
        "        \n",
        "\n",
        "    def train(self, train_loader, epoch):\n",
        "        progress = ProgressMeter([\"train_loss\", \"train_acc\"], len(train_loader), prefix=f'EP  {epoch:03d}')\n",
        "        self.model.train()\n",
        "\n",
        "        start_time = time.time()\n",
        "        for idx, (inputs, targets) in enumerate(train_loader):\n",
        "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "            outputs = self.model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            acc = accuracy(outputs, targets)\n",
        "            loss = loss.item()\n",
        "            progress.update([loss, acc], n=inputs.size(0))\n",
        "            if idx % 20 == 0:\n",
        "                progress.display(idx+1)\n",
        "\n",
        "        if self.scheduler: self.scheduler.step()\n",
        "        finish_time = time.time()\n",
        "        epoch_time = finish_time - start_time\n",
        "        progress.display(idx, f' | {epoch_time:.0f}s' + '\\n')\n",
        "\n",
        "    \n",
        "    def validate(self, val_loader, epoch):\n",
        "        progress = ProgressMeter([\"val_loss\", \"val_acc\"], len(val_loader), prefix=f'VAL {epoch:03d}')\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for idx, (inputs, targets) in enumerate(val_loader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                acc = accuracy(outputs, targets)\n",
        "                progress.update([loss, acc], n=inputs.size(0))\n",
        "\n",
        "            if progress.val_acc > self.best_acc:\n",
        "                ckpt = {\n",
        "                    'best_epoch': self.best_epoch,\n",
        "                    'best_acc': self.best_acc,\n",
        "                    'model_state_dict': model.state_dict()\n",
        "                }\n",
        "                torch.save(ckpt, 'ckpt.pt')\n",
        "                self.best_epoch = epoch\n",
        "                self.best_acc = progress.val_acc\n",
        "            \n",
        "            progress.display(idx, '\\n')\n",
        "\n",
        "    \n",
        "    def test(self, test_loader):\n",
        "        progress = ProgressMeter([\"test_loss\", \"test_acc\"], len(test_loader), prefix=f'TEST')\n",
        "        ckpt = torch.load('ckpt.pt')\n",
        "        self.model.load_state_dict(ckpt['model_state_dict'])\n",
        "        self.model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for idx, (inputs, targets) in enumerate(test_loader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                acc = accuracy(outputs, targets)\n",
        "                progress.update([loss, acc], n=inputs.size(0))\n",
        "\n",
        "            progress.display(idx, '\\n')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBBNWN96nzAx"
      },
      "source": [
        "trainer = Trainer(model, optimizer, device, scheduler)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EhdgFezXTyX",
        "outputId": "21e51e21-a445-4201-c8c6-0d8ba8656736"
      },
      "source": [
        "for ep in range(CONFIG['epoch_size']):\n",
        "    print('-' * 65)\n",
        "    trainer.train(train_loader, ep)\n",
        "    trainer.validate(val_loader, ep)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------\n",
            "EP  000 [352/352] | train_loss 2.473 | train_acc  0.183 | 42s\n",
            "VAL 000 [040/040] | val_loss   2.008 | val_acc    0.240\n",
            "-----------------------------------------------------------------\n",
            "EP  001 [352/352] | train_loss 1.899 | train_acc  0.280 | 42s\n",
            "VAL 001 [040/040] | val_loss   1.780 | val_acc    0.330\n",
            "-----------------------------------------------------------------\n",
            "EP  002 [352/352] | train_loss 1.715 | train_acc  0.357 | 42s\n",
            "VAL 002 [040/040] | val_loss   1.594 | val_acc    0.392\n",
            "-----------------------------------------------------------------\n",
            "EP  003 [352/352] | train_loss 1.506 | train_acc  0.441 | 42s\n",
            "VAL 003 [040/040] | val_loss   1.612 | val_acc    0.422\n",
            "-----------------------------------------------------------------\n",
            "EP  004 [352/352] | train_loss 1.282 | train_acc  0.536 | 42s\n",
            "VAL 004 [040/040] | val_loss   1.230 | val_acc    0.559\n",
            "-----------------------------------------------------------------\n",
            "EP  005 [352/352] | train_loss 1.109 | train_acc  0.604 | 42s\n",
            "VAL 005 [040/040] | val_loss   1.043 | val_acc    0.631\n",
            "-----------------------------------------------------------------\n",
            "EP  006 [352/352] | train_loss 0.999 | train_acc  0.642 | 42s\n",
            "VAL 006 [040/040] | val_loss   1.033 | val_acc    0.636\n",
            "-----------------------------------------------------------------\n",
            "EP  007 [352/352] | train_loss 0.923 | train_acc  0.673 | 42s\n",
            "VAL 007 [040/040] | val_loss   1.057 | val_acc    0.633\n",
            "-----------------------------------------------------------------\n",
            "EP  008 [352/352] | train_loss 0.875 | train_acc  0.691 | 42s\n",
            "VAL 008 [040/040] | val_loss   0.980 | val_acc    0.663\n",
            "-----------------------------------------------------------------\n",
            "EP  009 [352/352] | train_loss 0.813 | train_acc  0.717 | 42s\n",
            "VAL 009 [040/040] | val_loss   0.955 | val_acc    0.669\n",
            "-----------------------------------------------------------------\n",
            "EP  010 [352/352] | train_loss 0.752 | train_acc  0.737 | 42s\n",
            "VAL 010 [040/040] | val_loss   0.934 | val_acc    0.682\n",
            "-----------------------------------------------------------------\n",
            "EP  011 [352/352] | train_loss 0.707 | train_acc  0.752 | 42s\n",
            "VAL 011 [040/040] | val_loss   0.881 | val_acc    0.690\n",
            "-----------------------------------------------------------------\n",
            "EP  012 [352/352] | train_loss 0.664 | train_acc  0.769 | 42s\n",
            "VAL 012 [040/040] | val_loss   1.060 | val_acc    0.660\n",
            "-----------------------------------------------------------------\n",
            "EP  013 [352/352] | train_loss 0.643 | train_acc  0.778 | 42s\n",
            "VAL 013 [040/040] | val_loss   0.730 | val_acc    0.745\n",
            "-----------------------------------------------------------------\n",
            "EP  014 [352/352] | train_loss 0.613 | train_acc  0.788 | 42s\n",
            "VAL 014 [040/040] | val_loss   0.841 | val_acc    0.709\n",
            "-----------------------------------------------------------------\n",
            "EP  015 [352/352] | train_loss 0.601 | train_acc  0.793 | 42s\n",
            "VAL 015 [040/040] | val_loss   0.924 | val_acc    0.702\n",
            "-----------------------------------------------------------------\n",
            "EP  016 [352/352] | train_loss 0.588 | train_acc  0.796 | 42s\n",
            "VAL 016 [040/040] | val_loss   0.728 | val_acc    0.755\n",
            "-----------------------------------------------------------------\n",
            "EP  017 [352/352] | train_loss 0.569 | train_acc  0.805 | 42s\n",
            "VAL 017 [040/040] | val_loss   0.876 | val_acc    0.698\n",
            "-----------------------------------------------------------------\n",
            "EP  018 [352/352] | train_loss 0.556 | train_acc  0.807 | 42s\n",
            "VAL 018 [040/040] | val_loss   0.731 | val_acc    0.749\n",
            "-----------------------------------------------------------------\n",
            "EP  019 [352/352] | train_loss 0.541 | train_acc  0.813 | 42s\n",
            "VAL 019 [040/040] | val_loss   0.716 | val_acc    0.749\n",
            "-----------------------------------------------------------------\n",
            "EP  020 [352/352] | train_loss 0.528 | train_acc  0.819 | 42s\n",
            "VAL 020 [040/040] | val_loss   0.663 | val_acc    0.776\n",
            "-----------------------------------------------------------------\n",
            "EP  021 [352/352] | train_loss 0.513 | train_acc  0.824 | 42s\n",
            "VAL 021 [040/040] | val_loss   0.730 | val_acc    0.752\n",
            "-----------------------------------------------------------------\n",
            "EP  022 [352/352] | train_loss 0.505 | train_acc  0.826 | 42s\n",
            "VAL 022 [040/040] | val_loss   0.681 | val_acc    0.766\n",
            "-----------------------------------------------------------------\n",
            "EP  023 [352/352] | train_loss 0.501 | train_acc  0.828 | 42s\n",
            "VAL 023 [040/040] | val_loss   0.774 | val_acc    0.729\n",
            "-----------------------------------------------------------------\n",
            "EP  024 [352/352] | train_loss 0.488 | train_acc  0.833 | 42s\n",
            "VAL 024 [040/040] | val_loss   0.889 | val_acc    0.712\n",
            "-----------------------------------------------------------------\n",
            "EP  025 [352/352] | train_loss 0.491 | train_acc  0.831 | 42s\n",
            "VAL 025 [040/040] | val_loss   0.675 | val_acc    0.774\n",
            "-----------------------------------------------------------------\n",
            "EP  026 [352/352] | train_loss 0.470 | train_acc  0.839 | 42s\n",
            "VAL 026 [040/040] | val_loss   0.700 | val_acc    0.761\n",
            "-----------------------------------------------------------------\n",
            "EP  027 [352/352] | train_loss 0.467 | train_acc  0.839 | 42s\n",
            "VAL 027 [040/040] | val_loss   0.789 | val_acc    0.743\n",
            "-----------------------------------------------------------------\n",
            "EP  028 [352/352] | train_loss 0.468 | train_acc  0.840 | 42s\n",
            "VAL 028 [040/040] | val_loss   1.002 | val_acc    0.696\n",
            "-----------------------------------------------------------------\n",
            "EP  029 [352/352] | train_loss 0.458 | train_acc  0.843 | 42s\n",
            "VAL 029 [040/040] | val_loss   0.806 | val_acc    0.732\n",
            "-----------------------------------------------------------------\n",
            "EP  030 [352/352] | train_loss 0.460 | train_acc  0.842 | 42s\n",
            "VAL 030 [040/040] | val_loss   0.609 | val_acc    0.790\n",
            "-----------------------------------------------------------------\n",
            "EP  031 [352/352] | train_loss 0.447 | train_acc  0.847 | 42s\n",
            "VAL 031 [040/040] | val_loss   0.726 | val_acc    0.766\n",
            "-----------------------------------------------------------------\n",
            "EP  032 [352/352] | train_loss 0.441 | train_acc  0.848 | 42s\n",
            "VAL 032 [040/040] | val_loss   0.642 | val_acc    0.780\n",
            "-----------------------------------------------------------------\n",
            "EP  033 [352/352] | train_loss 0.440 | train_acc  0.848 | 42s\n",
            "VAL 033 [040/040] | val_loss   0.629 | val_acc    0.788\n",
            "-----------------------------------------------------------------\n",
            "EP  034 [352/352] | train_loss 0.433 | train_acc  0.851 | 42s\n",
            "VAL 034 [040/040] | val_loss   0.657 | val_acc    0.777\n",
            "-----------------------------------------------------------------\n",
            "EP  035 [352/352] | train_loss 0.427 | train_acc  0.854 | 42s\n",
            "VAL 035 [040/040] | val_loss   0.625 | val_acc    0.795\n",
            "-----------------------------------------------------------------\n",
            "EP  036 [352/352] | train_loss 0.426 | train_acc  0.852 | 42s\n",
            "VAL 036 [040/040] | val_loss   0.591 | val_acc    0.795\n",
            "-----------------------------------------------------------------\n",
            "EP  037 [352/352] | train_loss 0.420 | train_acc  0.854 | 42s\n",
            "VAL 037 [040/040] | val_loss   0.509 | val_acc    0.830\n",
            "-----------------------------------------------------------------\n",
            "EP  038 [352/352] | train_loss 0.417 | train_acc  0.856 | 42s\n",
            "VAL 038 [040/040] | val_loss   0.618 | val_acc    0.802\n",
            "-----------------------------------------------------------------\n",
            "EP  039 [352/352] | train_loss 0.413 | train_acc  0.858 | 42s\n",
            "VAL 039 [040/040] | val_loss   0.655 | val_acc    0.782\n",
            "-----------------------------------------------------------------\n",
            "EP  040 [352/352] | train_loss 0.411 | train_acc  0.859 | 42s\n",
            "VAL 040 [040/040] | val_loss   0.532 | val_acc    0.819\n",
            "-----------------------------------------------------------------\n",
            "EP  041 [352/352] | train_loss 0.402 | train_acc  0.863 | 42s\n",
            "VAL 041 [040/040] | val_loss   0.597 | val_acc    0.796\n",
            "-----------------------------------------------------------------\n",
            "EP  042 [352/352] | train_loss 0.405 | train_acc  0.862 | 42s\n",
            "VAL 042 [040/040] | val_loss   0.801 | val_acc    0.740\n",
            "-----------------------------------------------------------------\n",
            "EP  043 [352/352] | train_loss 0.399 | train_acc  0.865 | 42s\n",
            "VAL 043 [040/040] | val_loss   0.540 | val_acc    0.815\n",
            "-----------------------------------------------------------------\n",
            "EP  044 [352/352] | train_loss 0.399 | train_acc  0.864 | 43s\n",
            "VAL 044 [040/040] | val_loss   0.631 | val_acc    0.788\n",
            "-----------------------------------------------------------------\n",
            "EP  045 [352/352] | train_loss 0.391 | train_acc  0.866 | 43s\n",
            "VAL 045 [040/040] | val_loss   0.468 | val_acc    0.843\n",
            "-----------------------------------------------------------------\n",
            "EP  046 [352/352] | train_loss 0.392 | train_acc  0.865 | 42s\n",
            "VAL 046 [040/040] | val_loss   0.526 | val_acc    0.819\n",
            "-----------------------------------------------------------------\n",
            "EP  047 [352/352] | train_loss 0.388 | train_acc  0.866 | 42s\n",
            "VAL 047 [040/040] | val_loss   0.626 | val_acc    0.789\n",
            "-----------------------------------------------------------------\n",
            "EP  048 [352/352] | train_loss 0.387 | train_acc  0.867 | 42s\n",
            "VAL 048 [040/040] | val_loss   0.603 | val_acc    0.801\n",
            "-----------------------------------------------------------------\n",
            "EP  049 [352/352] | train_loss 0.381 | train_acc  0.870 | 43s\n",
            "VAL 049 [040/040] | val_loss   0.598 | val_acc    0.794\n",
            "-----------------------------------------------------------------\n",
            "EP  050 [352/352] | train_loss 0.375 | train_acc  0.872 | 42s\n",
            "VAL 050 [040/040] | val_loss   0.600 | val_acc    0.798\n",
            "-----------------------------------------------------------------\n",
            "EP  051 [352/352] | train_loss 0.372 | train_acc  0.870 | 43s\n",
            "VAL 051 [040/040] | val_loss   0.565 | val_acc    0.811\n",
            "-----------------------------------------------------------------\n",
            "EP  052 [352/352] | train_loss 0.371 | train_acc  0.872 | 42s\n",
            "VAL 052 [040/040] | val_loss   0.522 | val_acc    0.819\n",
            "-----------------------------------------------------------------\n",
            "EP  053 [352/352] | train_loss 0.369 | train_acc  0.874 | 42s\n",
            "VAL 053 [040/040] | val_loss   0.623 | val_acc    0.786\n",
            "-----------------------------------------------------------------\n",
            "EP  054 [352/352] | train_loss 0.361 | train_acc  0.877 | 42s\n",
            "VAL 054 [040/040] | val_loss   0.663 | val_acc    0.779\n",
            "-----------------------------------------------------------------\n",
            "EP  055 [352/352] | train_loss 0.360 | train_acc  0.877 | 42s\n",
            "VAL 055 [040/040] | val_loss   0.495 | val_acc    0.832\n",
            "-----------------------------------------------------------------\n",
            "EP  056 [352/352] | train_loss 0.359 | train_acc  0.878 | 43s\n",
            "VAL 056 [040/040] | val_loss   0.513 | val_acc    0.831\n",
            "-----------------------------------------------------------------\n",
            "EP  057 [352/352] | train_loss 0.360 | train_acc  0.875 | 43s\n",
            "VAL 057 [040/040] | val_loss   0.439 | val_acc    0.856\n",
            "-----------------------------------------------------------------\n",
            "EP  058 [352/352] | train_loss 0.352 | train_acc  0.880 | 43s\n",
            "VAL 058 [040/040] | val_loss   0.504 | val_acc    0.829\n",
            "-----------------------------------------------------------------\n",
            "EP  059 [352/352] | train_loss 0.356 | train_acc  0.878 | 43s\n",
            "VAL 059 [040/040] | val_loss   0.439 | val_acc    0.850\n",
            "-----------------------------------------------------------------\n",
            "EP  060 [352/352] | train_loss 0.346 | train_acc  0.881 | 43s\n",
            "VAL 060 [040/040] | val_loss   0.569 | val_acc    0.805\n",
            "-----------------------------------------------------------------\n",
            "EP  061 [352/352] | train_loss 0.344 | train_acc  0.883 | 43s\n",
            "VAL 061 [040/040] | val_loss   0.414 | val_acc    0.853\n",
            "-----------------------------------------------------------------\n",
            "EP  062 [352/352] | train_loss 0.344 | train_acc  0.881 | 42s\n",
            "VAL 062 [040/040] | val_loss   0.589 | val_acc    0.800\n",
            "-----------------------------------------------------------------\n",
            "EP  063 [352/352] | train_loss 0.349 | train_acc  0.880 | 42s\n",
            "VAL 063 [040/040] | val_loss   0.414 | val_acc    0.859\n",
            "-----------------------------------------------------------------\n",
            "EP  064 [352/352] | train_loss 0.335 | train_acc  0.885 | 42s\n",
            "VAL 064 [040/040] | val_loss   0.536 | val_acc    0.825\n",
            "-----------------------------------------------------------------\n",
            "EP  065 [352/352] | train_loss 0.338 | train_acc  0.883 | 42s\n",
            "VAL 065 [040/040] | val_loss   0.522 | val_acc    0.827\n",
            "-----------------------------------------------------------------\n",
            "EP  066 [352/352] | train_loss 0.331 | train_acc  0.886 | 42s\n",
            "VAL 066 [040/040] | val_loss   0.548 | val_acc    0.817\n",
            "-----------------------------------------------------------------\n",
            "EP  067 [352/352] | train_loss 0.331 | train_acc  0.888 | 42s\n",
            "VAL 067 [040/040] | val_loss   0.460 | val_acc    0.851\n",
            "-----------------------------------------------------------------\n",
            "EP  068 [352/352] | train_loss 0.333 | train_acc  0.887 | 42s\n",
            "VAL 068 [040/040] | val_loss   0.518 | val_acc    0.832\n",
            "-----------------------------------------------------------------\n",
            "EP  069 [352/352] | train_loss 0.328 | train_acc  0.888 | 42s\n",
            "VAL 069 [040/040] | val_loss   0.494 | val_acc    0.837\n",
            "-----------------------------------------------------------------\n",
            "EP  070 [352/352] | train_loss 0.328 | train_acc  0.887 | 42s\n",
            "VAL 070 [040/040] | val_loss   0.437 | val_acc    0.857\n",
            "-----------------------------------------------------------------\n",
            "EP  071 [352/352] | train_loss 0.321 | train_acc  0.890 | 42s\n",
            "VAL 071 [040/040] | val_loss   0.431 | val_acc    0.850\n",
            "-----------------------------------------------------------------\n",
            "EP  072 [352/352] | train_loss 0.317 | train_acc  0.891 | 42s\n",
            "VAL 072 [040/040] | val_loss   0.459 | val_acc    0.841\n",
            "-----------------------------------------------------------------\n",
            "EP  073 [352/352] | train_loss 0.312 | train_acc  0.893 | 42s\n",
            "VAL 073 [040/040] | val_loss   0.456 | val_acc    0.845\n",
            "-----------------------------------------------------------------\n",
            "EP  074 [352/352] | train_loss 0.309 | train_acc  0.894 | 43s\n",
            "VAL 074 [040/040] | val_loss   0.525 | val_acc    0.821\n",
            "-----------------------------------------------------------------\n",
            "EP  075 [352/352] | train_loss 0.312 | train_acc  0.893 | 43s\n",
            "VAL 075 [040/040] | val_loss   0.426 | val_acc    0.858\n",
            "-----------------------------------------------------------------\n",
            "EP  076 [352/352] | train_loss 0.306 | train_acc  0.894 | 43s\n",
            "VAL 076 [040/040] | val_loss   0.471 | val_acc    0.843\n",
            "-----------------------------------------------------------------\n",
            "EP  077 [352/352] | train_loss 0.309 | train_acc  0.894 | 42s\n",
            "VAL 077 [040/040] | val_loss   0.469 | val_acc    0.849\n",
            "-----------------------------------------------------------------\n",
            "EP  078 [352/352] | train_loss 0.298 | train_acc  0.898 | 42s\n",
            "VAL 078 [040/040] | val_loss   0.582 | val_acc    0.804\n",
            "-----------------------------------------------------------------\n",
            "EP  079 [352/352] | train_loss 0.298 | train_acc  0.898 | 43s\n",
            "VAL 079 [040/040] | val_loss   0.428 | val_acc    0.855\n",
            "-----------------------------------------------------------------\n",
            "EP  080 [352/352] | train_loss 0.289 | train_acc  0.901 | 42s\n",
            "VAL 080 [040/040] | val_loss   0.488 | val_acc    0.842\n",
            "-----------------------------------------------------------------\n",
            "EP  081 [352/352] | train_loss 0.291 | train_acc  0.900 | 42s\n",
            "VAL 081 [040/040] | val_loss   0.572 | val_acc    0.812\n",
            "-----------------------------------------------------------------\n",
            "EP  082 [352/352] | train_loss 0.290 | train_acc  0.900 | 43s\n",
            "VAL 082 [040/040] | val_loss   0.483 | val_acc    0.837\n",
            "-----------------------------------------------------------------\n",
            "EP  083 [352/352] | train_loss 0.287 | train_acc  0.901 | 42s\n",
            "VAL 083 [040/040] | val_loss   0.533 | val_acc    0.824\n",
            "-----------------------------------------------------------------\n",
            "EP  084 [352/352] | train_loss 0.281 | train_acc  0.902 | 43s\n",
            "VAL 084 [040/040] | val_loss   0.492 | val_acc    0.835\n",
            "-----------------------------------------------------------------\n",
            "EP  085 [352/352] | train_loss 0.279 | train_acc  0.904 | 43s\n",
            "VAL 085 [040/040] | val_loss   0.398 | val_acc    0.868\n",
            "-----------------------------------------------------------------\n",
            "EP  086 [352/352] | train_loss 0.276 | train_acc  0.905 | 43s\n",
            "VAL 086 [040/040] | val_loss   0.464 | val_acc    0.848\n",
            "-----------------------------------------------------------------\n",
            "EP  087 [352/352] | train_loss 0.273 | train_acc  0.906 | 43s\n",
            "VAL 087 [040/040] | val_loss   0.375 | val_acc    0.873\n",
            "-----------------------------------------------------------------\n",
            "EP  088 [352/352] | train_loss 0.271 | train_acc  0.908 | 43s\n",
            "VAL 088 [040/040] | val_loss   0.457 | val_acc    0.850\n",
            "-----------------------------------------------------------------\n",
            "EP  089 [352/352] | train_loss 0.269 | train_acc  0.908 | 43s\n",
            "VAL 089 [040/040] | val_loss   0.396 | val_acc    0.876\n",
            "-----------------------------------------------------------------\n",
            "EP  090 [352/352] | train_loss 0.262 | train_acc  0.909 | 43s\n",
            "VAL 090 [040/040] | val_loss   0.422 | val_acc    0.856\n",
            "-----------------------------------------------------------------\n",
            "EP  091 [352/352] | train_loss 0.264 | train_acc  0.911 | 43s\n",
            "VAL 091 [040/040] | val_loss   0.367 | val_acc    0.874\n",
            "-----------------------------------------------------------------\n",
            "EP  092 [352/352] | train_loss 0.251 | train_acc  0.913 | 42s\n",
            "VAL 092 [040/040] | val_loss   0.361 | val_acc    0.880\n",
            "-----------------------------------------------------------------\n",
            "EP  093 [352/352] | train_loss 0.256 | train_acc  0.911 | 42s\n",
            "VAL 093 [040/040] | val_loss   0.438 | val_acc    0.857\n",
            "-----------------------------------------------------------------\n",
            "EP  094 [352/352] | train_loss 0.255 | train_acc  0.912 | 42s\n",
            "VAL 094 [040/040] | val_loss   0.452 | val_acc    0.852\n",
            "-----------------------------------------------------------------\n",
            "EP  095 [352/352] | train_loss 0.246 | train_acc  0.916 | 42s\n",
            "VAL 095 [040/040] | val_loss   0.481 | val_acc    0.851\n",
            "-----------------------------------------------------------------\n",
            "EP  096 [352/352] | train_loss 0.251 | train_acc  0.915 | 42s\n",
            "VAL 096 [040/040] | val_loss   0.414 | val_acc    0.870\n",
            "-----------------------------------------------------------------\n",
            "EP  097 [352/352] | train_loss 0.241 | train_acc  0.917 | 42s\n",
            "VAL 097 [040/040] | val_loss   0.396 | val_acc    0.869\n",
            "-----------------------------------------------------------------\n",
            "EP  098 [352/352] | train_loss 0.235 | train_acc  0.919 | 42s\n",
            "VAL 098 [040/040] | val_loss   0.416 | val_acc    0.872\n",
            "-----------------------------------------------------------------\n",
            "EP  099 [352/352] | train_loss 0.241 | train_acc  0.918 | 42s\n",
            "VAL 099 [040/040] | val_loss   0.408 | val_acc    0.870\n",
            "-----------------------------------------------------------------\n",
            "EP  100 [352/352] | train_loss 0.232 | train_acc  0.920 | 43s\n",
            "VAL 100 [040/040] | val_loss   0.389 | val_acc    0.874\n",
            "-----------------------------------------------------------------\n",
            "EP  101 [352/352] | train_loss 0.233 | train_acc  0.919 | 43s\n",
            "VAL 101 [040/040] | val_loss   0.386 | val_acc    0.867\n",
            "-----------------------------------------------------------------\n",
            "EP  102 [352/352] | train_loss 0.231 | train_acc  0.922 | 43s\n",
            "VAL 102 [040/040] | val_loss   0.428 | val_acc    0.861\n",
            "-----------------------------------------------------------------\n",
            "EP  103 [352/352] | train_loss 0.220 | train_acc  0.924 | 43s\n",
            "VAL 103 [040/040] | val_loss   0.377 | val_acc    0.870\n",
            "-----------------------------------------------------------------\n",
            "EP  104 [352/352] | train_loss 0.224 | train_acc  0.923 | 43s\n",
            "VAL 104 [040/040] | val_loss   0.417 | val_acc    0.865\n",
            "-----------------------------------------------------------------\n",
            "EP  105 [352/352] | train_loss 0.214 | train_acc  0.925 | 43s\n",
            "VAL 105 [040/040] | val_loss   0.457 | val_acc    0.849\n",
            "-----------------------------------------------------------------\n",
            "EP  106 [352/352] | train_loss 0.211 | train_acc  0.928 | 43s\n",
            "VAL 106 [040/040] | val_loss   0.377 | val_acc    0.872\n",
            "-----------------------------------------------------------------\n",
            "EP  107 [352/352] | train_loss 0.210 | train_acc  0.927 | 43s\n",
            "VAL 107 [040/040] | val_loss   0.364 | val_acc    0.878\n",
            "-----------------------------------------------------------------\n",
            "EP  108 [352/352] | train_loss 0.209 | train_acc  0.928 | 43s\n",
            "VAL 108 [040/040] | val_loss   0.428 | val_acc    0.856\n",
            "-----------------------------------------------------------------\n",
            "EP  109 [352/352] | train_loss 0.206 | train_acc  0.929 | 42s\n",
            "VAL 109 [040/040] | val_loss   0.381 | val_acc    0.877\n",
            "-----------------------------------------------------------------\n",
            "EP  110 [352/352] | train_loss 0.200 | train_acc  0.932 | 43s\n",
            "VAL 110 [040/040] | val_loss   0.361 | val_acc    0.881\n",
            "-----------------------------------------------------------------\n",
            "EP  111 [352/352] | train_loss 0.201 | train_acc  0.931 | 43s\n",
            "VAL 111 [040/040] | val_loss   0.454 | val_acc    0.850\n",
            "-----------------------------------------------------------------\n",
            "EP  112 [352/352] | train_loss 0.191 | train_acc  0.933 | 43s\n",
            "VAL 112 [040/040] | val_loss   0.353 | val_acc    0.881\n",
            "-----------------------------------------------------------------\n",
            "EP  113 [352/352] | train_loss 0.193 | train_acc  0.934 | 43s\n",
            "VAL 113 [040/040] | val_loss   0.371 | val_acc    0.879\n",
            "-----------------------------------------------------------------\n",
            "EP  114 [352/352] | train_loss 0.189 | train_acc  0.935 | 43s\n",
            "VAL 114 [040/040] | val_loss   0.312 | val_acc    0.894\n",
            "-----------------------------------------------------------------\n",
            "EP  115 [352/352] | train_loss 0.180 | train_acc  0.938 | 43s\n",
            "VAL 115 [040/040] | val_loss   0.449 | val_acc    0.854\n",
            "-----------------------------------------------------------------\n",
            "EP  116 [352/352] | train_loss 0.186 | train_acc  0.937 | 43s\n",
            "VAL 116 [040/040] | val_loss   0.385 | val_acc    0.870\n",
            "-----------------------------------------------------------------\n",
            "EP  117 [352/352] | train_loss 0.177 | train_acc  0.939 | 43s\n",
            "VAL 117 [040/040] | val_loss   0.357 | val_acc    0.884\n",
            "-----------------------------------------------------------------\n",
            "EP  118 [352/352] | train_loss 0.174 | train_acc  0.940 | 43s\n",
            "VAL 118 [040/040] | val_loss   0.313 | val_acc    0.895\n",
            "-----------------------------------------------------------------\n",
            "EP  119 [352/352] | train_loss 0.166 | train_acc  0.944 | 43s\n",
            "VAL 119 [040/040] | val_loss   0.326 | val_acc    0.890\n",
            "-----------------------------------------------------------------\n",
            "EP  120 [352/352] | train_loss 0.172 | train_acc  0.941 | 43s\n",
            "VAL 120 [040/040] | val_loss   0.322 | val_acc    0.891\n",
            "-----------------------------------------------------------------\n",
            "EP  121 [352/352] | train_loss 0.163 | train_acc  0.945 | 43s\n",
            "VAL 121 [040/040] | val_loss   0.295 | val_acc    0.904\n",
            "-----------------------------------------------------------------\n",
            "EP  122 [352/352] | train_loss 0.158 | train_acc  0.947 | 43s\n",
            "VAL 122 [040/040] | val_loss   0.420 | val_acc    0.872\n",
            "-----------------------------------------------------------------\n",
            "EP  123 [352/352] | train_loss 0.157 | train_acc  0.946 | 43s\n",
            "VAL 123 [040/040] | val_loss   0.482 | val_acc    0.858\n",
            "-----------------------------------------------------------------\n",
            "EP  124 [352/352] | train_loss 0.157 | train_acc  0.946 | 43s\n",
            "VAL 124 [040/040] | val_loss   0.309 | val_acc    0.898\n",
            "-----------------------------------------------------------------\n",
            "EP  125 [352/352] | train_loss 0.142 | train_acc  0.951 | 43s\n",
            "VAL 125 [040/040] | val_loss   0.307 | val_acc    0.897\n",
            "-----------------------------------------------------------------\n",
            "EP  126 [352/352] | train_loss 0.145 | train_acc  0.950 | 43s\n",
            "VAL 126 [040/040] | val_loss   0.373 | val_acc    0.880\n",
            "-----------------------------------------------------------------\n",
            "EP  127 [352/352] | train_loss 0.140 | train_acc  0.952 | 43s\n",
            "VAL 127 [040/040] | val_loss   0.329 | val_acc    0.893\n",
            "-----------------------------------------------------------------\n",
            "EP  128 [352/352] | train_loss 0.137 | train_acc  0.953 | 43s\n",
            "VAL 128 [040/040] | val_loss   0.306 | val_acc    0.901\n",
            "-----------------------------------------------------------------\n",
            "EP  129 [352/352] | train_loss 0.133 | train_acc  0.954 | 42s\n",
            "VAL 129 [040/040] | val_loss   0.286 | val_acc    0.910\n",
            "-----------------------------------------------------------------\n",
            "EP  130 [352/352] | train_loss 0.135 | train_acc  0.954 | 42s\n",
            "VAL 130 [040/040] | val_loss   0.360 | val_acc    0.891\n",
            "-----------------------------------------------------------------\n",
            "EP  131 [352/352] | train_loss 0.125 | train_acc  0.958 | 42s\n",
            "VAL 131 [040/040] | val_loss   0.344 | val_acc    0.894\n",
            "-----------------------------------------------------------------\n",
            "EP  132 [352/352] | train_loss 0.122 | train_acc  0.958 | 42s\n",
            "VAL 132 [040/040] | val_loss   0.352 | val_acc    0.890\n",
            "-----------------------------------------------------------------\n",
            "EP  133 [352/352] | train_loss 0.115 | train_acc  0.960 | 42s\n",
            "VAL 133 [040/040] | val_loss   0.363 | val_acc    0.884\n",
            "-----------------------------------------------------------------\n",
            "EP  134 [352/352] | train_loss 0.119 | train_acc  0.959 | 42s\n",
            "VAL 134 [040/040] | val_loss   0.343 | val_acc    0.893\n",
            "-----------------------------------------------------------------\n",
            "EP  135 [352/352] | train_loss 0.110 | train_acc  0.963 | 42s\n",
            "VAL 135 [040/040] | val_loss   0.297 | val_acc    0.909\n",
            "-----------------------------------------------------------------\n",
            "EP  136 [352/352] | train_loss 0.110 | train_acc  0.962 | 42s\n",
            "VAL 136 [040/040] | val_loss   0.358 | val_acc    0.887\n",
            "-----------------------------------------------------------------\n",
            "EP  137 [352/352] | train_loss 0.102 | train_acc  0.965 | 43s\n",
            "VAL 137 [040/040] | val_loss   0.396 | val_acc    0.884\n",
            "-----------------------------------------------------------------\n",
            "EP  138 [352/352] | train_loss 0.099 | train_acc  0.966 | 43s\n",
            "VAL 138 [040/040] | val_loss   0.355 | val_acc    0.895\n",
            "-----------------------------------------------------------------\n",
            "EP  139 [352/352] | train_loss 0.099 | train_acc  0.966 | 43s\n",
            "VAL 139 [040/040] | val_loss   0.330 | val_acc    0.895\n",
            "-----------------------------------------------------------------\n",
            "EP  140 [352/352] | train_loss 0.084 | train_acc  0.972 | 43s\n",
            "VAL 140 [040/040] | val_loss   0.298 | val_acc    0.910\n",
            "-----------------------------------------------------------------\n",
            "EP  141 [352/352] | train_loss 0.093 | train_acc  0.969 | 43s\n",
            "VAL 141 [040/040] | val_loss   0.301 | val_acc    0.910\n",
            "-----------------------------------------------------------------\n",
            "EP  142 [352/352] | train_loss 0.081 | train_acc  0.973 | 43s\n",
            "VAL 142 [040/040] | val_loss   0.355 | val_acc    0.893\n",
            "-----------------------------------------------------------------\n",
            "EP  143 [352/352] | train_loss 0.082 | train_acc  0.973 | 43s\n",
            "VAL 143 [040/040] | val_loss   0.302 | val_acc    0.908\n",
            "-----------------------------------------------------------------\n",
            "EP  144 [352/352] | train_loss 0.078 | train_acc  0.974 | 43s\n",
            "VAL 144 [040/040] | val_loss   0.319 | val_acc    0.903\n",
            "-----------------------------------------------------------------\n",
            "EP  145 [352/352] | train_loss 0.075 | train_acc  0.975 | 43s\n",
            "VAL 145 [040/040] | val_loss   0.279 | val_acc    0.916\n",
            "-----------------------------------------------------------------\n",
            "EP  146 [352/352] | train_loss 0.073 | train_acc  0.975 | 43s\n",
            "VAL 146 [040/040] | val_loss   0.277 | val_acc    0.916\n",
            "-----------------------------------------------------------------\n",
            "EP  147 [352/352] | train_loss 0.068 | train_acc  0.977 | 43s\n",
            "VAL 147 [040/040] | val_loss   0.333 | val_acc    0.902\n",
            "-----------------------------------------------------------------\n",
            "EP  148 [352/352] | train_loss 0.065 | train_acc  0.978 | 43s\n",
            "VAL 148 [040/040] | val_loss   0.316 | val_acc    0.903\n",
            "-----------------------------------------------------------------\n",
            "EP  149 [352/352] | train_loss 0.062 | train_acc  0.979 | 43s\n",
            "VAL 149 [040/040] | val_loss   0.245 | val_acc    0.926\n",
            "-----------------------------------------------------------------\n",
            "EP  150 [352/352] | train_loss 0.055 | train_acc  0.982 | 43s\n",
            "VAL 150 [040/040] | val_loss   0.266 | val_acc    0.920\n",
            "-----------------------------------------------------------------\n",
            "EP  151 [352/352] | train_loss 0.052 | train_acc  0.982 | 43s\n",
            "VAL 151 [040/040] | val_loss   0.280 | val_acc    0.917\n",
            "-----------------------------------------------------------------\n",
            "EP  152 [352/352] | train_loss 0.053 | train_acc  0.982 | 43s\n",
            "VAL 152 [040/040] | val_loss   0.267 | val_acc    0.920\n",
            "-----------------------------------------------------------------\n",
            "EP  153 [352/352] | train_loss 0.045 | train_acc  0.985 | 43s\n",
            "VAL 153 [040/040] | val_loss   0.254 | val_acc    0.927\n",
            "-----------------------------------------------------------------\n",
            "EP  154 [352/352] | train_loss 0.047 | train_acc  0.985 | 43s\n",
            "VAL 154 [040/040] | val_loss   0.253 | val_acc    0.927\n",
            "-----------------------------------------------------------------\n",
            "EP  155 [352/352] | train_loss 0.041 | train_acc  0.987 | 43s\n",
            "VAL 155 [040/040] | val_loss   0.251 | val_acc    0.927\n",
            "-----------------------------------------------------------------\n",
            "EP  156 [352/352] | train_loss 0.038 | train_acc  0.988 | 43s\n",
            "VAL 156 [040/040] | val_loss   0.253 | val_acc    0.922\n",
            "-----------------------------------------------------------------\n",
            "EP  157 [352/352] | train_loss 0.033 | train_acc  0.990 | 43s\n",
            "VAL 157 [040/040] | val_loss   0.228 | val_acc    0.938\n",
            "-----------------------------------------------------------------\n",
            "EP  158 [352/352] | train_loss 0.030 | train_acc  0.990 | 43s\n",
            "VAL 158 [040/040] | val_loss   0.256 | val_acc    0.925\n",
            "-----------------------------------------------------------------\n",
            "EP  159 [352/352] | train_loss 0.027 | train_acc  0.992 | 43s\n",
            "VAL 159 [040/040] | val_loss   0.239 | val_acc    0.933\n",
            "-----------------------------------------------------------------\n",
            "EP  160 [352/352] | train_loss 0.022 | train_acc  0.993 | 43s\n",
            "VAL 160 [040/040] | val_loss   0.253 | val_acc    0.932\n",
            "-----------------------------------------------------------------\n",
            "EP  161 [352/352] | train_loss 0.027 | train_acc  0.991 | 43s\n",
            "VAL 161 [040/040] | val_loss   0.241 | val_acc    0.935\n",
            "-----------------------------------------------------------------\n",
            "EP  162 [352/352] | train_loss 0.021 | train_acc  0.994 | 43s\n",
            "VAL 162 [040/040] | val_loss   0.238 | val_acc    0.935\n",
            "-----------------------------------------------------------------\n",
            "EP  163 [352/352] | train_loss 0.019 | train_acc  0.995 | 43s\n",
            "VAL 163 [040/040] | val_loss   0.241 | val_acc    0.931\n",
            "-----------------------------------------------------------------\n",
            "EP  164 [352/352] | train_loss 0.018 | train_acc  0.994 | 43s\n",
            "VAL 164 [040/040] | val_loss   0.225 | val_acc    0.937\n",
            "-----------------------------------------------------------------\n",
            "EP  165 [352/352] | train_loss 0.014 | train_acc  0.996 | 43s\n",
            "VAL 165 [040/040] | val_loss   0.207 | val_acc    0.942\n",
            "-----------------------------------------------------------------\n",
            "EP  166 [352/352] | train_loss 0.013 | train_acc  0.996 | 43s\n",
            "VAL 166 [040/040] | val_loss   0.238 | val_acc    0.936\n",
            "-----------------------------------------------------------------\n",
            "EP  167 [352/352] | train_loss 0.013 | train_acc  0.996 | 43s\n",
            "VAL 167 [040/040] | val_loss   0.233 | val_acc    0.936\n",
            "-----------------------------------------------------------------\n",
            "EP  168 [352/352] | train_loss 0.011 | train_acc  0.997 | 43s\n",
            "VAL 168 [040/040] | val_loss   0.229 | val_acc    0.940\n",
            "-----------------------------------------------------------------\n",
            "EP  169 [352/352] | train_loss 0.006 | train_acc  0.999 | 43s\n",
            "VAL 169 [040/040] | val_loss   0.229 | val_acc    0.941\n",
            "-----------------------------------------------------------------\n",
            "EP  170 [352/352] | train_loss 0.006 | train_acc  0.999 | 43s\n",
            "VAL 170 [040/040] | val_loss   0.224 | val_acc    0.942\n",
            "-----------------------------------------------------------------\n",
            "EP  171 [352/352] | train_loss 0.006 | train_acc  0.999 | 43s\n",
            "VAL 171 [040/040] | val_loss   0.209 | val_acc    0.945\n",
            "-----------------------------------------------------------------\n",
            "EP  172 [352/352] | train_loss 0.006 | train_acc  0.998 | 43s\n",
            "VAL 172 [040/040] | val_loss   0.219 | val_acc    0.941\n",
            "-----------------------------------------------------------------\n",
            "EP  173 [352/352] | train_loss 0.005 | train_acc  0.999 | 43s\n",
            "VAL 173 [040/040] | val_loss   0.213 | val_acc    0.944\n",
            "-----------------------------------------------------------------\n",
            "EP  174 [352/352] | train_loss 0.005 | train_acc  0.999 | 43s\n",
            "VAL 174 [040/040] | val_loss   0.204 | val_acc    0.948\n",
            "-----------------------------------------------------------------\n",
            "EP  175 [352/352] | train_loss 0.004 | train_acc  0.999 | 43s\n",
            "VAL 175 [040/040] | val_loss   0.206 | val_acc    0.947\n",
            "-----------------------------------------------------------------\n",
            "EP  176 [352/352] | train_loss 0.003 | train_acc  0.999 | 43s\n",
            "VAL 176 [040/040] | val_loss   0.205 | val_acc    0.949\n",
            "-----------------------------------------------------------------\n",
            "EP  177 [352/352] | train_loss 0.003 | train_acc  1.000 | 43s\n",
            "VAL 177 [040/040] | val_loss   0.212 | val_acc    0.947\n",
            "-----------------------------------------------------------------\n",
            "EP  178 [352/352] | train_loss 0.002 | train_acc  1.000 | 43s\n",
            "VAL 178 [040/040] | val_loss   0.205 | val_acc    0.946\n",
            "-----------------------------------------------------------------\n",
            "EP  179 [352/352] | train_loss 0.002 | train_acc  1.000 | 43s\n",
            "VAL 179 [040/040] | val_loss   0.198 | val_acc    0.950\n",
            "-----------------------------------------------------------------\n",
            "EP  180 [352/352] | train_loss 0.002 | train_acc  1.000 | 43s\n",
            "VAL 180 [040/040] | val_loss   0.211 | val_acc    0.949\n",
            "-----------------------------------------------------------------\n",
            "EP  181 [352/352] | train_loss 0.002 | train_acc  1.000 | 43s\n",
            "VAL 181 [040/040] | val_loss   0.212 | val_acc    0.947\n",
            "-----------------------------------------------------------------\n",
            "EP  182 [352/352] | train_loss 0.002 | train_acc  1.000 | 43s\n",
            "VAL 182 [040/040] | val_loss   0.204 | val_acc    0.949\n",
            "-----------------------------------------------------------------\n",
            "EP  183 [352/352] | train_loss 0.002 | train_acc  1.000 | 43s\n",
            "VAL 183 [040/040] | val_loss   0.198 | val_acc    0.948\n",
            "-----------------------------------------------------------------\n",
            "EP  184 [352/352] | train_loss 0.002 | train_acc  1.000 | 43s\n",
            "VAL 184 [040/040] | val_loss   0.200 | val_acc    0.950\n",
            "-----------------------------------------------------------------\n",
            "EP  185 [352/352] | train_loss 0.002 | train_acc  1.000 | 43s\n",
            "VAL 185 [040/040] | val_loss   0.213 | val_acc    0.945\n",
            "-----------------------------------------------------------------\n",
            "EP  186 [352/352] | train_loss 0.001 | train_acc  1.000 | 43s\n",
            "VAL 186 [040/040] | val_loss   0.197 | val_acc    0.951\n",
            "-----------------------------------------------------------------\n",
            "EP  187 [352/352] | train_loss 0.001 | train_acc  1.000 | 43s\n",
            "VAL 187 [040/040] | val_loss   0.206 | val_acc    0.949\n",
            "-----------------------------------------------------------------\n",
            "EP  188 [352/352] | train_loss 0.001 | train_acc  1.000 | 43s\n",
            "VAL 188 [040/040] | val_loss   0.209 | val_acc    0.949\n",
            "-----------------------------------------------------------------\n",
            "EP  189 [352/352] | train_loss 0.001 | train_acc  1.000 | 43s\n",
            "VAL 189 [040/040] | val_loss   0.193 | val_acc    0.951\n",
            "-----------------------------------------------------------------\n",
            "EP  190 [352/352] | train_loss 0.001 | train_acc  1.000 | 43s\n",
            "VAL 190 [040/040] | val_loss   0.196 | val_acc    0.951\n",
            "-----------------------------------------------------------------\n",
            "EP  191 [352/352] | train_loss 0.002 | train_acc  1.000 | 43s\n",
            "VAL 191 [040/040] | val_loss   0.207 | val_acc    0.949\n",
            "-----------------------------------------------------------------\n",
            "EP  192 [352/352] | train_loss 0.001 | train_acc  1.000 | 43s\n",
            "VAL 192 [040/040] | val_loss   0.196 | val_acc    0.948\n",
            "-----------------------------------------------------------------\n",
            "EP  193 [352/352] | train_loss 0.001 | train_acc  1.000 | 43s\n",
            "VAL 193 [040/040] | val_loss   0.196 | val_acc    0.948\n",
            "-----------------------------------------------------------------\n",
            "EP  194 [352/352] | train_loss 0.001 | train_acc  1.000 | 43s\n",
            "VAL 194 [040/040] | val_loss   0.209 | val_acc    0.946\n",
            "-----------------------------------------------------------------\n",
            "EP  195 [352/352] | train_loss 0.001 | train_acc  1.000 | 43s\n",
            "VAL 195 [040/040] | val_loss   0.202 | val_acc    0.950\n",
            "-----------------------------------------------------------------\n",
            "EP  196 [352/352] | train_loss 0.001 | train_acc  1.000 | 43s\n",
            "VAL 196 [040/040] | val_loss   0.193 | val_acc    0.952\n",
            "-----------------------------------------------------------------\n",
            "EP  197 [352/352] | train_loss 0.001 | train_acc  1.000 | 43s\n",
            "VAL 197 [040/040] | val_loss   0.195 | val_acc    0.951\n",
            "-----------------------------------------------------------------\n",
            "EP  198 [352/352] | train_loss 0.001 | train_acc  1.000 | 43s\n",
            "VAL 198 [040/040] | val_loss   0.203 | val_acc    0.950\n",
            "-----------------------------------------------------------------\n",
            "EP  199 [352/352] | train_loss 0.001 | train_acc  1.000 | 43s\n",
            "VAL 199 [040/040] | val_loss   0.203 | val_acc    0.946\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zziHauA7qUha",
        "outputId": "6b7b0b57-d595-4d6c-c597-d3be50dac2c6"
      },
      "source": [
        "trainer.test(test_loader)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rTEST [079/079] | test_loss  0.212 | test_acc   0.951\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}