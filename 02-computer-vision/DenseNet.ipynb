{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DenseNet",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "79a5f35991bb49cc86a1c52016eb5095": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_64a7ce6ab3bd48daae6d1ac695eebc0c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d92a87ad8e4f4f4789fdff34e2ae0fd4",
              "IPY_MODEL_5397de4f509c4283a9643545add2065c"
            ]
          }
        },
        "64a7ce6ab3bd48daae6d1ac695eebc0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d92a87ad8e4f4f4789fdff34e2ae0fd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_92700c10c8804a7a94ff73ed4bce0c74",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_99a19eb297ac4f0c92f3e2de22cd05a5"
          }
        },
        "5397de4f509c4283a9643545add2065c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c4e6bd0e967349ce9660cee5c52091b5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [01:01&lt;00:00, 2787835.68it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0a36d77aced34132b02329fb993cc1e4"
          }
        },
        "92700c10c8804a7a94ff73ed4bce0c74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "99a19eb297ac4f0c92f3e2de22cd05a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c4e6bd0e967349ce9660cee5c52091b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0a36d77aced34132b02329fb993cc1e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzh7yGxNDluC"
      },
      "source": [
        "# DenseNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iki6sM3EDsjS"
      },
      "source": [
        "## 0. Paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "subFe3E5DttM"
      },
      "source": [
        "### Info\n",
        "* Title: Densely Connected Convolutional Networks\n",
        "* Author: Gao Huang\n",
        "* Task: Image Classification\n",
        "* Link: https://arxiv.org/abs/1608.06993\n",
        "\n",
        "\n",
        "### Features\n",
        "* Dataset: CIFAR-10\n",
        "\n",
        "\n",
        "### Reference\n",
        "* https://github.com/kuangliu/pytorch-cifar\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuaQpOWMD9tz"
      },
      "source": [
        "## 1. Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZizQk84lEC3L"
      },
      "source": [
        "# Libraries\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from glob import glob\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchsummary import torchsummary"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsk7XAE2Lz_g"
      },
      "source": [
        "CONFIG = {\n",
        "    'lr': 0.1,\n",
        "    'momentum': 0.9,\n",
        "    'weight_decay': 5e-4,\n",
        "    'batch_size': 128,\n",
        "    'epoch_size': 200,\n",
        "    'base_dir': '/content/drive/Shared drives/Yoon/Project/Doing/Deep Learning Paper Implementation',\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd9D_aouEVMB"
      },
      "source": [
        "## 2. Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-LDNaypRX0c"
      },
      "source": [
        "def create_dataset():\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2439, 0.2616)),\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2439, 0.2616)),\n",
        "    ])\n",
        "\n",
        "    train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "    test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [45000, 5000])\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "\n",
        "def create_dataloader(batch_size):\n",
        "    train_dataset, val_dataset, test_dataset = create_dataset()\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "    return train_loader, val_loader, test_loader"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzYjD0NkEzIs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134,
          "referenced_widgets": [
            "79a5f35991bb49cc86a1c52016eb5095",
            "64a7ce6ab3bd48daae6d1ac695eebc0c",
            "d92a87ad8e4f4f4789fdff34e2ae0fd4",
            "5397de4f509c4283a9643545add2065c",
            "92700c10c8804a7a94ff73ed4bce0c74",
            "99a19eb297ac4f0c92f3e2de22cd05a5",
            "c4e6bd0e967349ce9660cee5c52091b5",
            "0a36d77aced34132b02329fb993cc1e4"
          ]
        },
        "outputId": "92ca8912-5d6e-4526-b615-14bebe6ae751"
      },
      "source": [
        "train_loader, val_loader, test_loader = create_dataloader(CONFIG['batch_size'])\n",
        "inputs, targets = next(iter(train_loader))\n",
        "inputs.size(), targets.size()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "79a5f35991bb49cc86a1c52016eb5095",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=170498071.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([128, 3, 32, 32]), torch.Size([128]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLljDsOoFoLN"
      },
      "source": [
        "## 3. Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NbRhIkmFo47"
      },
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_C, growth_rate):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        C = growth_rate * self.expansion\n",
        "        self.bn1 = nn.BatchNorm2d(in_C)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv1 = nn.Conv2d(in_C, C, kernel_size=1, bias=False)\n",
        "        \n",
        "        self.bn2 = nn.BatchNorm2d(C)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(C, growth_rate, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        shortcut = x\n",
        "        x = self.conv1(self.relu1(self.bn1(x)))\n",
        "        x = self.conv2(self.relu2(self.bn2(x)))\n",
        "        x = torch.cat([x, shortcut], dim=1)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Transition(nn.Module):\n",
        "    def __init__(self, in_C, out_C):\n",
        "        super(Transition, self).__init__()\n",
        "        self.bn = nn.BatchNorm2d(in_C)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv = nn.Conv2d(in_C, out_C, kernel_size=1, bias=False)\n",
        "        self.pool = nn.AvgPool2d(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.pool(self.conv(self.relu(self.bn(x))))\n",
        "\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, cfg, num_classes=10):\n",
        "        super(DenseNet, self).__init__()\n",
        "        self.growth_rate = cfg['growth_rate']\n",
        "        self.in_C = self.growth_rate * 2\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(3, self.in_C, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(self.in_C),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.layer1 = self._make_layer(cfg['num_blocks'][0], True)\n",
        "        self.layer2 = self._make_layer(cfg['num_blocks'][1], True)\n",
        "        self.layer3 = self._make_layer(cfg['num_blocks'][2], True)\n",
        "        self.layer4 = self._make_layer(cfg['num_blocks'][3], False)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.BatchNorm2d(self.in_C),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d(1),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(self.in_C, num_classes)\n",
        "        )\n",
        "\n",
        "\n",
        "    def _make_layer(self, num_blocks, down_sample):\n",
        "        layers = []\n",
        "        for _ in range(num_blocks):\n",
        "            layers.append(Bottleneck(self.in_C, self.growth_rate))\n",
        "            self.in_C += self.growth_rate\n",
        "\n",
        "        if down_sample:\n",
        "            out_C = self.in_C // 2\n",
        "            layers.append(Transition(self.in_C, out_C))\n",
        "            self.in_C = out_C\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.head(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def DenseNet121():\n",
        "    cfg = {\n",
        "        'growth_rate': 12,\n",
        "        'num_blocks': [6, 12, 24, 16]\n",
        "    }\n",
        "    return DenseNet(cfg)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_7_PUNrFpCS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f31003f-d0d2-49dd-99a5-a5b27b50086b"
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)\n",
        "model = DenseNet121().to(device)\n",
        "\n",
        "optimizer = optimizer = torch.optim.SGD(model.parameters(), lr=CONFIG['lr'], \n",
        "    momentum=CONFIG['momentum'], weight_decay=CONFIG['weight_decay'])\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CONFIG['epoch_size'])\n",
        "\n",
        "torchsummary.summary(model, input_size=inputs.size()[1:], device=device)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device:  cuda\n",
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 24, 32, 32]             648\n",
            "       BatchNorm2d-2           [-1, 24, 32, 32]              48\n",
            "              ReLU-3           [-1, 24, 32, 32]               0\n",
            "       BatchNorm2d-4           [-1, 24, 32, 32]              48\n",
            "              ReLU-5           [-1, 24, 32, 32]               0\n",
            "            Conv2d-6           [-1, 48, 32, 32]           1,152\n",
            "       BatchNorm2d-7           [-1, 48, 32, 32]              96\n",
            "              ReLU-8           [-1, 48, 32, 32]               0\n",
            "            Conv2d-9           [-1, 12, 32, 32]           5,184\n",
            "       Bottleneck-10           [-1, 36, 32, 32]               0\n",
            "      BatchNorm2d-11           [-1, 36, 32, 32]              72\n",
            "             ReLU-12           [-1, 36, 32, 32]               0\n",
            "           Conv2d-13           [-1, 48, 32, 32]           1,728\n",
            "      BatchNorm2d-14           [-1, 48, 32, 32]              96\n",
            "             ReLU-15           [-1, 48, 32, 32]               0\n",
            "           Conv2d-16           [-1, 12, 32, 32]           5,184\n",
            "       Bottleneck-17           [-1, 48, 32, 32]               0\n",
            "      BatchNorm2d-18           [-1, 48, 32, 32]              96\n",
            "             ReLU-19           [-1, 48, 32, 32]               0\n",
            "           Conv2d-20           [-1, 48, 32, 32]           2,304\n",
            "      BatchNorm2d-21           [-1, 48, 32, 32]              96\n",
            "             ReLU-22           [-1, 48, 32, 32]               0\n",
            "           Conv2d-23           [-1, 12, 32, 32]           5,184\n",
            "       Bottleneck-24           [-1, 60, 32, 32]               0\n",
            "      BatchNorm2d-25           [-1, 60, 32, 32]             120\n",
            "             ReLU-26           [-1, 60, 32, 32]               0\n",
            "           Conv2d-27           [-1, 48, 32, 32]           2,880\n",
            "      BatchNorm2d-28           [-1, 48, 32, 32]              96\n",
            "             ReLU-29           [-1, 48, 32, 32]               0\n",
            "           Conv2d-30           [-1, 12, 32, 32]           5,184\n",
            "       Bottleneck-31           [-1, 72, 32, 32]               0\n",
            "      BatchNorm2d-32           [-1, 72, 32, 32]             144\n",
            "             ReLU-33           [-1, 72, 32, 32]               0\n",
            "           Conv2d-34           [-1, 48, 32, 32]           3,456\n",
            "      BatchNorm2d-35           [-1, 48, 32, 32]              96\n",
            "             ReLU-36           [-1, 48, 32, 32]               0\n",
            "           Conv2d-37           [-1, 12, 32, 32]           5,184\n",
            "       Bottleneck-38           [-1, 84, 32, 32]               0\n",
            "      BatchNorm2d-39           [-1, 84, 32, 32]             168\n",
            "             ReLU-40           [-1, 84, 32, 32]               0\n",
            "           Conv2d-41           [-1, 48, 32, 32]           4,032\n",
            "      BatchNorm2d-42           [-1, 48, 32, 32]              96\n",
            "             ReLU-43           [-1, 48, 32, 32]               0\n",
            "           Conv2d-44           [-1, 12, 32, 32]           5,184\n",
            "       Bottleneck-45           [-1, 96, 32, 32]               0\n",
            "      BatchNorm2d-46           [-1, 96, 32, 32]             192\n",
            "             ReLU-47           [-1, 96, 32, 32]               0\n",
            "           Conv2d-48           [-1, 48, 32, 32]           4,608\n",
            "        AvgPool2d-49           [-1, 48, 16, 16]               0\n",
            "       Transition-50           [-1, 48, 16, 16]               0\n",
            "      BatchNorm2d-51           [-1, 48, 16, 16]              96\n",
            "             ReLU-52           [-1, 48, 16, 16]               0\n",
            "           Conv2d-53           [-1, 48, 16, 16]           2,304\n",
            "      BatchNorm2d-54           [-1, 48, 16, 16]              96\n",
            "             ReLU-55           [-1, 48, 16, 16]               0\n",
            "           Conv2d-56           [-1, 12, 16, 16]           5,184\n",
            "       Bottleneck-57           [-1, 60, 16, 16]               0\n",
            "      BatchNorm2d-58           [-1, 60, 16, 16]             120\n",
            "             ReLU-59           [-1, 60, 16, 16]               0\n",
            "           Conv2d-60           [-1, 48, 16, 16]           2,880\n",
            "      BatchNorm2d-61           [-1, 48, 16, 16]              96\n",
            "             ReLU-62           [-1, 48, 16, 16]               0\n",
            "           Conv2d-63           [-1, 12, 16, 16]           5,184\n",
            "       Bottleneck-64           [-1, 72, 16, 16]               0\n",
            "      BatchNorm2d-65           [-1, 72, 16, 16]             144\n",
            "             ReLU-66           [-1, 72, 16, 16]               0\n",
            "           Conv2d-67           [-1, 48, 16, 16]           3,456\n",
            "      BatchNorm2d-68           [-1, 48, 16, 16]              96\n",
            "             ReLU-69           [-1, 48, 16, 16]               0\n",
            "           Conv2d-70           [-1, 12, 16, 16]           5,184\n",
            "       Bottleneck-71           [-1, 84, 16, 16]               0\n",
            "      BatchNorm2d-72           [-1, 84, 16, 16]             168\n",
            "             ReLU-73           [-1, 84, 16, 16]               0\n",
            "           Conv2d-74           [-1, 48, 16, 16]           4,032\n",
            "      BatchNorm2d-75           [-1, 48, 16, 16]              96\n",
            "             ReLU-76           [-1, 48, 16, 16]               0\n",
            "           Conv2d-77           [-1, 12, 16, 16]           5,184\n",
            "       Bottleneck-78           [-1, 96, 16, 16]               0\n",
            "      BatchNorm2d-79           [-1, 96, 16, 16]             192\n",
            "             ReLU-80           [-1, 96, 16, 16]               0\n",
            "           Conv2d-81           [-1, 48, 16, 16]           4,608\n",
            "      BatchNorm2d-82           [-1, 48, 16, 16]              96\n",
            "             ReLU-83           [-1, 48, 16, 16]               0\n",
            "           Conv2d-84           [-1, 12, 16, 16]           5,184\n",
            "       Bottleneck-85          [-1, 108, 16, 16]               0\n",
            "      BatchNorm2d-86          [-1, 108, 16, 16]             216\n",
            "             ReLU-87          [-1, 108, 16, 16]               0\n",
            "           Conv2d-88           [-1, 48, 16, 16]           5,184\n",
            "      BatchNorm2d-89           [-1, 48, 16, 16]              96\n",
            "             ReLU-90           [-1, 48, 16, 16]               0\n",
            "           Conv2d-91           [-1, 12, 16, 16]           5,184\n",
            "       Bottleneck-92          [-1, 120, 16, 16]               0\n",
            "      BatchNorm2d-93          [-1, 120, 16, 16]             240\n",
            "             ReLU-94          [-1, 120, 16, 16]               0\n",
            "           Conv2d-95           [-1, 48, 16, 16]           5,760\n",
            "      BatchNorm2d-96           [-1, 48, 16, 16]              96\n",
            "             ReLU-97           [-1, 48, 16, 16]               0\n",
            "           Conv2d-98           [-1, 12, 16, 16]           5,184\n",
            "       Bottleneck-99          [-1, 132, 16, 16]               0\n",
            "     BatchNorm2d-100          [-1, 132, 16, 16]             264\n",
            "            ReLU-101          [-1, 132, 16, 16]               0\n",
            "          Conv2d-102           [-1, 48, 16, 16]           6,336\n",
            "     BatchNorm2d-103           [-1, 48, 16, 16]              96\n",
            "            ReLU-104           [-1, 48, 16, 16]               0\n",
            "          Conv2d-105           [-1, 12, 16, 16]           5,184\n",
            "      Bottleneck-106          [-1, 144, 16, 16]               0\n",
            "     BatchNorm2d-107          [-1, 144, 16, 16]             288\n",
            "            ReLU-108          [-1, 144, 16, 16]               0\n",
            "          Conv2d-109           [-1, 48, 16, 16]           6,912\n",
            "     BatchNorm2d-110           [-1, 48, 16, 16]              96\n",
            "            ReLU-111           [-1, 48, 16, 16]               0\n",
            "          Conv2d-112           [-1, 12, 16, 16]           5,184\n",
            "      Bottleneck-113          [-1, 156, 16, 16]               0\n",
            "     BatchNorm2d-114          [-1, 156, 16, 16]             312\n",
            "            ReLU-115          [-1, 156, 16, 16]               0\n",
            "          Conv2d-116           [-1, 48, 16, 16]           7,488\n",
            "     BatchNorm2d-117           [-1, 48, 16, 16]              96\n",
            "            ReLU-118           [-1, 48, 16, 16]               0\n",
            "          Conv2d-119           [-1, 12, 16, 16]           5,184\n",
            "      Bottleneck-120          [-1, 168, 16, 16]               0\n",
            "     BatchNorm2d-121          [-1, 168, 16, 16]             336\n",
            "            ReLU-122          [-1, 168, 16, 16]               0\n",
            "          Conv2d-123           [-1, 48, 16, 16]           8,064\n",
            "     BatchNorm2d-124           [-1, 48, 16, 16]              96\n",
            "            ReLU-125           [-1, 48, 16, 16]               0\n",
            "          Conv2d-126           [-1, 12, 16, 16]           5,184\n",
            "      Bottleneck-127          [-1, 180, 16, 16]               0\n",
            "     BatchNorm2d-128          [-1, 180, 16, 16]             360\n",
            "            ReLU-129          [-1, 180, 16, 16]               0\n",
            "          Conv2d-130           [-1, 48, 16, 16]           8,640\n",
            "     BatchNorm2d-131           [-1, 48, 16, 16]              96\n",
            "            ReLU-132           [-1, 48, 16, 16]               0\n",
            "          Conv2d-133           [-1, 12, 16, 16]           5,184\n",
            "      Bottleneck-134          [-1, 192, 16, 16]               0\n",
            "     BatchNorm2d-135          [-1, 192, 16, 16]             384\n",
            "            ReLU-136          [-1, 192, 16, 16]               0\n",
            "          Conv2d-137           [-1, 96, 16, 16]          18,432\n",
            "       AvgPool2d-138             [-1, 96, 8, 8]               0\n",
            "      Transition-139             [-1, 96, 8, 8]               0\n",
            "     BatchNorm2d-140             [-1, 96, 8, 8]             192\n",
            "            ReLU-141             [-1, 96, 8, 8]               0\n",
            "          Conv2d-142             [-1, 48, 8, 8]           4,608\n",
            "     BatchNorm2d-143             [-1, 48, 8, 8]              96\n",
            "            ReLU-144             [-1, 48, 8, 8]               0\n",
            "          Conv2d-145             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-146            [-1, 108, 8, 8]               0\n",
            "     BatchNorm2d-147            [-1, 108, 8, 8]             216\n",
            "            ReLU-148            [-1, 108, 8, 8]               0\n",
            "          Conv2d-149             [-1, 48, 8, 8]           5,184\n",
            "     BatchNorm2d-150             [-1, 48, 8, 8]              96\n",
            "            ReLU-151             [-1, 48, 8, 8]               0\n",
            "          Conv2d-152             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-153            [-1, 120, 8, 8]               0\n",
            "     BatchNorm2d-154            [-1, 120, 8, 8]             240\n",
            "            ReLU-155            [-1, 120, 8, 8]               0\n",
            "          Conv2d-156             [-1, 48, 8, 8]           5,760\n",
            "     BatchNorm2d-157             [-1, 48, 8, 8]              96\n",
            "            ReLU-158             [-1, 48, 8, 8]               0\n",
            "          Conv2d-159             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-160            [-1, 132, 8, 8]               0\n",
            "     BatchNorm2d-161            [-1, 132, 8, 8]             264\n",
            "            ReLU-162            [-1, 132, 8, 8]               0\n",
            "          Conv2d-163             [-1, 48, 8, 8]           6,336\n",
            "     BatchNorm2d-164             [-1, 48, 8, 8]              96\n",
            "            ReLU-165             [-1, 48, 8, 8]               0\n",
            "          Conv2d-166             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-167            [-1, 144, 8, 8]               0\n",
            "     BatchNorm2d-168            [-1, 144, 8, 8]             288\n",
            "            ReLU-169            [-1, 144, 8, 8]               0\n",
            "          Conv2d-170             [-1, 48, 8, 8]           6,912\n",
            "     BatchNorm2d-171             [-1, 48, 8, 8]              96\n",
            "            ReLU-172             [-1, 48, 8, 8]               0\n",
            "          Conv2d-173             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-174            [-1, 156, 8, 8]               0\n",
            "     BatchNorm2d-175            [-1, 156, 8, 8]             312\n",
            "            ReLU-176            [-1, 156, 8, 8]               0\n",
            "          Conv2d-177             [-1, 48, 8, 8]           7,488\n",
            "     BatchNorm2d-178             [-1, 48, 8, 8]              96\n",
            "            ReLU-179             [-1, 48, 8, 8]               0\n",
            "          Conv2d-180             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-181            [-1, 168, 8, 8]               0\n",
            "     BatchNorm2d-182            [-1, 168, 8, 8]             336\n",
            "            ReLU-183            [-1, 168, 8, 8]               0\n",
            "          Conv2d-184             [-1, 48, 8, 8]           8,064\n",
            "     BatchNorm2d-185             [-1, 48, 8, 8]              96\n",
            "            ReLU-186             [-1, 48, 8, 8]               0\n",
            "          Conv2d-187             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-188            [-1, 180, 8, 8]               0\n",
            "     BatchNorm2d-189            [-1, 180, 8, 8]             360\n",
            "            ReLU-190            [-1, 180, 8, 8]               0\n",
            "          Conv2d-191             [-1, 48, 8, 8]           8,640\n",
            "     BatchNorm2d-192             [-1, 48, 8, 8]              96\n",
            "            ReLU-193             [-1, 48, 8, 8]               0\n",
            "          Conv2d-194             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-195            [-1, 192, 8, 8]               0\n",
            "     BatchNorm2d-196            [-1, 192, 8, 8]             384\n",
            "            ReLU-197            [-1, 192, 8, 8]               0\n",
            "          Conv2d-198             [-1, 48, 8, 8]           9,216\n",
            "     BatchNorm2d-199             [-1, 48, 8, 8]              96\n",
            "            ReLU-200             [-1, 48, 8, 8]               0\n",
            "          Conv2d-201             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-202            [-1, 204, 8, 8]               0\n",
            "     BatchNorm2d-203            [-1, 204, 8, 8]             408\n",
            "            ReLU-204            [-1, 204, 8, 8]               0\n",
            "          Conv2d-205             [-1, 48, 8, 8]           9,792\n",
            "     BatchNorm2d-206             [-1, 48, 8, 8]              96\n",
            "            ReLU-207             [-1, 48, 8, 8]               0\n",
            "          Conv2d-208             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-209            [-1, 216, 8, 8]               0\n",
            "     BatchNorm2d-210            [-1, 216, 8, 8]             432\n",
            "            ReLU-211            [-1, 216, 8, 8]               0\n",
            "          Conv2d-212             [-1, 48, 8, 8]          10,368\n",
            "     BatchNorm2d-213             [-1, 48, 8, 8]              96\n",
            "            ReLU-214             [-1, 48, 8, 8]               0\n",
            "          Conv2d-215             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-216            [-1, 228, 8, 8]               0\n",
            "     BatchNorm2d-217            [-1, 228, 8, 8]             456\n",
            "            ReLU-218            [-1, 228, 8, 8]               0\n",
            "          Conv2d-219             [-1, 48, 8, 8]          10,944\n",
            "     BatchNorm2d-220             [-1, 48, 8, 8]              96\n",
            "            ReLU-221             [-1, 48, 8, 8]               0\n",
            "          Conv2d-222             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-223            [-1, 240, 8, 8]               0\n",
            "     BatchNorm2d-224            [-1, 240, 8, 8]             480\n",
            "            ReLU-225            [-1, 240, 8, 8]               0\n",
            "          Conv2d-226             [-1, 48, 8, 8]          11,520\n",
            "     BatchNorm2d-227             [-1, 48, 8, 8]              96\n",
            "            ReLU-228             [-1, 48, 8, 8]               0\n",
            "          Conv2d-229             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-230            [-1, 252, 8, 8]               0\n",
            "     BatchNorm2d-231            [-1, 252, 8, 8]             504\n",
            "            ReLU-232            [-1, 252, 8, 8]               0\n",
            "          Conv2d-233             [-1, 48, 8, 8]          12,096\n",
            "     BatchNorm2d-234             [-1, 48, 8, 8]              96\n",
            "            ReLU-235             [-1, 48, 8, 8]               0\n",
            "          Conv2d-236             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-237            [-1, 264, 8, 8]               0\n",
            "     BatchNorm2d-238            [-1, 264, 8, 8]             528\n",
            "            ReLU-239            [-1, 264, 8, 8]               0\n",
            "          Conv2d-240             [-1, 48, 8, 8]          12,672\n",
            "     BatchNorm2d-241             [-1, 48, 8, 8]              96\n",
            "            ReLU-242             [-1, 48, 8, 8]               0\n",
            "          Conv2d-243             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-244            [-1, 276, 8, 8]               0\n",
            "     BatchNorm2d-245            [-1, 276, 8, 8]             552\n",
            "            ReLU-246            [-1, 276, 8, 8]               0\n",
            "          Conv2d-247             [-1, 48, 8, 8]          13,248\n",
            "     BatchNorm2d-248             [-1, 48, 8, 8]              96\n",
            "            ReLU-249             [-1, 48, 8, 8]               0\n",
            "          Conv2d-250             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-251            [-1, 288, 8, 8]               0\n",
            "     BatchNorm2d-252            [-1, 288, 8, 8]             576\n",
            "            ReLU-253            [-1, 288, 8, 8]               0\n",
            "          Conv2d-254             [-1, 48, 8, 8]          13,824\n",
            "     BatchNorm2d-255             [-1, 48, 8, 8]              96\n",
            "            ReLU-256             [-1, 48, 8, 8]               0\n",
            "          Conv2d-257             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-258            [-1, 300, 8, 8]               0\n",
            "     BatchNorm2d-259            [-1, 300, 8, 8]             600\n",
            "            ReLU-260            [-1, 300, 8, 8]               0\n",
            "          Conv2d-261             [-1, 48, 8, 8]          14,400\n",
            "     BatchNorm2d-262             [-1, 48, 8, 8]              96\n",
            "            ReLU-263             [-1, 48, 8, 8]               0\n",
            "          Conv2d-264             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-265            [-1, 312, 8, 8]               0\n",
            "     BatchNorm2d-266            [-1, 312, 8, 8]             624\n",
            "            ReLU-267            [-1, 312, 8, 8]               0\n",
            "          Conv2d-268             [-1, 48, 8, 8]          14,976\n",
            "     BatchNorm2d-269             [-1, 48, 8, 8]              96\n",
            "            ReLU-270             [-1, 48, 8, 8]               0\n",
            "          Conv2d-271             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-272            [-1, 324, 8, 8]               0\n",
            "     BatchNorm2d-273            [-1, 324, 8, 8]             648\n",
            "            ReLU-274            [-1, 324, 8, 8]               0\n",
            "          Conv2d-275             [-1, 48, 8, 8]          15,552\n",
            "     BatchNorm2d-276             [-1, 48, 8, 8]              96\n",
            "            ReLU-277             [-1, 48, 8, 8]               0\n",
            "          Conv2d-278             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-279            [-1, 336, 8, 8]               0\n",
            "     BatchNorm2d-280            [-1, 336, 8, 8]             672\n",
            "            ReLU-281            [-1, 336, 8, 8]               0\n",
            "          Conv2d-282             [-1, 48, 8, 8]          16,128\n",
            "     BatchNorm2d-283             [-1, 48, 8, 8]              96\n",
            "            ReLU-284             [-1, 48, 8, 8]               0\n",
            "          Conv2d-285             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-286            [-1, 348, 8, 8]               0\n",
            "     BatchNorm2d-287            [-1, 348, 8, 8]             696\n",
            "            ReLU-288            [-1, 348, 8, 8]               0\n",
            "          Conv2d-289             [-1, 48, 8, 8]          16,704\n",
            "     BatchNorm2d-290             [-1, 48, 8, 8]              96\n",
            "            ReLU-291             [-1, 48, 8, 8]               0\n",
            "          Conv2d-292             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-293            [-1, 360, 8, 8]               0\n",
            "     BatchNorm2d-294            [-1, 360, 8, 8]             720\n",
            "            ReLU-295            [-1, 360, 8, 8]               0\n",
            "          Conv2d-296             [-1, 48, 8, 8]          17,280\n",
            "     BatchNorm2d-297             [-1, 48, 8, 8]              96\n",
            "            ReLU-298             [-1, 48, 8, 8]               0\n",
            "          Conv2d-299             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-300            [-1, 372, 8, 8]               0\n",
            "     BatchNorm2d-301            [-1, 372, 8, 8]             744\n",
            "            ReLU-302            [-1, 372, 8, 8]               0\n",
            "          Conv2d-303             [-1, 48, 8, 8]          17,856\n",
            "     BatchNorm2d-304             [-1, 48, 8, 8]              96\n",
            "            ReLU-305             [-1, 48, 8, 8]               0\n",
            "          Conv2d-306             [-1, 12, 8, 8]           5,184\n",
            "      Bottleneck-307            [-1, 384, 8, 8]               0\n",
            "     BatchNorm2d-308            [-1, 384, 8, 8]             768\n",
            "            ReLU-309            [-1, 384, 8, 8]               0\n",
            "          Conv2d-310            [-1, 192, 8, 8]          73,728\n",
            "       AvgPool2d-311            [-1, 192, 4, 4]               0\n",
            "      Transition-312            [-1, 192, 4, 4]               0\n",
            "     BatchNorm2d-313            [-1, 192, 4, 4]             384\n",
            "            ReLU-314            [-1, 192, 4, 4]               0\n",
            "          Conv2d-315             [-1, 48, 4, 4]           9,216\n",
            "     BatchNorm2d-316             [-1, 48, 4, 4]              96\n",
            "            ReLU-317             [-1, 48, 4, 4]               0\n",
            "          Conv2d-318             [-1, 12, 4, 4]           5,184\n",
            "      Bottleneck-319            [-1, 204, 4, 4]               0\n",
            "     BatchNorm2d-320            [-1, 204, 4, 4]             408\n",
            "            ReLU-321            [-1, 204, 4, 4]               0\n",
            "          Conv2d-322             [-1, 48, 4, 4]           9,792\n",
            "     BatchNorm2d-323             [-1, 48, 4, 4]              96\n",
            "            ReLU-324             [-1, 48, 4, 4]               0\n",
            "          Conv2d-325             [-1, 12, 4, 4]           5,184\n",
            "      Bottleneck-326            [-1, 216, 4, 4]               0\n",
            "     BatchNorm2d-327            [-1, 216, 4, 4]             432\n",
            "            ReLU-328            [-1, 216, 4, 4]               0\n",
            "          Conv2d-329             [-1, 48, 4, 4]          10,368\n",
            "     BatchNorm2d-330             [-1, 48, 4, 4]              96\n",
            "            ReLU-331             [-1, 48, 4, 4]               0\n",
            "          Conv2d-332             [-1, 12, 4, 4]           5,184\n",
            "      Bottleneck-333            [-1, 228, 4, 4]               0\n",
            "     BatchNorm2d-334            [-1, 228, 4, 4]             456\n",
            "            ReLU-335            [-1, 228, 4, 4]               0\n",
            "          Conv2d-336             [-1, 48, 4, 4]          10,944\n",
            "     BatchNorm2d-337             [-1, 48, 4, 4]              96\n",
            "            ReLU-338             [-1, 48, 4, 4]               0\n",
            "          Conv2d-339             [-1, 12, 4, 4]           5,184\n",
            "      Bottleneck-340            [-1, 240, 4, 4]               0\n",
            "     BatchNorm2d-341            [-1, 240, 4, 4]             480\n",
            "            ReLU-342            [-1, 240, 4, 4]               0\n",
            "          Conv2d-343             [-1, 48, 4, 4]          11,520\n",
            "     BatchNorm2d-344             [-1, 48, 4, 4]              96\n",
            "            ReLU-345             [-1, 48, 4, 4]               0\n",
            "          Conv2d-346             [-1, 12, 4, 4]           5,184\n",
            "      Bottleneck-347            [-1, 252, 4, 4]               0\n",
            "     BatchNorm2d-348            [-1, 252, 4, 4]             504\n",
            "            ReLU-349            [-1, 252, 4, 4]               0\n",
            "          Conv2d-350             [-1, 48, 4, 4]          12,096\n",
            "     BatchNorm2d-351             [-1, 48, 4, 4]              96\n",
            "            ReLU-352             [-1, 48, 4, 4]               0\n",
            "          Conv2d-353             [-1, 12, 4, 4]           5,184\n",
            "      Bottleneck-354            [-1, 264, 4, 4]               0\n",
            "     BatchNorm2d-355            [-1, 264, 4, 4]             528\n",
            "            ReLU-356            [-1, 264, 4, 4]               0\n",
            "          Conv2d-357             [-1, 48, 4, 4]          12,672\n",
            "     BatchNorm2d-358             [-1, 48, 4, 4]              96\n",
            "            ReLU-359             [-1, 48, 4, 4]               0\n",
            "          Conv2d-360             [-1, 12, 4, 4]           5,184\n",
            "      Bottleneck-361            [-1, 276, 4, 4]               0\n",
            "     BatchNorm2d-362            [-1, 276, 4, 4]             552\n",
            "            ReLU-363            [-1, 276, 4, 4]               0\n",
            "          Conv2d-364             [-1, 48, 4, 4]          13,248\n",
            "     BatchNorm2d-365             [-1, 48, 4, 4]              96\n",
            "            ReLU-366             [-1, 48, 4, 4]               0\n",
            "          Conv2d-367             [-1, 12, 4, 4]           5,184\n",
            "      Bottleneck-368            [-1, 288, 4, 4]               0\n",
            "     BatchNorm2d-369            [-1, 288, 4, 4]             576\n",
            "            ReLU-370            [-1, 288, 4, 4]               0\n",
            "          Conv2d-371             [-1, 48, 4, 4]          13,824\n",
            "     BatchNorm2d-372             [-1, 48, 4, 4]              96\n",
            "            ReLU-373             [-1, 48, 4, 4]               0\n",
            "          Conv2d-374             [-1, 12, 4, 4]           5,184\n",
            "      Bottleneck-375            [-1, 300, 4, 4]               0\n",
            "     BatchNorm2d-376            [-1, 300, 4, 4]             600\n",
            "            ReLU-377            [-1, 300, 4, 4]               0\n",
            "          Conv2d-378             [-1, 48, 4, 4]          14,400\n",
            "     BatchNorm2d-379             [-1, 48, 4, 4]              96\n",
            "            ReLU-380             [-1, 48, 4, 4]               0\n",
            "          Conv2d-381             [-1, 12, 4, 4]           5,184\n",
            "      Bottleneck-382            [-1, 312, 4, 4]               0\n",
            "     BatchNorm2d-383            [-1, 312, 4, 4]             624\n",
            "            ReLU-384            [-1, 312, 4, 4]               0\n",
            "          Conv2d-385             [-1, 48, 4, 4]          14,976\n",
            "     BatchNorm2d-386             [-1, 48, 4, 4]              96\n",
            "            ReLU-387             [-1, 48, 4, 4]               0\n",
            "          Conv2d-388             [-1, 12, 4, 4]           5,184\n",
            "      Bottleneck-389            [-1, 324, 4, 4]               0\n",
            "     BatchNorm2d-390            [-1, 324, 4, 4]             648\n",
            "            ReLU-391            [-1, 324, 4, 4]               0\n",
            "          Conv2d-392             [-1, 48, 4, 4]          15,552\n",
            "     BatchNorm2d-393             [-1, 48, 4, 4]              96\n",
            "            ReLU-394             [-1, 48, 4, 4]               0\n",
            "          Conv2d-395             [-1, 12, 4, 4]           5,184\n",
            "      Bottleneck-396            [-1, 336, 4, 4]               0\n",
            "     BatchNorm2d-397            [-1, 336, 4, 4]             672\n",
            "            ReLU-398            [-1, 336, 4, 4]               0\n",
            "          Conv2d-399             [-1, 48, 4, 4]          16,128\n",
            "     BatchNorm2d-400             [-1, 48, 4, 4]              96\n",
            "            ReLU-401             [-1, 48, 4, 4]               0\n",
            "          Conv2d-402             [-1, 12, 4, 4]           5,184\n",
            "      Bottleneck-403            [-1, 348, 4, 4]               0\n",
            "     BatchNorm2d-404            [-1, 348, 4, 4]             696\n",
            "            ReLU-405            [-1, 348, 4, 4]               0\n",
            "          Conv2d-406             [-1, 48, 4, 4]          16,704\n",
            "     BatchNorm2d-407             [-1, 48, 4, 4]              96\n",
            "            ReLU-408             [-1, 48, 4, 4]               0\n",
            "          Conv2d-409             [-1, 12, 4, 4]           5,184\n",
            "      Bottleneck-410            [-1, 360, 4, 4]               0\n",
            "     BatchNorm2d-411            [-1, 360, 4, 4]             720\n",
            "            ReLU-412            [-1, 360, 4, 4]               0\n",
            "          Conv2d-413             [-1, 48, 4, 4]          17,280\n",
            "     BatchNorm2d-414             [-1, 48, 4, 4]              96\n",
            "            ReLU-415             [-1, 48, 4, 4]               0\n",
            "          Conv2d-416             [-1, 12, 4, 4]           5,184\n",
            "      Bottleneck-417            [-1, 372, 4, 4]               0\n",
            "     BatchNorm2d-418            [-1, 372, 4, 4]             744\n",
            "            ReLU-419            [-1, 372, 4, 4]               0\n",
            "          Conv2d-420             [-1, 48, 4, 4]          17,856\n",
            "     BatchNorm2d-421             [-1, 48, 4, 4]              96\n",
            "            ReLU-422             [-1, 48, 4, 4]               0\n",
            "          Conv2d-423             [-1, 12, 4, 4]           5,184\n",
            "      Bottleneck-424            [-1, 384, 4, 4]               0\n",
            "     BatchNorm2d-425            [-1, 384, 4, 4]             768\n",
            "            ReLU-426            [-1, 384, 4, 4]               0\n",
            "AdaptiveAvgPool2d-427            [-1, 384, 1, 1]               0\n",
            "         Flatten-428                  [-1, 384]               0\n",
            "          Linear-429                   [-1, 10]           3,850\n",
            "================================================================\n",
            "Total params: 1,000,666\n",
            "Trainable params: 1,000,666\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 43.87\n",
            "Params size (MB): 3.82\n",
            "Estimated Total Size (MB): 47.70\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AILnMW_tFpOG"
      },
      "source": [
        "## 4. Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DevYm6PQhuM1"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "        self.avg = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = f'{self.name:10s} {self.avg:.3f}'\n",
        "        return fmtstr\n",
        "\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, meters, loader_length, prefix=\"\"):\n",
        "        self.meters = [AverageMeter(i) for i in meters]\n",
        "        self.loader_length = loader_length\n",
        "        self.prefix = prefix\n",
        "    \n",
        "    def reset(self):\n",
        "        for m in self.meters:\n",
        "            m.reset()\n",
        "    \n",
        "    def update(self, values, n=1):\n",
        "        for m, v in zip(self.meters, values):\n",
        "            m.update(v, n)\n",
        "            self.__setattr__(m.name, m.avg)\n",
        "\n",
        "    def display(self, batch_idx, postfix=\"\"):\n",
        "        batch_info = f'[{batch_idx+1:03d}/{self.loader_length:03d}]'\n",
        "        msg = [self.prefix + ' ' + batch_info]\n",
        "        msg += [str(meter) for meter in self.meters]\n",
        "        msg = ' | '.join(msg)\n",
        "\n",
        "        sys.stdout.write('\\r')\n",
        "        sys.stdout.write(msg + postfix)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "\n",
        "def accuracy(logits, targets):\n",
        "    _, pred = logits.max(1)\n",
        "    acc = pred.eq(targets).float().mean().item()\n",
        "    return acc\n",
        "\n",
        "def criterion(logits, targets):\n",
        "    return F.cross_entropy(logits, targets)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgWgO7V2g4qS"
      },
      "source": [
        "class Trainer(object):\n",
        "    def __init__(self, model, optimizer, device, scheduler=None):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.device = device\n",
        "        self.best_epoch, self.best_acc = 0, 0\n",
        "        \n",
        "\n",
        "    def train(self, train_loader, epoch):\n",
        "        progress = ProgressMeter([\"train_loss\", \"train_acc\"], len(train_loader), prefix=f'EP  {epoch:03d}')\n",
        "        self.model.train()\n",
        "\n",
        "        start_time = time.time()\n",
        "        for idx, (inputs, targets) in enumerate(train_loader):\n",
        "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "            outputs = self.model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            acc = accuracy(outputs, targets)\n",
        "            loss = loss.item()\n",
        "            progress.update([loss, acc], n=inputs.size(0))\n",
        "            if idx % 20 == 0:\n",
        "                progress.display(idx+1)\n",
        "\n",
        "        if self.scheduler: self.scheduler.step()\n",
        "        finish_time = time.time()\n",
        "        epoch_time = finish_time - start_time\n",
        "        progress.display(idx, f' | {epoch_time:.0f}s' + '\\n')\n",
        "\n",
        "    \n",
        "    def validate(self, val_loader, epoch):\n",
        "        progress = ProgressMeter([\"val_loss\", \"val_acc\"], len(val_loader), prefix=f'VAL {epoch:03d}')\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for idx, (inputs, targets) in enumerate(val_loader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                acc = accuracy(outputs, targets)\n",
        "                progress.update([loss, acc], n=inputs.size(0))\n",
        "\n",
        "            if progress.val_acc > self.best_acc:\n",
        "                ckpt = {\n",
        "                    'best_epoch': self.best_epoch,\n",
        "                    'best_acc': self.best_acc,\n",
        "                    'model_state_dict': model.state_dict()\n",
        "                }\n",
        "                torch.save(ckpt, 'ckpt.pt')\n",
        "                self.best_epoch = epoch\n",
        "                self.best_acc = progress.val_acc\n",
        "            \n",
        "            progress.display(idx, '\\n')\n",
        "\n",
        "    \n",
        "    def test(self, test_loader):\n",
        "        progress = ProgressMeter([\"test_loss\", \"test_acc\"], len(test_loader), prefix=f'TEST')\n",
        "        ckpt = torch.load('ckpt.pt')\n",
        "        self.model.load_state_dict(ckpt['model_state_dict'])\n",
        "        self.model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for idx, (inputs, targets) in enumerate(test_loader):\n",
        "                inputs, targets = inputs.to(device), targets.to(device)\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                acc = accuracy(outputs, targets)\n",
        "                progress.update([loss, acc], n=inputs.size(0))\n",
        "\n",
        "            progress.display(idx, '\\n')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBBNWN96nzAx"
      },
      "source": [
        "trainer = Trainer(model, optimizer, device, scheduler)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EhdgFezXTyX",
        "outputId": "7a4d62b3-6fae-48f3-9236-7a590a0ba46f"
      },
      "source": [
        "for ep in range(CONFIG['epoch_size']):\n",
        "    print('-' * 65)\n",
        "    trainer.train(train_loader, ep)\n",
        "    trainer.validate(val_loader, ep)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------------------------------------------------------\n",
            "EP  000 [352/352] | train_loss 1.599 | train_acc  0.411 | 60s\n",
            "VAL 000 [040/040] | val_loss   1.400 | val_acc    0.494\n",
            "-----------------------------------------------------------------\n",
            "EP  001 [352/352] | train_loss 1.123 | train_acc  0.598 | 60s\n",
            "VAL 001 [040/040] | val_loss   1.064 | val_acc    0.624\n",
            "-----------------------------------------------------------------\n",
            "EP  002 [352/352] | train_loss 0.889 | train_acc  0.684 | 60s\n",
            "VAL 002 [040/040] | val_loss   0.903 | val_acc    0.683\n",
            "-----------------------------------------------------------------\n",
            "EP  003 [352/352] | train_loss 0.746 | train_acc  0.738 | 60s\n",
            "VAL 003 [040/040] | val_loss   0.854 | val_acc    0.707\n",
            "-----------------------------------------------------------------\n",
            "EP  004 [352/352] | train_loss 0.638 | train_acc  0.779 | 60s\n",
            "VAL 004 [040/040] | val_loss   0.632 | val_acc    0.778\n",
            "-----------------------------------------------------------------\n",
            "EP  005 [352/352] | train_loss 0.585 | train_acc  0.798 | 60s\n",
            "VAL 005 [040/040] | val_loss   0.604 | val_acc    0.786\n",
            "-----------------------------------------------------------------\n",
            "EP  006 [352/352] | train_loss 0.542 | train_acc  0.813 | 60s\n",
            "VAL 006 [040/040] | val_loss   0.697 | val_acc    0.768\n",
            "-----------------------------------------------------------------\n",
            "EP  007 [352/352] | train_loss 0.516 | train_acc  0.821 | 60s\n",
            "VAL 007 [040/040] | val_loss   0.810 | val_acc    0.720\n",
            "-----------------------------------------------------------------\n",
            "EP  008 [352/352] | train_loss 0.501 | train_acc  0.827 | 59s\n",
            "VAL 008 [040/040] | val_loss   0.638 | val_acc    0.783\n",
            "-----------------------------------------------------------------\n",
            "EP  009 [352/352] | train_loss 0.491 | train_acc  0.831 | 59s\n",
            "VAL 009 [040/040] | val_loss   0.739 | val_acc    0.734\n",
            "-----------------------------------------------------------------\n",
            "EP  010 [352/352] | train_loss 0.475 | train_acc  0.836 | 60s\n",
            "VAL 010 [040/040] | val_loss   0.645 | val_acc    0.775\n",
            "-----------------------------------------------------------------\n",
            "EP  011 [352/352] | train_loss 0.473 | train_acc  0.837 | 59s\n",
            "VAL 011 [040/040] | val_loss   0.562 | val_acc    0.808\n",
            "-----------------------------------------------------------------\n",
            "EP  012 [352/352] | train_loss 0.474 | train_acc  0.838 | 60s\n",
            "VAL 012 [040/040] | val_loss   0.638 | val_acc    0.786\n",
            "-----------------------------------------------------------------\n",
            "EP  013 [352/352] | train_loss 0.456 | train_acc  0.841 | 59s\n",
            "VAL 013 [040/040] | val_loss   0.775 | val_acc    0.740\n",
            "-----------------------------------------------------------------\n",
            "EP  014 [352/352] | train_loss 0.445 | train_acc  0.845 | 59s\n",
            "VAL 014 [040/040] | val_loss   0.766 | val_acc    0.753\n",
            "-----------------------------------------------------------------\n",
            "EP  015 [352/352] | train_loss 0.439 | train_acc  0.848 | 59s\n",
            "VAL 015 [040/040] | val_loss   0.799 | val_acc    0.740\n",
            "-----------------------------------------------------------------\n",
            "EP  016 [352/352] | train_loss 0.436 | train_acc  0.849 | 59s\n",
            "VAL 016 [040/040] | val_loss   0.811 | val_acc    0.727\n",
            "-----------------------------------------------------------------\n",
            "EP  017 [352/352] | train_loss 0.455 | train_acc  0.844 | 59s\n",
            "VAL 017 [040/040] | val_loss   0.708 | val_acc    0.771\n",
            "-----------------------------------------------------------------\n",
            "EP  018 [352/352] | train_loss 0.436 | train_acc  0.851 | 59s\n",
            "VAL 018 [040/040] | val_loss   0.557 | val_acc    0.813\n",
            "-----------------------------------------------------------------\n",
            "EP  019 [352/352] | train_loss 0.425 | train_acc  0.854 | 59s\n",
            "VAL 019 [040/040] | val_loss   0.529 | val_acc    0.820\n",
            "-----------------------------------------------------------------\n",
            "EP  020 [352/352] | train_loss 0.421 | train_acc  0.855 | 59s\n",
            "VAL 020 [040/040] | val_loss   0.619 | val_acc    0.792\n",
            "-----------------------------------------------------------------\n",
            "EP  021 [352/352] | train_loss 0.410 | train_acc  0.861 | 61s\n",
            "VAL 021 [040/040] | val_loss   0.548 | val_acc    0.808\n",
            "-----------------------------------------------------------------\n",
            "EP  022 [352/352] | train_loss 0.406 | train_acc  0.860 | 60s\n",
            "VAL 022 [040/040] | val_loss   0.581 | val_acc    0.800\n",
            "-----------------------------------------------------------------\n",
            "EP  023 [352/352] | train_loss 0.413 | train_acc  0.857 | 59s\n",
            "VAL 023 [040/040] | val_loss   0.555 | val_acc    0.812\n",
            "-----------------------------------------------------------------\n",
            "EP  024 [352/352] | train_loss 0.403 | train_acc  0.862 | 59s\n",
            "VAL 024 [040/040] | val_loss   0.575 | val_acc    0.802\n",
            "-----------------------------------------------------------------\n",
            "EP  025 [352/352] | train_loss 0.406 | train_acc  0.860 | 59s\n",
            "VAL 025 [040/040] | val_loss   0.630 | val_acc    0.791\n",
            "-----------------------------------------------------------------\n",
            "EP  026 [352/352] | train_loss 0.409 | train_acc  0.858 | 59s\n",
            "VAL 026 [040/040] | val_loss   0.724 | val_acc    0.764\n",
            "-----------------------------------------------------------------\n",
            "EP  027 [352/352] | train_loss 0.396 | train_acc  0.863 | 59s\n",
            "VAL 027 [040/040] | val_loss   0.668 | val_acc    0.769\n",
            "-----------------------------------------------------------------\n",
            "EP  028 [352/352] | train_loss 0.392 | train_acc  0.866 | 59s\n",
            "VAL 028 [040/040] | val_loss   0.511 | val_acc    0.818\n",
            "-----------------------------------------------------------------\n",
            "EP  029 [352/352] | train_loss 0.389 | train_acc  0.866 | 59s\n",
            "VAL 029 [040/040] | val_loss   0.723 | val_acc    0.766\n",
            "-----------------------------------------------------------------\n",
            "EP  030 [352/352] | train_loss 0.384 | train_acc  0.868 | 59s\n",
            "VAL 030 [040/040] | val_loss   0.503 | val_acc    0.834\n",
            "-----------------------------------------------------------------\n",
            "EP  031 [352/352] | train_loss 0.378 | train_acc  0.869 | 59s\n",
            "VAL 031 [040/040] | val_loss   0.437 | val_acc    0.847\n",
            "-----------------------------------------------------------------\n",
            "EP  032 [352/352] | train_loss 0.375 | train_acc  0.872 | 59s\n",
            "VAL 032 [040/040] | val_loss   0.456 | val_acc    0.842\n",
            "-----------------------------------------------------------------\n",
            "EP  033 [352/352] | train_loss 0.382 | train_acc  0.868 | 59s\n",
            "VAL 033 [040/040] | val_loss   0.514 | val_acc    0.829\n",
            "-----------------------------------------------------------------\n",
            "EP  034 [352/352] | train_loss 0.390 | train_acc  0.867 | 59s\n",
            "VAL 034 [040/040] | val_loss   0.701 | val_acc    0.768\n",
            "-----------------------------------------------------------------\n",
            "EP  035 [352/352] | train_loss 0.399 | train_acc  0.864 | 59s\n",
            "VAL 035 [040/040] | val_loss   0.540 | val_acc    0.823\n",
            "-----------------------------------------------------------------\n",
            "EP  036 [352/352] | train_loss 0.378 | train_acc  0.872 | 59s\n",
            "VAL 036 [040/040] | val_loss   0.468 | val_acc    0.833\n",
            "-----------------------------------------------------------------\n",
            "EP  037 [352/352] | train_loss 0.372 | train_acc  0.871 | 59s\n",
            "VAL 037 [040/040] | val_loss   0.514 | val_acc    0.826\n",
            "-----------------------------------------------------------------\n",
            "EP  038 [352/352] | train_loss 0.378 | train_acc  0.871 | 59s\n",
            "VAL 038 [040/040] | val_loss   0.461 | val_acc    0.845\n",
            "-----------------------------------------------------------------\n",
            "EP  039 [352/352] | train_loss 0.370 | train_acc  0.873 | 59s\n",
            "VAL 039 [040/040] | val_loss   0.590 | val_acc    0.807\n",
            "-----------------------------------------------------------------\n",
            "EP  040 [352/352] | train_loss 0.359 | train_acc  0.876 | 59s\n",
            "VAL 040 [040/040] | val_loss   0.493 | val_acc    0.831\n",
            "-----------------------------------------------------------------\n",
            "EP  041 [352/352] | train_loss 0.362 | train_acc  0.875 | 59s\n",
            "VAL 041 [040/040] | val_loss   0.500 | val_acc    0.831\n",
            "-----------------------------------------------------------------\n",
            "EP  042 [352/352] | train_loss 0.356 | train_acc  0.876 | 59s\n",
            "VAL 042 [040/040] | val_loss   0.539 | val_acc    0.827\n",
            "-----------------------------------------------------------------\n",
            "EP  043 [352/352] | train_loss 0.349 | train_acc  0.879 | 59s\n",
            "VAL 043 [040/040] | val_loss   0.575 | val_acc    0.802\n",
            "-----------------------------------------------------------------\n",
            "EP  044 [352/352] | train_loss 0.354 | train_acc  0.878 | 59s\n",
            "VAL 044 [040/040] | val_loss   0.446 | val_acc    0.843\n",
            "-----------------------------------------------------------------\n",
            "EP  045 [352/352] | train_loss 0.349 | train_acc  0.880 | 59s\n",
            "VAL 045 [040/040] | val_loss   0.602 | val_acc    0.808\n",
            "-----------------------------------------------------------------\n",
            "EP  046 [352/352] | train_loss 0.348 | train_acc  0.881 | 59s\n",
            "VAL 046 [040/040] | val_loss   0.506 | val_acc    0.830\n",
            "-----------------------------------------------------------------\n",
            "EP  047 [352/352] | train_loss 0.355 | train_acc  0.877 | 59s\n",
            "VAL 047 [040/040] | val_loss   0.487 | val_acc    0.833\n",
            "-----------------------------------------------------------------\n",
            "EP  048 [352/352] | train_loss 0.350 | train_acc  0.880 | 59s\n",
            "VAL 048 [040/040] | val_loss   0.570 | val_acc    0.809\n",
            "-----------------------------------------------------------------\n",
            "EP  049 [352/352] | train_loss 0.343 | train_acc  0.882 | 59s\n",
            "VAL 049 [040/040] | val_loss   0.558 | val_acc    0.811\n",
            "-----------------------------------------------------------------\n",
            "EP  050 [352/352] | train_loss 0.341 | train_acc  0.882 | 58s\n",
            "VAL 050 [040/040] | val_loss   0.558 | val_acc    0.798\n",
            "-----------------------------------------------------------------\n",
            "EP  051 [352/352] | train_loss 0.336 | train_acc  0.884 | 58s\n",
            "VAL 051 [040/040] | val_loss   0.453 | val_acc    0.846\n",
            "-----------------------------------------------------------------\n",
            "EP  052 [352/352] | train_loss 0.337 | train_acc  0.884 | 60s\n",
            "VAL 052 [040/040] | val_loss   0.609 | val_acc    0.796\n",
            "-----------------------------------------------------------------\n",
            "EP  053 [352/352] | train_loss 0.336 | train_acc  0.884 | 59s\n",
            "VAL 053 [040/040] | val_loss   0.586 | val_acc    0.796\n",
            "-----------------------------------------------------------------\n",
            "EP  054 [352/352] | train_loss 0.352 | train_acc  0.879 | 59s\n",
            "VAL 054 [040/040] | val_loss   0.683 | val_acc    0.777\n",
            "-----------------------------------------------------------------\n",
            "EP  055 [352/352] | train_loss 0.345 | train_acc  0.881 | 58s\n",
            "VAL 055 [040/040] | val_loss   0.531 | val_acc    0.822\n",
            "-----------------------------------------------------------------\n",
            "EP  056 [352/352] | train_loss 0.326 | train_acc  0.888 | 58s\n",
            "VAL 056 [040/040] | val_loss   0.519 | val_acc    0.823\n",
            "-----------------------------------------------------------------\n",
            "EP  057 [352/352] | train_loss 0.331 | train_acc  0.886 | 59s\n",
            "VAL 057 [040/040] | val_loss   0.460 | val_acc    0.842\n",
            "-----------------------------------------------------------------\n",
            "EP  058 [352/352] | train_loss 0.327 | train_acc  0.886 | 59s\n",
            "VAL 058 [040/040] | val_loss   0.512 | val_acc    0.828\n",
            "-----------------------------------------------------------------\n",
            "EP  059 [352/352] | train_loss 0.325 | train_acc  0.888 | 59s\n",
            "VAL 059 [040/040] | val_loss   0.438 | val_acc    0.852\n",
            "-----------------------------------------------------------------\n",
            "EP  060 [352/352] | train_loss 0.324 | train_acc  0.888 | 59s\n",
            "VAL 060 [040/040] | val_loss   0.458 | val_acc    0.843\n",
            "-----------------------------------------------------------------\n",
            "EP  061 [352/352] | train_loss 0.315 | train_acc  0.892 | 58s\n",
            "VAL 061 [040/040] | val_loss   0.444 | val_acc    0.853\n",
            "-----------------------------------------------------------------\n",
            "EP  062 [352/352] | train_loss 0.317 | train_acc  0.893 | 58s\n",
            "VAL 062 [040/040] | val_loss   0.447 | val_acc    0.850\n",
            "-----------------------------------------------------------------\n",
            "EP  063 [352/352] | train_loss 0.317 | train_acc  0.890 | 59s\n",
            "VAL 063 [040/040] | val_loss   0.456 | val_acc    0.842\n",
            "-----------------------------------------------------------------\n",
            "EP  064 [352/352] | train_loss 0.312 | train_acc  0.892 | 59s\n",
            "VAL 064 [040/040] | val_loss   0.558 | val_acc    0.817\n",
            "-----------------------------------------------------------------\n",
            "EP  065 [352/352] | train_loss 0.311 | train_acc  0.893 | 59s\n",
            "VAL 065 [040/040] | val_loss   0.531 | val_acc    0.820\n",
            "-----------------------------------------------------------------\n",
            "EP  066 [352/352] | train_loss 0.308 | train_acc  0.892 | 58s\n",
            "VAL 066 [040/040] | val_loss   0.518 | val_acc    0.823\n",
            "-----------------------------------------------------------------\n",
            "EP  067 [352/352] | train_loss 0.316 | train_acc  0.892 | 58s\n",
            "VAL 067 [040/040] | val_loss   0.456 | val_acc    0.842\n",
            "-----------------------------------------------------------------\n",
            "EP  068 [352/352] | train_loss 0.303 | train_acc  0.895 | 59s\n",
            "VAL 068 [040/040] | val_loss   0.520 | val_acc    0.827\n",
            "-----------------------------------------------------------------\n",
            "EP  069 [352/352] | train_loss 0.307 | train_acc  0.895 | 59s\n",
            "VAL 069 [040/040] | val_loss   0.460 | val_acc    0.842\n",
            "-----------------------------------------------------------------\n",
            "EP  070 [352/352] | train_loss 0.294 | train_acc  0.899 | 58s\n",
            "VAL 070 [040/040] | val_loss   0.442 | val_acc    0.850\n",
            "-----------------------------------------------------------------\n",
            "EP  071 [352/352] | train_loss 0.299 | train_acc  0.896 | 58s\n",
            "VAL 071 [040/040] | val_loss   0.501 | val_acc    0.833\n",
            "-----------------------------------------------------------------\n",
            "EP  072 [352/352] | train_loss 0.299 | train_acc  0.895 | 58s\n",
            "VAL 072 [040/040] | val_loss   0.492 | val_acc    0.830\n",
            "-----------------------------------------------------------------\n",
            "EP  073 [352/352] | train_loss 0.294 | train_acc  0.898 | 59s\n",
            "VAL 073 [040/040] | val_loss   0.423 | val_acc    0.852\n",
            "-----------------------------------------------------------------\n",
            "EP  074 [352/352] | train_loss 0.292 | train_acc  0.899 | 58s\n",
            "VAL 074 [040/040] | val_loss   0.450 | val_acc    0.852\n",
            "-----------------------------------------------------------------\n",
            "EP  075 [352/352] | train_loss 0.291 | train_acc  0.900 | 58s\n",
            "VAL 075 [040/040] | val_loss   0.581 | val_acc    0.814\n",
            "-----------------------------------------------------------------\n",
            "EP  076 [352/352] | train_loss 0.288 | train_acc  0.900 | 58s\n",
            "VAL 076 [040/040] | val_loss   0.359 | val_acc    0.881\n",
            "-----------------------------------------------------------------\n",
            "EP  077 [352/352] | train_loss 0.285 | train_acc  0.902 | 58s\n",
            "VAL 077 [040/040] | val_loss   0.546 | val_acc    0.827\n",
            "-----------------------------------------------------------------\n",
            "EP  078 [352/352] | train_loss 0.281 | train_acc  0.904 | 59s\n",
            "VAL 078 [040/040] | val_loss   0.547 | val_acc    0.812\n",
            "-----------------------------------------------------------------\n",
            "EP  079 [352/352] | train_loss 0.282 | train_acc  0.902 | 62s\n",
            "VAL 079 [040/040] | val_loss   0.447 | val_acc    0.849\n",
            "-----------------------------------------------------------------\n",
            "EP  080 [352/352] | train_loss 0.282 | train_acc  0.903 | 59s\n",
            "VAL 080 [040/040] | val_loss   0.541 | val_acc    0.824\n",
            "-----------------------------------------------------------------\n",
            "EP  081 [352/352] | train_loss 0.277 | train_acc  0.906 | 58s\n",
            "VAL 081 [040/040] | val_loss   0.389 | val_acc    0.863\n",
            "-----------------------------------------------------------------\n",
            "EP  082 [352/352] | train_loss 0.268 | train_acc  0.908 | 58s\n",
            "VAL 082 [040/040] | val_loss   0.369 | val_acc    0.877\n",
            "-----------------------------------------------------------------\n",
            "EP  083 [352/352] | train_loss 0.308 | train_acc  0.893 | 58s\n",
            "VAL 083 [040/040] | val_loss   0.438 | val_acc    0.851\n",
            "-----------------------------------------------------------------\n",
            "EP  084 [352/352] | train_loss 0.277 | train_acc  0.903 | 59s\n",
            "VAL 084 [040/040] | val_loss   0.470 | val_acc    0.845\n",
            "-----------------------------------------------------------------\n",
            "EP  085 [352/352] | train_loss 0.272 | train_acc  0.905 | 59s\n",
            "VAL 085 [040/040] | val_loss   0.446 | val_acc    0.848\n",
            "-----------------------------------------------------------------\n",
            "EP  086 [352/352] | train_loss 0.269 | train_acc  0.907 | 59s\n",
            "VAL 086 [040/040] | val_loss   0.420 | val_acc    0.853\n",
            "-----------------------------------------------------------------\n",
            "EP  087 [352/352] | train_loss 0.265 | train_acc  0.909 | 59s\n",
            "VAL 087 [040/040] | val_loss   0.398 | val_acc    0.864\n",
            "-----------------------------------------------------------------\n",
            "EP  088 [352/352] | train_loss 0.267 | train_acc  0.908 | 59s\n",
            "VAL 088 [040/040] | val_loss   0.402 | val_acc    0.858\n",
            "-----------------------------------------------------------------\n",
            "EP  089 [352/352] | train_loss 0.265 | train_acc  0.907 | 59s\n",
            "VAL 089 [040/040] | val_loss   0.533 | val_acc    0.825\n",
            "-----------------------------------------------------------------\n",
            "EP  090 [352/352] | train_loss 0.258 | train_acc  0.912 | 59s\n",
            "VAL 090 [040/040] | val_loss   0.448 | val_acc    0.849\n",
            "-----------------------------------------------------------------\n",
            "EP  091 [352/352] | train_loss 0.250 | train_acc  0.915 | 59s\n",
            "VAL 091 [040/040] | val_loss   0.384 | val_acc    0.869\n",
            "-----------------------------------------------------------------\n",
            "EP  092 [352/352] | train_loss 0.252 | train_acc  0.913 | 59s\n",
            "VAL 092 [040/040] | val_loss   0.370 | val_acc    0.876\n",
            "-----------------------------------------------------------------\n",
            "EP  093 [352/352] | train_loss 0.257 | train_acc  0.911 | 59s\n",
            "VAL 093 [040/040] | val_loss   0.378 | val_acc    0.878\n",
            "-----------------------------------------------------------------\n",
            "EP  094 [352/352] | train_loss 0.253 | train_acc  0.913 | 59s\n",
            "VAL 094 [040/040] | val_loss   0.379 | val_acc    0.871\n",
            "-----------------------------------------------------------------\n",
            "EP  095 [352/352] | train_loss 0.245 | train_acc  0.917 | 59s\n",
            "VAL 095 [040/040] | val_loss   0.398 | val_acc    0.866\n",
            "-----------------------------------------------------------------\n",
            "EP  096 [352/352] | train_loss 0.240 | train_acc  0.917 | 59s\n",
            "VAL 096 [040/040] | val_loss   0.332 | val_acc    0.886\n",
            "-----------------------------------------------------------------\n",
            "EP  097 [352/352] | train_loss 0.236 | train_acc  0.919 | 59s\n",
            "VAL 097 [040/040] | val_loss   0.388 | val_acc    0.868\n",
            "-----------------------------------------------------------------\n",
            "EP  098 [352/352] | train_loss 0.232 | train_acc  0.921 | 60s\n",
            "VAL 098 [040/040] | val_loss   0.353 | val_acc    0.880\n",
            "-----------------------------------------------------------------\n",
            "EP  099 [352/352] | train_loss 0.243 | train_acc  0.916 | 60s\n",
            "VAL 099 [040/040] | val_loss   0.391 | val_acc    0.868\n",
            "-----------------------------------------------------------------\n",
            "EP  100 [352/352] | train_loss 0.233 | train_acc  0.919 | 63s\n",
            "VAL 100 [040/040] | val_loss   0.413 | val_acc    0.862\n",
            "-----------------------------------------------------------------\n",
            "EP  101 [352/352] | train_loss 0.232 | train_acc  0.919 | 59s\n",
            "VAL 101 [040/040] | val_loss   0.405 | val_acc    0.866\n",
            "-----------------------------------------------------------------\n",
            "EP  102 [352/352] | train_loss 0.225 | train_acc  0.921 | 59s\n",
            "VAL 102 [040/040] | val_loss   0.338 | val_acc    0.885\n",
            "-----------------------------------------------------------------\n",
            "EP  103 [352/352] | train_loss 0.220 | train_acc  0.924 | 59s\n",
            "VAL 103 [040/040] | val_loss   0.402 | val_acc    0.863\n",
            "-----------------------------------------------------------------\n",
            "EP  104 [352/352] | train_loss 0.222 | train_acc  0.922 | 59s\n",
            "VAL 104 [040/040] | val_loss   0.393 | val_acc    0.865\n",
            "-----------------------------------------------------------------\n",
            "EP  105 [352/352] | train_loss 0.218 | train_acc  0.924 | 59s\n",
            "VAL 105 [040/040] | val_loss   0.412 | val_acc    0.859\n",
            "-----------------------------------------------------------------\n",
            "EP  106 [352/352] | train_loss 0.215 | train_acc  0.926 | 59s\n",
            "VAL 106 [040/040] | val_loss   0.432 | val_acc    0.859\n",
            "-----------------------------------------------------------------\n",
            "EP  107 [352/352] | train_loss 0.208 | train_acc  0.929 | 60s\n",
            "VAL 107 [040/040] | val_loss   0.335 | val_acc    0.889\n",
            "-----------------------------------------------------------------\n",
            "EP  108 [352/352] | train_loss 0.204 | train_acc  0.929 | 59s\n",
            "VAL 108 [040/040] | val_loss   0.474 | val_acc    0.845\n",
            "-----------------------------------------------------------------\n",
            "EP  109 [352/352] | train_loss 0.205 | train_acc  0.929 | 59s\n",
            "VAL 109 [040/040] | val_loss   0.334 | val_acc    0.883\n",
            "-----------------------------------------------------------------\n",
            "EP  110 [352/352] | train_loss 0.200 | train_acc  0.930 | 59s\n",
            "VAL 110 [040/040] | val_loss   0.392 | val_acc    0.872\n",
            "-----------------------------------------------------------------\n",
            "EP  111 [352/352] | train_loss 0.210 | train_acc  0.926 | 59s\n",
            "VAL 111 [040/040] | val_loss   0.377 | val_acc    0.875\n",
            "-----------------------------------------------------------------\n",
            "EP  112 [352/352] | train_loss 0.202 | train_acc  0.930 | 59s\n",
            "VAL 112 [040/040] | val_loss   0.354 | val_acc    0.876\n",
            "-----------------------------------------------------------------\n",
            "EP  113 [352/352] | train_loss 0.197 | train_acc  0.930 | 59s\n",
            "VAL 113 [040/040] | val_loss   0.359 | val_acc    0.881\n",
            "-----------------------------------------------------------------\n",
            "EP  114 [352/352] | train_loss 0.191 | train_acc  0.935 | 59s\n",
            "VAL 114 [040/040] | val_loss   0.382 | val_acc    0.871\n",
            "-----------------------------------------------------------------\n",
            "EP  115 [352/352] | train_loss 0.189 | train_acc  0.934 | 59s\n",
            "VAL 115 [040/040] | val_loss   0.342 | val_acc    0.882\n",
            "-----------------------------------------------------------------\n",
            "EP  116 [352/352] | train_loss 0.195 | train_acc  0.933 | 59s\n",
            "VAL 116 [040/040] | val_loss   0.302 | val_acc    0.897\n",
            "-----------------------------------------------------------------\n",
            "EP  117 [352/352] | train_loss 0.182 | train_acc  0.936 | 59s\n",
            "VAL 117 [040/040] | val_loss   0.354 | val_acc    0.884\n",
            "-----------------------------------------------------------------\n",
            "EP  118 [352/352] | train_loss 0.177 | train_acc  0.938 | 58s\n",
            "VAL 118 [040/040] | val_loss   0.331 | val_acc    0.890\n",
            "-----------------------------------------------------------------\n",
            "EP  119 [352/352] | train_loss 0.176 | train_acc  0.938 | 59s\n",
            "VAL 119 [040/040] | val_loss   0.313 | val_acc    0.892\n",
            "-----------------------------------------------------------------\n",
            "EP  120 [352/352] | train_loss 0.180 | train_acc  0.937 | 59s\n",
            "VAL 120 [040/040] | val_loss   0.349 | val_acc    0.883\n",
            "-----------------------------------------------------------------\n",
            "EP  121 [352/352] | train_loss 0.180 | train_acc  0.938 | 59s\n",
            "VAL 121 [040/040] | val_loss   0.307 | val_acc    0.898\n",
            "-----------------------------------------------------------------\n",
            "EP  122 [352/352] | train_loss 0.167 | train_acc  0.942 | 59s\n",
            "VAL 122 [040/040] | val_loss   0.339 | val_acc    0.887\n",
            "-----------------------------------------------------------------\n",
            "EP  123 [352/352] | train_loss 0.165 | train_acc  0.944 | 59s\n",
            "VAL 123 [040/040] | val_loss   0.363 | val_acc    0.883\n",
            "-----------------------------------------------------------------\n",
            "EP  124 [352/352] | train_loss 0.158 | train_acc  0.945 | 59s\n",
            "VAL 124 [040/040] | val_loss   0.340 | val_acc    0.890\n",
            "-----------------------------------------------------------------\n",
            "EP  125 [352/352] | train_loss 0.151 | train_acc  0.948 | 59s\n",
            "VAL 125 [040/040] | val_loss   0.310 | val_acc    0.902\n",
            "-----------------------------------------------------------------\n",
            "EP  126 [352/352] | train_loss 0.154 | train_acc  0.947 | 59s\n",
            "VAL 126 [040/040] | val_loss   0.311 | val_acc    0.903\n",
            "-----------------------------------------------------------------\n",
            "EP  127 [352/352] | train_loss 0.152 | train_acc  0.948 | 59s\n",
            "VAL 127 [040/040] | val_loss   0.312 | val_acc    0.898\n",
            "-----------------------------------------------------------------\n",
            "EP  128 [352/352] | train_loss 0.148 | train_acc  0.948 | 59s\n",
            "VAL 128 [040/040] | val_loss   0.331 | val_acc    0.888\n",
            "-----------------------------------------------------------------\n",
            "EP  129 [352/352] | train_loss 0.144 | train_acc  0.951 | 59s\n",
            "VAL 129 [040/040] | val_loss   0.311 | val_acc    0.901\n",
            "-----------------------------------------------------------------\n",
            "EP  130 [352/352] | train_loss 0.146 | train_acc  0.949 | 59s\n",
            "VAL 130 [040/040] | val_loss   0.303 | val_acc    0.899\n",
            "-----------------------------------------------------------------\n",
            "EP  131 [352/352] | train_loss 0.135 | train_acc  0.953 | 59s\n",
            "VAL 131 [040/040] | val_loss   0.371 | val_acc    0.880\n",
            "-----------------------------------------------------------------\n",
            "EP  132 [352/352] | train_loss 0.134 | train_acc  0.954 | 59s\n",
            "VAL 132 [040/040] | val_loss   0.310 | val_acc    0.899\n",
            "-----------------------------------------------------------------\n",
            "EP  133 [352/352] | train_loss 0.129 | train_acc  0.956 | 58s\n",
            "VAL 133 [040/040] | val_loss   0.328 | val_acc    0.893\n",
            "-----------------------------------------------------------------\n",
            "EP  134 [352/352] | train_loss 0.125 | train_acc  0.956 | 59s\n",
            "VAL 134 [040/040] | val_loss   0.305 | val_acc    0.898\n",
            "-----------------------------------------------------------------\n",
            "EP  135 [352/352] | train_loss 0.129 | train_acc  0.954 | 58s\n",
            "VAL 135 [040/040] | val_loss   0.280 | val_acc    0.909\n",
            "-----------------------------------------------------------------\n",
            "EP  136 [352/352] | train_loss 0.114 | train_acc  0.961 | 59s\n",
            "VAL 136 [040/040] | val_loss   0.320 | val_acc    0.899\n",
            "-----------------------------------------------------------------\n",
            "EP  137 [352/352] | train_loss 0.115 | train_acc  0.960 | 59s\n",
            "VAL 137 [040/040] | val_loss   0.285 | val_acc    0.905\n",
            "-----------------------------------------------------------------\n",
            "EP  138 [352/352] | train_loss 0.119 | train_acc  0.959 | 59s\n",
            "VAL 138 [040/040] | val_loss   0.364 | val_acc    0.885\n",
            "-----------------------------------------------------------------\n",
            "EP  139 [352/352] | train_loss 0.107 | train_acc  0.962 | 59s\n",
            "VAL 139 [040/040] | val_loss   0.339 | val_acc    0.895\n",
            "-----------------------------------------------------------------\n",
            "EP  140 [352/352] | train_loss 0.101 | train_acc  0.966 | 58s\n",
            "VAL 140 [040/040] | val_loss   0.302 | val_acc    0.909\n",
            "-----------------------------------------------------------------\n",
            "EP  141 [352/352] | train_loss 0.105 | train_acc  0.964 | 59s\n",
            "VAL 141 [040/040] | val_loss   0.278 | val_acc    0.912\n",
            "-----------------------------------------------------------------\n",
            "EP  142 [352/352] | train_loss 0.096 | train_acc  0.966 | 58s\n",
            "VAL 142 [040/040] | val_loss   0.342 | val_acc    0.894\n",
            "-----------------------------------------------------------------\n",
            "EP  143 [352/352] | train_loss 0.093 | train_acc  0.968 | 58s\n",
            "VAL 143 [040/040] | val_loss   0.276 | val_acc    0.910\n",
            "-----------------------------------------------------------------\n",
            "EP  144 [352/352] | train_loss 0.092 | train_acc  0.968 | 59s\n",
            "VAL 144 [040/040] | val_loss   0.332 | val_acc    0.901\n",
            "-----------------------------------------------------------------\n",
            "EP  145 [352/352] | train_loss 0.091 | train_acc  0.969 | 58s\n",
            "VAL 145 [040/040] | val_loss   0.386 | val_acc    0.883\n",
            "-----------------------------------------------------------------\n",
            "EP  146 [352/352] | train_loss 0.085 | train_acc  0.971 | 59s\n",
            "VAL 146 [040/040] | val_loss   0.292 | val_acc    0.909\n",
            "-----------------------------------------------------------------\n",
            "EP  147 [352/352] | train_loss 0.083 | train_acc  0.971 | 58s\n",
            "VAL 147 [040/040] | val_loss   0.313 | val_acc    0.902\n",
            "-----------------------------------------------------------------\n",
            "EP  148 [352/352] | train_loss 0.076 | train_acc  0.974 | 58s\n",
            "VAL 148 [040/040] | val_loss   0.281 | val_acc    0.911\n",
            "-----------------------------------------------------------------\n",
            "EP  149 [352/352] | train_loss 0.074 | train_acc  0.975 | 58s\n",
            "VAL 149 [040/040] | val_loss   0.309 | val_acc    0.903\n",
            "-----------------------------------------------------------------\n",
            "EP  150 [352/352] | train_loss 0.067 | train_acc  0.977 | 58s\n",
            "VAL 150 [040/040] | val_loss   0.259 | val_acc    0.919\n",
            "-----------------------------------------------------------------\n",
            "EP  151 [352/352] | train_loss 0.066 | train_acc  0.978 | 58s\n",
            "VAL 151 [040/040] | val_loss   0.285 | val_acc    0.912\n",
            "-----------------------------------------------------------------\n",
            "EP  152 [352/352] | train_loss 0.061 | train_acc  0.979 | 58s\n",
            "VAL 152 [040/040] | val_loss   0.306 | val_acc    0.909\n",
            "-----------------------------------------------------------------\n",
            "EP  153 [352/352] | train_loss 0.059 | train_acc  0.981 | 58s\n",
            "VAL 153 [040/040] | val_loss   0.297 | val_acc    0.910\n",
            "-----------------------------------------------------------------\n",
            "EP  154 [352/352] | train_loss 0.054 | train_acc  0.982 | 59s\n",
            "VAL 154 [040/040] | val_loss   0.278 | val_acc    0.914\n",
            "-----------------------------------------------------------------\n",
            "EP  155 [352/352] | train_loss 0.054 | train_acc  0.981 | 58s\n",
            "VAL 155 [040/040] | val_loss   0.268 | val_acc    0.922\n",
            "-----------------------------------------------------------------\n",
            "EP  156 [352/352] | train_loss 0.048 | train_acc  0.984 | 58s\n",
            "VAL 156 [040/040] | val_loss   0.287 | val_acc    0.918\n",
            "-----------------------------------------------------------------\n",
            "EP  157 [352/352] | train_loss 0.045 | train_acc  0.985 | 58s\n",
            "VAL 157 [040/040] | val_loss   0.304 | val_acc    0.915\n",
            "-----------------------------------------------------------------\n",
            "EP  158 [352/352] | train_loss 0.040 | train_acc  0.987 | 58s\n",
            "VAL 158 [040/040] | val_loss   0.277 | val_acc    0.922\n",
            "-----------------------------------------------------------------\n",
            "EP  159 [352/352] | train_loss 0.040 | train_acc  0.987 | 58s\n",
            "VAL 159 [040/040] | val_loss   0.267 | val_acc    0.924\n",
            "-----------------------------------------------------------------\n",
            "EP  160 [352/352] | train_loss 0.040 | train_acc  0.987 | 58s\n",
            "VAL 160 [040/040] | val_loss   0.272 | val_acc    0.919\n",
            "-----------------------------------------------------------------\n",
            "EP  161 [352/352] | train_loss 0.032 | train_acc  0.990 | 58s\n",
            "VAL 161 [040/040] | val_loss   0.289 | val_acc    0.917\n",
            "-----------------------------------------------------------------\n",
            "EP  162 [352/352] | train_loss 0.031 | train_acc  0.990 | 59s\n",
            "VAL 162 [040/040] | val_loss   0.272 | val_acc    0.923\n",
            "-----------------------------------------------------------------\n",
            "EP  163 [352/352] | train_loss 0.027 | train_acc  0.992 | 59s\n",
            "VAL 163 [040/040] | val_loss   0.265 | val_acc    0.925\n",
            "-----------------------------------------------------------------\n",
            "EP  164 [352/352] | train_loss 0.025 | train_acc  0.993 | 60s\n",
            "VAL 164 [040/040] | val_loss   0.249 | val_acc    0.930\n",
            "-----------------------------------------------------------------\n",
            "EP  165 [352/352] | train_loss 0.021 | train_acc  0.994 | 59s\n",
            "VAL 165 [040/040] | val_loss   0.255 | val_acc    0.930\n",
            "-----------------------------------------------------------------\n",
            "EP  166 [352/352] | train_loss 0.019 | train_acc  0.995 | 59s\n",
            "VAL 166 [040/040] | val_loss   0.259 | val_acc    0.928\n",
            "-----------------------------------------------------------------\n",
            "EP  167 [352/352] | train_loss 0.017 | train_acc  0.995 | 59s\n",
            "VAL 167 [040/040] | val_loss   0.245 | val_acc    0.931\n",
            "-----------------------------------------------------------------\n",
            "EP  168 [352/352] | train_loss 0.015 | train_acc  0.996 | 59s\n",
            "VAL 168 [040/040] | val_loss   0.240 | val_acc    0.934\n",
            "-----------------------------------------------------------------\n",
            "EP  169 [352/352] | train_loss 0.014 | train_acc  0.997 | 60s\n",
            "VAL 169 [040/040] | val_loss   0.257 | val_acc    0.932\n",
            "-----------------------------------------------------------------\n",
            "EP  170 [352/352] | train_loss 0.013 | train_acc  0.996 | 60s\n",
            "VAL 170 [040/040] | val_loss   0.239 | val_acc    0.934\n",
            "-----------------------------------------------------------------\n",
            "EP  171 [352/352] | train_loss 0.011 | train_acc  0.997 | 59s\n",
            "VAL 171 [040/040] | val_loss   0.241 | val_acc    0.938\n",
            "-----------------------------------------------------------------\n",
            "EP  172 [352/352] | train_loss 0.008 | train_acc  0.998 | 59s\n",
            "VAL 172 [040/040] | val_loss   0.238 | val_acc    0.935\n",
            "-----------------------------------------------------------------\n",
            "EP  173 [352/352] | train_loss 0.009 | train_acc  0.998 | 59s\n",
            "VAL 173 [040/040] | val_loss   0.255 | val_acc    0.933\n",
            "-----------------------------------------------------------------\n",
            "EP  174 [352/352] | train_loss 0.008 | train_acc  0.998 | 60s\n",
            "VAL 174 [040/040] | val_loss   0.249 | val_acc    0.939\n",
            "-----------------------------------------------------------------\n",
            "EP  175 [352/352] | train_loss 0.007 | train_acc  0.999 | 60s\n",
            "VAL 175 [040/040] | val_loss   0.234 | val_acc    0.941\n",
            "-----------------------------------------------------------------\n",
            "EP  176 [352/352] | train_loss 0.007 | train_acc  0.999 | 60s\n",
            "VAL 176 [040/040] | val_loss   0.237 | val_acc    0.940\n",
            "-----------------------------------------------------------------\n",
            "EP  177 [352/352] | train_loss 0.006 | train_acc  0.999 | 59s\n",
            "VAL 177 [040/040] | val_loss   0.230 | val_acc    0.940\n",
            "-----------------------------------------------------------------\n",
            "EP  178 [352/352] | train_loss 0.005 | train_acc  0.999 | 59s\n",
            "VAL 178 [040/040] | val_loss   0.229 | val_acc    0.941\n",
            "-----------------------------------------------------------------\n",
            "EP  179 [352/352] | train_loss 0.004 | train_acc  0.999 | 60s\n",
            "VAL 179 [040/040] | val_loss   0.223 | val_acc    0.941\n",
            "-----------------------------------------------------------------\n",
            "EP  180 [352/352] | train_loss 0.004 | train_acc  0.999 | 60s\n",
            "VAL 180 [040/040] | val_loss   0.222 | val_acc    0.943\n",
            "-----------------------------------------------------------------\n",
            "EP  181 [352/352] | train_loss 0.004 | train_acc  1.000 | 60s\n",
            "VAL 181 [040/040] | val_loss   0.224 | val_acc    0.944\n",
            "-----------------------------------------------------------------\n",
            "EP  182 [352/352] | train_loss 0.003 | train_acc  1.000 | 60s\n",
            "VAL 182 [040/040] | val_loss   0.233 | val_acc    0.942\n",
            "-----------------------------------------------------------------\n",
            "EP  183 [352/352] | train_loss 0.003 | train_acc  1.000 | 59s\n",
            "VAL 183 [040/040] | val_loss   0.237 | val_acc    0.943\n",
            "-----------------------------------------------------------------\n",
            "EP  184 [352/352] | train_loss 0.004 | train_acc  1.000 | 59s\n",
            "VAL 184 [040/040] | val_loss   0.238 | val_acc    0.940\n",
            "-----------------------------------------------------------------\n",
            "EP  185 [352/352] | train_loss 0.003 | train_acc  1.000 | 60s\n",
            "VAL 185 [040/040] | val_loss   0.224 | val_acc    0.942\n",
            "-----------------------------------------------------------------\n",
            "EP  186 [352/352] | train_loss 0.003 | train_acc  1.000 | 60s\n",
            "VAL 186 [040/040] | val_loss   0.225 | val_acc    0.944\n",
            "-----------------------------------------------------------------\n",
            "EP  187 [352/352] | train_loss 0.003 | train_acc  1.000 | 60s\n",
            "VAL 187 [040/040] | val_loss   0.219 | val_acc    0.946\n",
            "-----------------------------------------------------------------\n",
            "EP  188 [352/352] | train_loss 0.003 | train_acc  1.000 | 59s\n",
            "VAL 188 [040/040] | val_loss   0.222 | val_acc    0.943\n",
            "-----------------------------------------------------------------\n",
            "EP  189 [352/352] | train_loss 0.003 | train_acc  1.000 | 59s\n",
            "VAL 189 [040/040] | val_loss   0.227 | val_acc    0.941\n",
            "-----------------------------------------------------------------\n",
            "EP  190 [352/352] | train_loss 0.003 | train_acc  1.000 | 60s\n",
            "VAL 190 [040/040] | val_loss   0.227 | val_acc    0.944\n",
            "-----------------------------------------------------------------\n",
            "EP  191 [352/352] | train_loss 0.003 | train_acc  1.000 | 60s\n",
            "VAL 191 [040/040] | val_loss   0.241 | val_acc    0.939\n",
            "-----------------------------------------------------------------\n",
            "EP  192 [352/352] | train_loss 0.003 | train_acc  1.000 | 60s\n",
            "VAL 192 [040/040] | val_loss   0.228 | val_acc    0.945\n",
            "-----------------------------------------------------------------\n",
            "EP  193 [352/352] | train_loss 0.003 | train_acc  1.000 | 59s\n",
            "VAL 193 [040/040] | val_loss   0.226 | val_acc    0.944\n",
            "-----------------------------------------------------------------\n",
            "EP  194 [352/352] | train_loss 0.003 | train_acc  1.000 | 59s\n",
            "VAL 194 [040/040] | val_loss   0.223 | val_acc    0.945\n",
            "-----------------------------------------------------------------\n",
            "EP  195 [352/352] | train_loss 0.003 | train_acc  1.000 | 60s\n",
            "VAL 195 [040/040] | val_loss   0.224 | val_acc    0.942\n",
            "-----------------------------------------------------------------\n",
            "EP  196 [352/352] | train_loss 0.003 | train_acc  1.000 | 60s\n",
            "VAL 196 [040/040] | val_loss   0.223 | val_acc    0.940\n",
            "-----------------------------------------------------------------\n",
            "EP  197 [352/352] | train_loss 0.002 | train_acc  1.000 | 60s\n",
            "VAL 197 [040/040] | val_loss   0.217 | val_acc    0.946\n",
            "-----------------------------------------------------------------\n",
            "EP  198 [352/352] | train_loss 0.002 | train_acc  1.000 | 59s\n",
            "VAL 198 [040/040] | val_loss   0.231 | val_acc    0.943\n",
            "-----------------------------------------------------------------\n",
            "EP  199 [352/352] | train_loss 0.002 | train_acc  1.000 | 59s\n",
            "VAL 199 [040/040] | val_loss   0.227 | val_acc    0.943\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zziHauA7qUha",
        "outputId": "c02a708c-919b-48c9-bcea-38a1b9d9ce6a"
      },
      "source": [
        "trainer.test(test_loader)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rTEST [079/079] | test_loss  0.218 | test_acc   0.943\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}